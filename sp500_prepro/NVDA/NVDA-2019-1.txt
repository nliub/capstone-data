Colette M. Kress: Jensen, why don't you start on the question for Stacy and I'll follow-up afterwards after you speak.
Colette M. Kress: Thanks so much for the question. When you think about our gross margins, just over this last quarter, as you know, we were working on stabilizing the overall supply that was out there in the market for consumer GPUs. We benefited from that with a higher gross margin as we filled and completed that. You've seen us absorb a significant amount of the component pricing changes that we have seen, particularly around the memory. We're not here to be able to forecast generally when those pricing of those components will stabilize. But we believe in terms of the value added that our platforms provide, the components are an important part of finishing that. But I think we have tremendous amount more value that we are adding in terms of the software on top of our platforms, which is enabling our gross margins.
Colette M. Kress: So, I'm going to ask the operator if they could ask for the question again because it was also on our side a little crackly.
Colette M. Kress: Sure, I'll take that question. Generally, our OEM business can be a little bit volatile. Because remember, OEM business incorporates our mainstream GPUs as well as our Tegra integrated. So we have development platforms that we sell on some of the Tegra piece of it. But they are slightly below and I think you can go back and refer to our discussion at Investor Day as there's a slide there that talks about those embedded pieces and them being below. So yes, you're correct. Again, a very small part of our business right now.
Colette M. Kress: Sure, Stacy. Let me see if I can bridge together, Jensen, and then some comments here. Unfortunately, they're moving quite fast to the next question, so I wasn't able to add-on. But let me see if I can add-on here and provide a little bit of clarity in terms of the seasonality. Remember in Q1, we outgrew seasonality significantly. We left Q4 with very low inventory in terms of in the channel. We spent Q1 working on establishing a decent amount of inventory available. We wanted to concentrate on our miners separately. And then you can see we did that in terms of Q1 by moving that to OEM and moving that to cryptocurrency only boards. So we left Q1 at this point with healthy overall channel inventory levels as far as where we stand. So that then takes you now to Q2. But if we overshot in terms of seasonality in terms of Q1, we don't have to do those channel fill dynamics again as we get into Q2. But we do have demand out there for our gamers that we can now address very carefully with the overall inventory that we now have available. So putting together, Q1 and Q2 together, yes, we are within normal seasonality, again, for a guidance. And we'll see how we'll finish in terms of the quarter. But you should be in that range. So, yes, from a normal seasonality, at a year-to-date inclusive of Q2, yes, we're on that overall seasonality. Always keep in mind, generally, our H2s are usually higher than our overall H1s, and that's what you should think about our overall guidance. Gaming is still strong. We have to comment that our overall drivers that have taken us to this place over the last three to five years with phenomenal growth and our ability to grow that overall market is still here and all of those things are together. We just had a few quarters in terms of making sure that we get the overall channel correct and put our miners separately. I hope that clarifies in terms of where we are, in terms of gaming seasonality.
Colette M. Kress: Let me start off here, and I'll have Jensen finish up on the last part of that question. But overall, our datacenter business did phenomenal. Volta is doing extremely well. And even now with 32-bit, we're seeing tremendous adoption throughout. Again, remember it's very different than the overall consumer business. You have significant amount of time for qualification and that is moving extremely fast, based on a lot of other industries and their ability to qualify. So no, there is not a supply challenge at all in terms of our datacenter. And our overall growth in datacenter, we're extremely pleased with in terms of how the quarter came out. I'll turn it over to you, Jensen, and you can answer the rest of the part of it.
Jen-Hsun Huang - NVIDIA Corp.: Okay. Hi Stacy, so let's see. Q1, as you probably know, Fortnite and PUBG are global phenomenons (sic) [phenomena] (18:51). The success of Fortnite and PUBG are just beyond comprehension, really. Those two games are a combination of Hunger Games and Survivor has just captured imaginations of gamers all over the world. And we saw the uptick and we saw the demand on GPUs from all over the world. Surely, there was scarcity as you know. Crypto miners bought a lot of our GPUs during the quarter and it drove prices up. And I think that a lot of the gamers weren't able to buy into the new GeForce as a result. And so we're starting to see the prices come down. We monitor spot pricing every single day around the world. And the prices are starting to normalize. It's still higher than where they should be. And so obviously, the demand is still quite strong out there. But my sense is that there's a fair amount of pent-up demand still. Fortnite is still growing in popularity. PUBG is doing great. And then we've got some amazing titles coming out. And so my sense is that the overall gaming market is just really, is super healthy. And our job is to make sure that we work as hard as we can to get supply out into the marketplace. And hopefully, by doing that, the pricing will normalize and the gamers can buy into their favorite graphics card at a price that we hope they can get it at. And so I think there's a fair – I mean the simple answer to your question is Fortnite and PUBG. And the demand is just really great. They did a great job.
Jen-Hsun Huang - NVIDIA Corp.: Sure hi Joe. So as you know, there are 30 million servers around the world. And they were put in place during the time when the world didn't have deep learning. And now with deep learning and with machine learning approaches, the accuracy of prediction, the accuracy of recommendation has jumped so much that just about every Internet service provider in the world that has a lot of different customers and consumers are jumping onto this new software approach. And in order to take this neural network – and the software that's written by deep learning in these frameworks are massive software. The way to think about these deep neural nets is it has millions and millions and millions of parameters in it and these networks are getting larger every year. And they're enormously complex. And the output of these neural nets have to be optimized for the computing platform that it targets. How you would optimize the neural network for a CPU or a GPU is very, very different. And how you optimize for different neural networks, whether it's image recognition, speech recognition, natural language translation, recommendation systems, all of these networks have different architectures and an optimizing compiler that's necessary to make the neural network run smoothly and fast is incredibly complex. And so that's why we created TensorRT. That's what TensorRT is. TensorRT is an optimizing graph neural network compiler. And it optimizes for each one of our platforms. And even each one of our platforms has very different architectures. For example, we invented recently – reinvented the GPU and it's called the Tensor Core GPU, and the first of its kind is called Volta. And so TensorRT 4.0 now supports, in addition to image recognition, all of the different types of neural network models. The answer to your question is internal consumption. Internal consumption is going to be the first users. Video recognition, detecting for inappropriate video for example all over the world, making recommendations from the videos that you search or the images that you're uploading. All of these types of applications are going to require enormous amount of computation.
Jen-Hsun Huang - NVIDIA Corp.: Yeah. Thanks for your question, Toshiya. At the core, the program was about making sure that gamers who buy graphics cards knows exactly the GPU brand that's inside. And the reason for that is because, we want gamers to – the gaming experience of a graphics card depends so much on the GPU that is chosen. And we felt that using one gaming brand, a graphics card brand, and interchanging the GPU underneath causes it to be less – causes it to be more opaque and less transparent for gamers to choose the GPU brand that they wanted. And most of the ecosystem loved it. And some of the people really disliked it. And so instead of all that distraction, we're doing so well. And we're going to continue to help the gamers choose the graphics cards, like we always have, and things will sort out. And so we decided to pull the plug because the distraction was unnecessary and we have too much good stuff to go do.
Jen-Hsun Huang - NVIDIA Corp.: Yeah, thanks for the question. Well, HPC. First of all, at the core, CPU scaling has stalled and it's reached the limits of physics. And the world needs another approach to go forward. We created the GPU computing approach a decade and half ago. And I think at this point, with the number developers jumping on, the number of applications that's emerging, it's really clear that the future of HPC has accelerated. And our GPU approach, because of its flexibility, because of its performance, because of the value that we create that as a result of the throughput of a datacenter, we save people so much money just in cables alone, often times, more than pays for the GPUs that they buy. And the reason for that is because the number of servers reduced dramatically. And so I think the future of HPC is about acceleration and the NVIDIA CUDA GPUs are really in a great position to serve this vacuum that's been created. With respect to benchmarks, you might have seen that earlier this week, we released three speed records: the fastest single GPU, the fastest single computer node, a definition of a computer node is something that fits in a box that runs one operating system, one node; and one instance, one cloud instance. We now have the fastest speed record for one GPU, one node, and one instance. And so we love benchmarks. Nothing is more joyful than having a benchmark to demonstrate your leadership position. And in the world of deep learning, the number of networks is just growing so fast, because the number of different applications that deep learning is able to solve is really huge. And so you need a lot of software capability and the versatility of your platform needs to be great. We also have a lot of expertise in the company in software. I mean, NVIDIA is really a full stack computing company, from architecture, to system software, to algorithms, to applications. We have a great deal of expertise across the entire stack. And so we love these complicated benchmarking – benchmarks that are out there. And I think this is a great way for us to simplify our leadership position. I think long-term, the number of networks that are going to emerge will continue to grow. And so the flexibility of ASICs is going to be its greatest downfall. And if someone were to create a general-purpose parallel accelerating processor like ours and had it designed to be incredibly good at deep learning, like recently what we did with our Tensor Core GPU, which is a reinvented GPU, and Volta is the first one, it's going to be hard. It's going to be expensive. And we've been doing it a very long time. And so I think this is a – it's a great time for us.
Jen-Hsun Huang - NVIDIA Corp.: Yeah, Blayne. The largest inference opportunity for us is actually in the cloud and the datacenter. That's the first great opportunity. And the reason for that is there's just an explosion in the number of different types of neural networks that are available. They're image recognition, there's video sequencing, there's video, there's recommender systems, there's speech recognition, speech synthesis, natural language understanding. There are just so many different types of neural networks that are being created. And creating one ASIC that can be adapted to all of these different types of networks is just a real challenge. And by the time that you create such a thing, it's called a Tensor Core GPU, which is what we created. And so I think that the first opportunity for us in large-scale opportunity will be in the datacenter and the cloud. The second will be in vertical markets. The vertical market that you mentioned is self-driving cars. And we see a great opportunity in autonomous vehicles, both in the creation of autonomous vehicles. And I mentioned that before, between now and the time that we ramp our AV computers we call DRIVE, we're going to be selling a whole lot of servers, so that the companies could develop their neural network models for their self-driving cars, as well as simulating in virtual reality their various test drives, as well as testing their neural network and their self-driving car stack against billions and billions of miles of saved up pre-recorded videos. And so in the vertical markets, we're going to see inference both in the datacenter for developing the self-driving car stack as well as in the self-driving cars themselves. Now, in the self-driving cars, the ASPs for Level 2 could be a few hundred dollars to a Level 5 self-driving car, taxi or driverless taxi being a few thousand dollars. And I expect that driverless taxis will start going to market about 2019 and self-driving cars probably somewhere between 2020 and 2021. And I think the size of the market is fairly well modeled. And the simple way to think about that is I believe that every single – everything that moves someday will be autonomous or have autonomous capabilities. And so the 100 million cars, the countless taxis, all the trucks, all the agriculture equipment, all the pizza delivery vehicles, you name it. Everything is going to be autonomous. And the market opportunity is going to be quite large. And that's the reason why we're so determined to go create that market.
Jen-Hsun Huang - NVIDIA Corp.: Yes. So first of all, Volta is a reinvented GPU. Volta is the world's first GPU that has been designed to be incredibly good at deep learning. We call it the Tensor Core GPU. It still retained all of the flexibilities of all – everything that CUDA has ever run is backwards compatible, with everything that runs on CUDA. But it has new architectures designed to be incredibly good at deep learning. We call it a Tensor Core GPU. And that's the reason why it has all of the benefits of our GPU but none of the ASICs can catch up to it. And so Volta is really a breakthrough. We're going to be very successful with Volta. Every cloud will have it. The initial deployment is for internal consumption. Volta has been shipping to the cloud providers, the Internet service companies for the vast majority of last quarter, as you guys know. And they're using it internally. And now they're starting to open up Volta for external consumption of their cloud customers. And they are moving as fast as they can. My expectation is that you're going to see a lot more coming online this quarter.
Jen-Hsun Huang - NVIDIA Corp.: I see. Thank you. DGX-2 and DGX-1 will both be in the market at the same time. And DGX is a few hundred million dollar business. It was introduced last year. So its growth rate is obviously very high. It's designed for enterprises where they don't – they need to have their computers on-premise, but they don't want to build a supercomputer. And they don't have the expertise to do so. And they would like to pull a supercomputer out of a box, plug it in and start doing supercomputing. And so DGX is really designed for enterprises. It's designed for car companies; it's designed for healthcare companies doing life sciences work or medical imaging work. We recently announced a project called Project Clara, which basically takes medical imaging equipment, virtualizes them, containerizes the software and turns it into a – and most medical imaging equipment today are computational and they – a lot of them run on NVIDIA CUDA anyways. We can put that into the datacenter, we can virtualize their medical instruments and it gives them the opportunity to upgrade the millions of instruments that are out in the marketplace today. And so DGX is really designed for enterprises and we're seeing great success there. It's really super easy to use and it comes with direct support from HPC and AI researchers at NVIDIA. And the answer to your question at the end is both of them will be in the marketplace at the same time.
Jen-Hsun Huang - NVIDIA Corp.: Well, we try to as transparently review our numbers as best we can. Our strategy is to create a SKU that allows the crypto miners to fulfill their needs and we call it CMP. And to be – as much as possible, fulfill their demand that way. Sometimes, it's just not possible because the demand is too great but we try to do so. And we try to keep the miners on the CMP SKUs as much as we can. And so I'm not exactly sure how other people do it, but that's the way we do it.
Jen-Hsun Huang - NVIDIA Corp.: Yeah, I appreciate it. NVIDIA RTX is the biggest computer graphics invention in the last 15 years. It took us a decade to do. We literally worked on it continuously for one decade. And to put it into perspective, it's basically film rendering, cinematic rendering except it's in real time. It merges the style of computer graphics, rasterization, and light simulation, what people call ray tracing as well as deep learning and AI, merged it into one unified framework, so that we can achieve cinematic rendering in real time. What it currently takes is a server about a few hours, depending on the scene, it might take as long as a full day, take a few hours to render one frame. So it takes a server, one node of a server, several hours to render one frame. And in order to render 30 frames per second, just imagine the number of servers you need. If you take several hours per frame and you need to render 30 frames per second in order to be real-time, it basically takes a high-performance computer, a supercomputer, a render farm, that's why they call it a render farm, it's a full datacenter designed just for rendering. And now we've created NVIDIA RTX which makes it possible to do in real time. We demonstrated RTX on four Quadro GV100s. It takes four of our latest generation Volta Tensor Core GPUs to be able to render 30 frames per second, the Star Wars cinematic that people enjoyed. And so the amount that we saved, we basically took an entire datacenter and reduced it into one node. And we're now doing it in real time. And so the amount of money that we can save, people who create movies, people who do commercials, people who use film rendering to create the game content, almost every single game is done that way. There's quite a bit of offline rendering to create the imagery and the textures and the lighting. And then there are of course, architectural design and car design, the number of applications, the number of industries that are built on top of modern computer graphics is really quite large. And I'm certain that NVIDIA RTX is going to impact every single one of them. And so that's our starting point, is to dramatically reduce the cost of film rendering, dramatically reduce the time that it takes to do it and hopefully, more GPU servers will be purchased. And of course, better content will be created. Long-term, we've also now plotted the path towards doing it in real time. And someday, we will be able to put RTX into a GeForce gaming card and the transformation to the revolution to the gaming industry will be quite extraordinary. So we're super excited about RTX.
Jen-Hsun Huang - NVIDIA Corp.: Yeah. The reason why miners love GeForce is because miners are everywhere in the world. One of the benefits of cryptocurrency is that it's not any sovereign currency. And it's in the digital world, it's distributed. And GeForce is the single largest distributed supercomputing infrastructure on the planet. Every gamer has a supercomputer in their PC. And GeForce is so broadly distributed, it's available everywhere. And so GeForce is really a good candidate for any new cryptocurrency or any new cryptography algorithm that comes along. We try the best we can to go directly to the major miners. And they represent the vast majority of the demand. And to the best of our ability, serve their needs directly and we call that C&P (1:02:21) and that's why it's not called GeForce, they're called C&P (1:02:25). And we can serve those miners directly, hopefully to take some of the demand pressure off of the GeForce market. Because ultimately, what we would like is we would like the market for GeForce pricing to come down, so that the gamers could benefit from the GeForces that we built for them. And the gaming demand is strong. I mean, the bottom line is Fortnite is a homerun. The bottom line is PUBG is a homerun. And the number of gamers that are enjoying these games is really astronomic as people know very well. And it's a global phenomenon. These two games are equally fun in Asia as it is in Europe as it is in the United States. And because you team up and this is a Battle Royale, you'd rather play with your friends. So it's incredibly social. It's incredibly sticky. And more and more – more gamers that play, more of their friends join, and more of their friends join, more gamers that play. And so it's this positive feedback system, and the guys at Epic did a fantastic job creating Fortnite. And it's just a wonderful game genre that people are really enjoying. And so I think at the core of it, gaming is strong and we are looking forward to inventory normalizing in the channel so that pricing could normalize in the channel, so that gamers can come back to buy the GeForce cards that has now been in short supply for over a quarter. And so the pent-up demand is quite significant. And I'm expecting the gamers to be able to buy new GeForces pretty soon.
Jen-Hsun Huang - NVIDIA Corp.: Okay. We had another great quarter, record revenue, record margins, record earnings, growth across every platform. Datacenter achieved another record with strong demand for Volta and AI inference. Gaming was strong. We are delighted to see prices normalizing and we can better serve pent-up gamer demand. At the heart of our opportunity is the incredible growth of computing demand of AI just as traditional computing has slowed. The GPU computing approach that we've pioneered is ideal for filling this vacuum. And our invention of the Tensor Core GPU has further enhanced our strong position to power the AI era. I look forward to giving you another update next quarter. Thank you.
