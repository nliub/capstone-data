Jensen Huang: Thanks, Colette. The cumulation of technology breakthroughs has brought AI to an inflection point. Generative AI's versatility and capability has triggered a sense of urgency at enterprises around the world to develop and deploy AI strategies. Yet, the AI supercomputer infrastructure, model algorithms, data processing and training techniques remain an insurmountable obstacle for most. Today, I want to share with you the next level of our business model to help put AI within reach of every enterprise customer.  We are partnering with major service -- cloud service providers to offer NVIDIA AI cloud services, offered directly by NVIDIA and through our network of go-to-market partners, and hosted within the world's largest clouds. NVIDIA AI as a service offers enterprises easy access to the world's most advanced AI platform, while remaining close to the storage, networking, security and cloud services offered by the world's most advanced clouds.  Customers can engage NVIDIA AI cloud services at the AI supercomputer, acceleration library software or pretrained AI model layers. NVIDIA DGX is an AI supercomputer, and the blueprint of AI factories being built around the world. AI supercomputers are hard and time-consuming to build. Today, we are announcing the NVIDIA DGX Cloud, the fastest and easiest way to have your own DGX AI supercomputer, just open your browser. NVIDIA DGX Cloud is already available through Oracle Cloud Infrastructure and Microsoft Azure, Google GCP and others on the way.  At the AI platform software layer, customers can access NVIDIA AI enterprise for training and deploying large language models or other AI workloads. And at the pretrained generative AI model layer, we will be offering NeMo and BioNeMo, customizable AI models, to enterprise customers who want to build proprietary generative AI models and services for their businesses. With our new business model, customers can engage NVIDIA's full scale of AI computing across their private to any public cloud. We will share more details about NVIDIA AI cloud services at our upcoming GTC so be sure to tune in.  Now let me turn it back to Colette on gaming.
Jensen Huang: Yes, first of all, taking a step back, NVIDIA AI is essentially the operating system of AI systems today. It starts from data processing to learning, training, to validations, to inference. And so this body of software is completely accelerated. It runs in every cloud. It runs on-prem. And it supports every framework, every model that we know of, and it's accelerated everywhere.  By using NVIDIA AI, your entire machine learning operations is more efficient, and it is more cost effective. You save money by using accelerated software. Our announcement today of putting NVIDIA's infrastructure and have it be hosted from within the world's leading cloud service providers accelerates the enterprise's ability to utilize NVIDIA AI enterprise. It accelerates people's adoption of this machine learning pipeline, which is not for the faint of heart. It is a very extensive body of software. It is not deployed in enterprises broadly, but we believe that by hosting everything in the cloud, from the infrastructure through the operating system software, all the way through pretrained models, we can accelerate the adoption of generative AI in enterprises. And so we're excited about this new extended part of our business model. We really believe that it will accelerate the adoption of software.
Jensen Huang: Large language models are called large because they are quite large. However, remember that we've accelerated and advanced AI processing by a million x over the last decade. Moore's Law, in its best days, would have delivered 100x in a decade. By coming up with new processors, new systems, new interconnects, new frameworks and algorithms and working with data scientists, AI researchers on new models, across that entire span, we've made large language model processing a million times faster, a million times faster.  What would have taken a couple of months in the beginning, now it happens in about 10 days. And of course, you still need a large infrastructure. And even the large infrastructure, we're introducing Hopper, which, with its transformer engine, it's new NVLink switches and its new InfiniBand 400 gigabits per second data rates, we're able to take another leap in the processing of large language models.  And so I think the -- by putting NVIDIA's DGX supercomputers into the cloud with NVIDIA DGX cloud, we're going to democratize the access of this infrastructure, and with accelerated training capabilities, really make this technology and this capability quite accessible. So that's one thought.  The second is the number of large language models or foundation models that have to be developed is quite large. Different countries with different cultures and its body of knowledge are different. Different fields, different domains, whether it's imaging or its biology or its physics, each one of them need their own domain of foundation models. With large language models, of course, we now have a prior that could be used to accelerate the development of all these other fields, which is really quite exciting.  The other thing to remember is that the number of companies in the world have their own proprietary data. The most valuable data in the world are proprietary. And they belong to the company. It's inside their company. It will never leave the company. And that body of data will also be harnessed to train new AI models for the very first time. And so we -- our strategy and our goal is to put the DGX infrastructure in the cloud so that we can make this capability available to every enterprise, every company in the world who would like to create proprietary data and so -- proprietary models.  The second thing about competition. We've had competition for a long time. Our approach, our computing architecture, as you know, is quite different on several dimensions. Number one, it is universal, meaning you could use it for training, you can use it for inference, you can use it for models of all different types. It supports every framework. It supports every cloud. It's everywhere. It's cloud to private cloud, cloud to on-prem. It's all the way out to the edge. It could be an autonomous system. This one architecture allows developers to develop their AI models and deploy it everywhere.  The second very large idea is that no AI in itself is an application. There's a preprocessing part of it and a post-processing part of it to turn it into an application or service. Most people don't talk about the pre and post processing because it's maybe not as sexy and not as interesting. However, it turns out that preprocessing and post-processing oftentimes consumes half or 2/3 of the overall workload. And so by accelerating the entire end-to-end pipeline, from preprocessing, from data ingestion, data processing, all the way to the preprocessing all the way to post processing, we're able to accelerate the entire pipeline versus just accelerating half of the pipeline. The limit to speed up, even if you're instantly passed if you only accelerate half of the workload, is twice as fast. Whereas if you accelerate the entire workload, you could accelerate the workload maybe 10, 20, 50x faster, which is the reason why when you hear about NVIDIA accelerating applications, you routinely hear 10x, 20x, 50x speed up. And the reason for that is because we accelerate things end to end, not just the deep learning part of it, but using CUDA to accelerate everything from end to end.  And so I think the universality of our computing -- accelerated computing platform, the fact that we're in every cloud, the fact that we're from cloud to edge, makes our architecture really quite accessible and very differentiated in this way. And most importantly, to all the service providers, because of the utilization is so high, because you can use it to accelerate the end-to-end workload and get such a good throughput, our architecture is the lowest operating cost. It's not -- the comparison is not even close. So -- anyhow those are the 2 answers.
Jensen Huang: ChatGPT is a wonderful piece of work, and the team did a great job, OpenAI did a great job with it. They stuck with it. And the accumulation of all of the breakthroughs led to a service with a model inside that surprised everybody with its versatility and its capability.  What people were surprised by, and this is in our -- and close within the industry is well understood. But the surprising capability of a single AI model that can perform tasks and skills that it was never trained to do. And for this language model to not just speak English, or can translate, of course, but not just speak human language, it can be prompted in human language, but output Python, output Cobalt, a language that very few people even remember, output Python for Blender, a 3D program. So it's a program that writes a program for another program.  We now realize -- the world now realizes that maybe human language is a perfectly good computer programming language, and that we've democratized computer programming for everyone, almost anyone who could explain in human language a particular task to be performed. This new computer -- when I say new era of computing, this new computing platform, this new computer could take whatever your prompt is, whatever your human-explained request is, and translate it to a sequence of instructions that you process it directly, or it waits for you to decide whether you want to process it or not.  And so this type of computer is utterly revolutionary in its application because it's democratized programming to so many people really has excited enterprises all over the world. Every single CSP, every single Internet service provider, and they're, frankly, every single software company, because of what I just explained, that this is an AI model that can write a program for any program. Because of that reason, everybody who develops software is either alerted or shocked into alert or actively working on something that is like ChatGPT to be integrated into their application or integrated into their service. And so this is, as you can imagine, utterly worldwide.  The activity around the AI infrastructure that we build Hopper and the activity around inferencing using Hopper and Ampere to inference large language models, has just gone through the roof in the last 60 days. And so there's no question that whatever our views are of this year as we enter the year has been fairly, dramatically changed as a result of the last 60, 90 days.
Jensen Huang: Yes, I appreciate the question. First of all, as you know, our Data Center business is a GPU business only in the context of a conceptual GPU because what we actually sell to the cloud service providers is a panel, a fairly large computing panel of 8 Hoppers or 8 Amperes that's connected with NVLink switches that are connected with NVLink. And so this board represents essentially 1 GPU. It's 8 chips connected together into 1 GPU with a very high-speed chip-to-chip interconnect. And so we've been working on, if you will, multi-die computers for quite some time. And that is 1 GPU.  So when we think about a GPU, we actually think about an HGX GPU, and that's 8 GPUs. We're going to continue to do that. And the thing that the cloud service providers are really excited about is by hosting our infrastructure for NVIDIA to offer because we have so many companies that we work directly with. We're working directly with 10,000 AI start-ups around the world, with enterprises in every industry. And all of those relationships today would really love to be able to deploy both into the cloud at least or into the cloud and on-prem and oftentimes multi-cloud.  And so by having NVIDIA DGX and NVIDIA's infrastructure are full stack in their cloud, we're effectively attracting customers to the CSPs. This is a very, very exciting model for them. And they welcomed us with open arms. And we're going to be the best AI salespeople for the world's clouds. And for the customers, they now have an instantaneous infrastructure that is the most advanced. They have a team of people who are extremely good from the infrastructure to the acceleration software, the NVIDIA AI open operating system, all the way up to AI models. Within 1 entity, they have access to expertise across that entire span. And so this is a great model for customers. It's a great model for CSPs. And it's a great model for us. It lets us really run like the wind. As much as we will continue and continue to advance DGX AI supercomputers, it does take time to build AI supercomputers on-prem. It's hard no matter how you look at it. It takes time no matter how you look at it. And so now we have the ability to really prefetch a lot of that and get customers up and running as fast as possible.
Jensen Huang: I think those numbers are really good anchor still. The difference is because of the, if you will, incredible capabilities and versatility of generative AI and all of the converging breakthroughs that happened towards the middle and the end of last year, we're probably going to arrive at that TAM sooner than later. There's no question that this is a very big moment for the computer industry. Every single platform change, every inflection point in the way that people develop computers happened because it was easier to use, easier to program and more accessible. This happened with the PC revolution. This happened with the Internet revolution. This happened with mobile cloud. Remember, mobile cloud, because of the iPhone and the App Store, 5 million applications and counting emerged. There weren't 5 million mainframe applications. There weren't 5 million workstation applications. There weren't 5 million PC applications. And because it was so easy to develop and deploy amazing applications part cloud, part on the mobile device and so easy to distribute because of app stores, the same exact thing is now happening to AI.  In no computing era did 1 computing platform, ChatGPT, reached 150 million people in 60, 90 days. I mean, this is quite an extraordinary thing. And people are using it to create all kinds of things. And so I think that what you're seeing now is just a torrent of new companies and new applications that are emerging. There's no question this is, in every way, a new computing era. And so I think this -- the TAM that we explained and expressed, it really is even more realizable today and sooner than before.
Jensen Huang: One of the things that, if I could add, Stacy, to say something about the wisdom of what Mercedes is doing. This is the only large luxury brand that has, across the board, from every -- from the entry all the way to the highest end of their luxury cars, to install every single one of them with a rich sensor set, every single one of them with an AI supercomputer, so that every future car in the Mercedes fleet will contribute to an installed base that could be upgradable and forever renewed for customers going forward. If you could just imagine what it looks like if the entire Mercedes fleet that is on the road today were completely programmable, that you can OTA, it would represent tens of millions of Mercedeses that would represent revenue-generating opportunity. And that's the vision that Ola has. And what they're building for, I think, it's going to be extraordinary. The large installed base of luxury cars that will continue to renew with -- for customers' benefits and also for revenue-generating benefits.
Jensen Huang: Yes, Mark, I really appreciate the question. First of all, I have new applications that you don't know about and new workloads that we've never shared that I would like to share with you at GTC. And so that's my hook to come to GTC, and I think you're going to be very surprised and quite delighted by the applications that we're going to talk about.  Now there's a reason why it is the case that you're constantly hearing about new applications. The reason for that is, number one, NVIDIA is a multi-domain accelerated computing platform. It is not completely general purpose like a CPU because a CPU is 95%, 98% control functions and only 2% mathematics, which makes it completely flexible. We're not that way. We're an accelerated computing platform that works with the CPU that offloads the really heavy computing units, things that could be highly, highly paralyzed to offload them. But we're multi-domain. We could do particle systems. We could do fluids. We could do neurons. And we can do computer graphics. We can do . There are all kinds of different applications that we can accelerate, number one.  Number two, our installed base is so large. This is the only accelerated computing platform, the only platform. Literally, the only one that is architecturally compatible across every single cloud from PCs to workstations, gamers to cars to on-prem. Every single computer is architecturally compatible, which means that a developer who developed something special would seek out our platform because they like the reach. They like the universal reach. They like the acceleration, number one. They like the ecosystem of programming tools and the ease of using it and the fact that they have so many people they can reach out to, to help them. There are millions of CUDA experts around the world, software all accelerated, tool all accelerated. And then very importantly, they like the reach. They like the fact that you can see -- they can reach so many users after they develop the software. And it is the reason why we just keep attracting new applications.  And then finally, this is a very important point. Remember, the rate of CPU computing advance has slowed tremendously. And whereas back in the first 30 years of my career, at 10x in performance at about the same power every 5 years and then 10x every 5 years. That rate of continued advance has slowed. At a time when people still have really, really urging applications that they would like to bring to the world, and they can't afford to do that with the power keep going up. Everybody needs to be sustainable. You can't continue to consume power. By accelerating it, we can decrease the amount of power you use for any workload. And so all of these multitude of reasons is really driving people to use accelerated computing, and we keep discovering new exciting applications.
Jensen Huang: First, I'll start backwards. I believe the number of AI infrastructures are going to grow all over the world. And the reason for that is AI, the production of intelligence, is going to be manufacturing. There was a time when people manufacture just physical goods. In the future, there will be -- almost every company will manufacture soft goods. It just happens to be in the form of intelligence. Data comes in. That data center does exactly 1 thing and 1 thing only. It cranks on that data and it produces a new updated model. Where raw material comes in, a building or an infrastructure cranks on it, and something refined or improved comes out that is of great value, that's called the factory. And so I expect to see AI factories all over the world. Some of it will be hosted in cloud. Some of it will be on-prem. There will be some that are large, and there are some that will be mega large, and then there'll be some that are smaller. And so I fully expect that to happen, number one.  Number two. Over the course of the next 10 years, I hope through new chips, new interconnects, new systems, new operating systems, new distributed computing algorithms and new AI algorithms and working with developers coming up with new models, I believe we're going to accelerate AI by another million x. There's a lot of ways for us to do that. And that's one of the reasons why NVIDIA is not just a chip company because the problem we're trying to solve is just too complex. You have to think across the entire stack all the way from the chip, all the way into the data center across the network through the software. And in the mind of 1 single company, we can think across that entire stack. And it's really quite a great playground for computer scientists for that reason because we can innovate across that entire stack. So my expectation is that you're going to see really gigantic breakthroughs in AI models in the next company, the AI platforms in the coming decade. But simultaneously, because of the incredible growth and adoption of this, you're going to see these AI factories everywhere.
Jensen Huang: Thank you. The accumulation of breakthroughs from transformers, large language model and generative AI has elevated the capability and versatility of AI to a remarkable level. A new computing platform has emerged. New companies, new applications and new solutions to long-standing challenges are being invented at an astounding rate. Enterprises in just about every industry are activating to apply generative AI to reimagine their products and businesses. The level of activity around AI, which was already high, has accelerated significantly. This is the moment we've been working towards for over a decade. And we are ready. Our Hopper AI supercomputer with the new transformer engine and Quantum InfiniBand fabric is in full production, and CSPs are racing to open their Hopper cloud services. As we work to meet the strong demand for our GPUs, we look forward to accelerating growth through the year.  Don't miss the upcoming GTC. We have much to tell you about new chips, systems and software, new CUDA applications and customers, new ecosystem partners and a lot more on NVIDIA AI and Omniverse. This will be our best GTC yet. See you there.
Colette Kress: So I'll start and turn it over to Jensen to talk more because I believe this will be a great topic and discussion also at our GTC.  Our plans in terms of software, we continue to see growth even in our Q4 results, we're making quite good progress in both working with our partners, onboarding more partners and increasing our software. You are correct. We've talked about our software revenues being in the hundreds of millions. And we're getting even stronger each day as Q4 was probably a record level in terms of our software levels. But there's more to unpack in terms of there, and I'm going to turn it to Jensen.
Colette Kress: Thanks for the question. First, talking about our data center guidance that we provided for Q1. We do expect a sequential growth in terms of our data center, strong sequential growth. And we are also expecting a growth year-over-year for our data center. We actually expect a great year with our year-over-year growth in data center probably accelerating past Q1.
Colette Kress: Great. Thanks, Stacy, for the question. Let me first start with your question you had about H-100 and A100. We began initial shipments of H-100 back in Q3. It was a great start. Many of them began that process many quarters ago. And this was a time for us to get production level to them in Q3. So Q4 was an important time for us to see a great ramp of H-100 that we saw. What that means is our H-100 was the focus of many of our CSPs within Q4, and they were all wanting to get both get up and running in cloud instances. And so we actually saw less of A100 in Q4 of what we saw in H-100 at a larger amount. We tend to continue to sell both architectures going forward, but just in Q4, it was a strong quarter for  Your additional questions that you had on Mercedes Benz. I'm very pleased with the joint connection that we have with them and the work. We've been working very diligently about getting ready to come to market. And you're right. They did talk about the software opportunity. They talked about their software opportunity in 2 phases, about what they can do with Drive as well as what they can also do with Connect. They extended out to a position of probably about 10 years looking at the opportunity that they see in front of us. So it aligns with what our thoughts are with a long-term partner of that and sharing that revenue over time.
Colette Kress: Sure. Thanks for the question. When we think about our growth, yes, we're going to grow sequentially in Q1 and do expect year-over-year growth in Q1 as well. It will likely accelerate there going forward. So what do we see as the drivers of that? Yes, we have multiple product cycles coming to market. We have H-100 in market now. We are continuing with our new launches as well that are sometimes fueled with our GPU computing with our networking. And then we have grades coming likely in the second half of the year. Additionally, generative AI, it's sparked interest definitely among our customers, whether those be CSPs, whether those be enterprises, one of those be start-ups. We expect that to be a part of our revenue growth this year. And then lastly, let's just not forget that given the end of Moore's Law, there's an error here of focusing on AI, focusing on accelerated continuing. So as the economy improves, this is probably very important to the enterprises and it can be fueled by the existence of cloud first for the enterprises as they [indiscernible]. I'm going to turn it to Jensen to see if has any additional things he'd like to add.
