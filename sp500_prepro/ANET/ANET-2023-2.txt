Liz Stine: Thank you, operator. Good afternoon, everyone, and thank you for joining us. With me on today's call are Jayshree Ullal, Arista Networks’ President and Chief Executive Officer, and Ita Brennan, Arista's Chief Financial Officer. This afternoon, Arista Networks issued a press release announcing the results for its fiscal second quarter ending June 30, 2023. If you would like a copy of the release, you can access it online at our website. During the course of this conference call, Arista Networks management will make forward-looking statements, including those relating to our financial outlook for the third quarter of the 2023 fiscal year, longer term financial outlooks for 2023 and beyond, our total addressable market and strategy for addressing these market opportunities, including AI, customer demand trends, supply chain constraints, component costs, manufacturing output, inventory management, and inflationary pressures on our business, lead times, product innovation, working capital optimization, and the benefits of acquisition, which are subject to the risks and uncertainties that we discuss in detail in our documents filed with the SEC, specifically, in our most recent Form 10-Q and Form 10-K, and which could cause actual results to differ materially from those anticipated by these statements. These forward-looking statements apply as of today and you should not rely on them as representing our views in the future. We undertake no obligation to update these statements after this call. Also, please note that certain financial measures we use on this call are expressed on a non-GAAP basis and have been adjusted to exclude certain charges. We have provided reconciliations of these non-GAAP financial measures to GAAP financial measures in our earnings press release. With that, I will turn the call over to Jayshree.
Liz Stine: Thank you, Ita. We will now move to the Q&A portion of the Arista earnings call. To allow for greater participation, I'd like to request that everyone please limit themselves to a single question. Thank you for your understanding. Operator, take it away.
Liz Stine: Thanks Ben. This concludes the Arista Networks second quarter 2023 earnings call. We have posted a presentation which provides additional information on our results, which you can access on our Investors section of our website. Thank you for joining us today and thank you for your interest in Arista.
Jayshree Ullal: Yeah. Thanks, Mike. I think it's pretty clear that from quarter-over-quarter, our enterprise momentum continues to get stronger and better. And our cloud is strong. However, it's got two components now. There's the classic cloud networking and then the AI. So we're reconciling how we double down more on AI, which we are feeling stronger and stronger about. And even on the cloud, you know that the last two years have just been out of this world and phenomenal. So while it's moderating, it’s still pretty good.
Jayshree Ullal: Okay. Thank you, Tim. Maybe my answer will be shorter than your question. But I think the gist of what I'd like to, first of all, say is majority of Arista's participation has been in the front end of the network. Right? And we're getting a chance for the first time ever to play in the back end. So when we think AI, there's clearly some ramifications of bandwidth on the front end of the network, but we're not counting that. So we're truly thinking of something that's incremental, brand new, lot of work to do in testing, proving, pilots, trials before we get into production. Today, I would say in the back end of the network, there are basically three classes of networks. One is very, very small networks that are within a server where customers use PCIe, CXL. There's proprietary NVIDIA-specific technologies like NVLink that Arista does not participate. Then there's more medium clusters, you can think generative AI mostly and inference, where they may well get built on Ethernet. For the extremely large clusters with large language training models, especially with the advent of ChatGPT-3 and ChatGPT-4, you're not talking about not just billion parameters, but an aggregate of trillion parameters. And this is where Ethernet will shine. But today, the only technology that is available to customers is InfiniBand. So, obviously, InfiniBand with 10, 15 years of familiarity in an HPC environment is often being bundled with the GPUs. But the right long-term technology is Ethernet, which is why I'm so proud of what the Ultra Ethernet Consortium and a number of vendors are doing to make that happen. So near-term, there's going to be a lot of InfiniBand, and Arista will be watching that outside in. But longer term, Arista will be participating in an Ethernet AI network. And, neither technology, I want to say, were perfectly designed for AI. InfiniBand was more focused on HPC and Ethernet was more focused on general purpose networking. So I think the work we are doing with the UEC to improve Ethernet for AI is very important.
Jayshree Ullal: I always [Technical Difficulty] for so long eventually you have to eat. So I think we will see a nice mix of AI and classic cloud networking over time.
Jayshree Ullal: Exactly, Anshul. I think the way to look at our AI opportunity is it's -- there's 10 years ahead of us. And we'll have early customers in the cloud with very large datasets, trialing our Ethernet now. And then we will have more cloud customers, not only Titans, but other high-end Tier 2 cloud providers and enterprises with large datasets that would also trial us over time. So in 2025, I expect to have a large list of customers of which cloud titans will still end up being some of the biggest, but not the only ones.
Jayshree Ullal: Yeah. That's a really good question. Stay tuned for our 2024 guide when we have our Analyst Day sometime in November. But qualitatively speaking, we're a very different company today than three years ago. Clearly, we've doubled down on our cloud titans and you'll that they're getting stronger and stronger. But even in the cloud titans, Anshul and the team have worked to have a number of use cases. It isn't just one. And the addition of AI to that use cases just gave us a whole lot of broad opportunity from front end to back end. Right? So to me, the holistic and seamless and cohesion between the front end and back end will get even more important as time goes on in cloud titans. We also see that we're stronger in Tier 2 providers and, of course, the broader enterprise. Both of these were not as strong for us three years ago, and they also represent AI opportunities, but as you know, they represent campus, routing, classical data center opportunities, and allow us to go target a much larger TAM, again, three years ago, it was probably $30 billion. Three years later, it's well north of $50 billion. So I feel we are much more diversified, and while we deeply appreciate M&M, we got a lot more candy beyond that.
Jayshree Ullal: Tal, I think -- this is Jayshree. I know you asked a difficult question. Look, it’s -- we'll know more as time goes on. And we think the business is strong and whether that comes in strongly in ‘24 or ‘25 or somewhere in between [Technical Difficulty] we shall see. Right? And the reality is it'll be difficult to repeat the last two years of exceptional cloud CapEx for cloud networking. So as they go through that deployment and as they look at AI, and as we bring in the enterprise and Tier 2 clouds, we've got a nice mix of things. And I urge everyone to think of our business as Ita has always alluded to, not in one quarter or even one year, but really a three-year CAGR. And I think our three-year CAGR will continue to be in double digits and good numbers.
Jayshree Ullal: Yeah. That's a really good -- Sebastien. I -- since we have so many products in the mix, I have to break your question into visibility across multiple areas. Enterprise, I would say six to 12 months, generally speaking. In the cloud, given the reduced lead times on classic cloud networking, it's less than six months. However, on AI, it is greater since it's an early cycle and we have to do a lot more joint development. So you can think of it as, three migrations going on with different visibility patterns.
Jayshree Ullal: Look, I think, Samik, this scenario that we feel pretty good about. And it's an area of great execution from Anshul, Chris Schmidt, Ashwin, and the entire team, where we have really diversified our business globally in the enterprise. We're not just in the high end financials. We're in just about every major vertical, healthcare, transportation, public sector, education, banks, insurances. So I feel enterprise, barring any macro issue, which is the thing we were always worry about for 2024. So If macro doesn't let us down and you don't have to worry about the economy, we will have a strong year in enterprise.
Jayshree Ullal: Yeah. I want to give a shout out, Mark Brillhart, our new Senior VP of Manufacturing, and John McCool, both in Anshul's team, have done a fantastic job of optimizing the supply chain. So those improvements are really playing a role in our quarter-to-quarter gross margins.
Jayshree Ullal: Yes. And, James, just to confirm, we expect a more normal setting in 2024 in terms of lead times. You're right to assume that.
Jayshree Ullal: Sure, Simon. Well, of course, we have market share gains. That is a result of our enterprise traction, I would say. But if you ask me why are we winning in the enterprise, I would say, number one, from an alternative perspective, our customers haven't had one for a very long time. They haven't had a high-quality, high supportive, very, very friendly software experience, a common leaf spine architecture across their data center campus routing in a long, long time. So, I think the architectural shift in the enterprise to move into a modern cloud operating model is the number one reason that Arista has been chosen. They are seeking our architecture for that quality experience. In fact, Anshul and I were just talking about the call. We use the word cloudify a lot, and it’s quite tricky right. Our high-end enterprises are really looking for the cloud principles, but however on their premise. In terms of the shift between workloads in the cloud and workloads on the enterprise, it depends on the customer. You're still seeing some of the mid-market customers want to move their e-commerce workloads on the cloud. But a lot of their mission-critical applications stay on the premise. So, our hybrid strategy continues to dominate the enterprise decisions for the data center. Secondly, our entry into the campus and routing as well as zero-trust security, observability, et cetera. is adding more layers to the cake. So, our product depth and breadth is getting better and better. So, the cloud operating model, the product that depth -- and now actually, we've been at it now for, what do you say, Anshul, three years to five years maybe? So, especially, in United States, we're got more work to do internationally. I would say we've been engaging with these customers -- I remember when Ita and I had a discussion, I want to say five years ago, where she was right and I was wrong, and she persuaded me to invest more in the enterprise. So, I think all these things have gone into really making us who we are in the enterprise. And clearly, we are a gold standard and we have a seat at the table there.
Jayshree Ullal: We'll share the legal plan more. But David, rest assured that we are aiming for at least double-digits next year. And so, we'll go from there.
Jayshree Ullal: Sure. So, as you're probably well aware, the white box question has remained with the Arista as one of the most popular questions asked right from the time of our IPO, whether it's a 10 gig, 40 gig, 100 gig, 400 gig, or now you ask it at 800 gig, I think there will always be an element of white box if somebody is just looking to build something and throw in some quick traffic. But for some of these most mission-critical networks, it's less about the box and more about the software stack and how much performance availability, power you really get out of this. So, the cost of putting in the box, if you save something, if you even save something, is far dwarfed by the total OpEx you need to make that box work. So, we continue to believe that we will coexist with white box in some of our cloud titan customers. We will continue to run both SONiC and FBOSS in the case of Microsoft and Meta along with our EOS. But at the end of the day, whether it's a white box or a blue box, it's the software stack that really wins.
Jayshree Ullal: I would say to you that probably our enterprise demand has always been strong and not subdued, far from that, however, dwarfed by the excellence of our cloud performance. You didn't notice it and now you're noticing it.
Anshul Sadana: Sure. Meta, when you look at the cloud customers, in the last few quarters, especially since the advent of ChatGPT, there’s been a rotation into AI. It's not that they're done with the upgrades or this is one of the upgrades, but they had to reprioritize their business and their deployments for AI. You've seen the competitive battle between the largest or the largest titans in the world trying to get ahead. But we see signs of that slowing. And in the future, we believe they'll be back to adding and refreshing the standard computer infrastructure as well.
Anshul Sadana: So this gets asked very often, how we're doing that, and we continue to do well with them. As I mentioned before, not all titans are the same in terms of size. Some are small and we do very well with them, but they're just not as big as our two largest customers. And others who have the potential, we're still doing very well technologically with them but we haven't seen the opportunity materialize. It’s not that we're losing to anybody. It's just nothing has changed. And we continue to invest with them, and we believe the opportunity is still ahead of us.
Anshul Sadana: Aaron, we had the same discussion when the world went to 400 gig, switching from 100 gig to 400gig. The reality was customers continue to buy both, 100 gig and 400 gig for different use cases. 51T and 800 gig, especially, are being puller by AI clusters, the AI teams, are very anxious to get their hands on it, move the data as quickly as possible, and reduce the job completion times. So you'll see early traction there. You'll see, as we mentioned, trial study in ‘24 going into volume in ‘25. And that should be the ramp we'll follow for 800 gig. But that does not mean everything they just bought last few years at 400 gig for DCI or the spines and so on for classic clusters are going to get upgrade to 800 gig. I think that's going to be a longer cycle. So you will see 100, 200, 400, and 800 get deployed in parallel as we enter that cycle in ‘24, ‘25.
Anshul Sadana: Sure. There is no uniform recipe, but in general, when they're buying GPUs, [Technical Difficulty] to connect. These could be off few quarters depending on their timing of deployments till they build the network [Technical Difficulty] very large things. But it takes some -- a couple of months sometimes, a quarter or more, to fine-tune the cluster and benchmark and test everything before it is actually released to production. So, you can think of that as sort of the basis, a couple of months, a couple of quarters minimum before you can get there. Sometimes it adds up to about a year before you really ramp into production.
Anshul Sadana: If I can add one more thing here, what are the milestones to get to these 2025 [large scale] (ph) deployments, there is one key milestone. That has nothing to do with GPUs or our switches, which is does the customer have enough power and the site ready to deploy that many megawatts or gigawatts of capacity. And as you know, getting a 1,500 megawatt site takes a couple of years, which is why this is a floor ramp. This does not certainly turn on the key and you have thousands of GPUs.
Ita Brennan: Yeah. And I think that the lead time improvements have kind of facilitated them waiting for a little bit longer than what we've gotten used to over the last couple of years. But I think, again, that's kind of -- we're going to start coming within lead time here pretty soon, then we’ll see. Yeah.
Ita Brennan: Yeah. I think, look, the lead times are mixed across products. I mean, our goal certainly is to try to get back to, like, a six-month lead time here, maybe the end of the year or certainly early year. But I think it is currently mixed across products. The commentary around customer inventory itself, we've been very diligent all the way through this process, the supply chain process of trying to make sure we understood demand when it showed up and that it was being put into reasonable deployment schedules and deployment plans. And we just want to continue to do that as we come through the other side of really that whole supply chain disruption. So it's really more, understanding kind of what customers need, when they need it and, again, being able to prioritize and make sure that we understand that. So it's really a continuation of what doing, honestly, on the other side of the supply chain when you have these -- when you have this kind of accelerated demand, and then we were very focused on deployment schedules and timing. And this is just the other side of that. Again, making sure we understand what's happening.
Ita Brennan: I mean, look, we haven't talked about backlog and orders. I think we've talked more just in terms of deployments and deployment slots. And, if you think back to the -- to my commentary, I mean, we do believe that there are ongoing deployments that will go well into 2024. Right? So it's not, again, I don't necessarily sign up the terminology of the backlog and the drawdown, et cetera. So given how the orders and the patterns, it's very difficult [stopping that language] (ph). But I think in terms of deployments, you will have deployments that are already planned and scheduled into 2024. I think it will -- we're taking it quarter by quarter through the end of the year, but I'd still go back to my kind of incremental, look at it kind of incrementally quarter-over-quarter and continue to show some improvements as we guide to Q3. So Q4 is, take some similar kind of incremental improvement into Q4, and I think that's the way to think about it for now. But, again, I don't know that it's -- our commentary on kind of demand and lead time stands. Right? I mean, as lead time shorten, you will see some period of time where customers don't need to place orders until you get back into the lead time. And that dynamic is certainly there. And, as we get closer to the end of the year, we'll get more visibility into next year.
Ita Brennan: I mean, we're definitely seeing improvement on the supply chain side. We're seeing improvements with freight, improvements with just some of the expedite costs and the things that we were dealing with and we're kind of inventoried and now we're releasing them. So I think we're coming out from underneath that. There is some small shift in mix as well, but it's still a good strong cloud mix this quarter, this year. So, it's not like we're back to a heavy enterprise mix without creating a much smaller part. There's still a very healthy kind of cloud mix in this year. So it's more where we can back out our -- the supply chain stuff that we'd incurred in the past.
Ita Brennan: Yes. I think it’s absolutely right? And I think that, we kind of forget that cloud is still an important part of 2023. Right? We're still executing on deployments and planning that we did some time back, right, all the way through this year. So cloud is still a significant piece of the of the business in in 2023.
Ita Brennan: So, now you want to go to '25 as well. I don't think we're ready to do that. But that's a really good conversation for the Analyst Day honestly. I think, we obviously, are very focused internally. As Jayshree reiterated earlier on, the business is a lot more robust with many different drivers. As you go through that period, cloud will ebb and flow, but it's still a healthy business. It has been a healthy business through those cycles. So, I think we've got a lot of the building blocks, but how we're going to assemble them, maybe we'll save for the for the Analyst Day.
