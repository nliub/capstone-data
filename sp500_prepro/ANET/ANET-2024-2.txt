Liz Stine: Thank you, operator. Good afternoon, everyone and thank you for joining us. With me on today's call are Jayshree Ullal, Arista Networks Chairperson and Chief Executive Officer; and Chantelle Breithaupt, Arista's Chief Financial Officer. This afternoon, Arista Networks issued a press release announcing the results for its fiscal second quarter ending June 30, 2024. If you would like a copy of this release, you can access it online at our website. During the course of this conference call, Arista Networks management will make forward-looking statements, including those relating to our financial outlook for the third quarter of the 2024 fiscal year, longer-term financial outlooks for 2024 and beyond, our total addressable market and strategy for addressing these market opportunities, including AI, customer demand trends, supply chain constraints, component costs, manufacturing output, inventory management and inflationary pressures on our business, lead times, product innovation, working capital optimization and the benefits of acquisitions which are subject to the risks and uncertainties that we discuss in detail in our documents filed with the SEC, specifically in our most recent Form 10-Q and Form 10-K and which could cause actual results to differ materially from those anticipated by these statements. These forward-looking statements apply as of today and you should not rely on them as representing our views in the future. We undertake no obligation to update these statements after this call. Also, please note that certain financial measures we use on the call are expressed on a non-GAAP basis and have been adjusted to exclude certain charges. We have provided reconciliations of these non-GAAP financial measures to GAAP financial measures in our earnings press release. With that, I will turn the call over to Jayshree.
Liz Stine: Thank you, Chantelle. We will now move to the Q&A portion of the Arista earnings call. To allow for greater participation, I'd like to request that everyone please limit themselves to a single question. Thank you for your understanding. Operator, take it away.
Liz Stine: This concludes the Arista Networks second quarter 2024 earnings call. We have posted a presentation which provides additional information on our results which you can access on the Investors section of our website. Thank you for joining us today and thank you for your interest in Arista.
Jayshree Ullal: Thank you, Liz and thank you, everyone, for joining us this afternoon for our second quarter 2024 earnings call. As a pure-play networking innovator with greater than $70 billion TAM ahead of us, we are pleased with our superior execution this quarter. We delivered revenues of $1.69 billion for the quarter, with a non-GAAP earnings per share of $2.10. Services and Software Support renewals contributed strongly at approximately 17.6% of revenue. Our non-GAAP gross margin of 65.4% was influenced by outstanding manufacturing discipline realizing cost reductions. International contribution for the quarter registered at 19%, with the Americas strong at 81%. As we celebrated our tenth anniversary at the New York Stock Exchange with our near and dear investors and customers, we are now supporting over 10,000 customers with a cumulative of 100 million ports deployed worldwide. In June 2024, we launched Arista's Etherlink AI platforms that are ultra-Ethernet consortium compatible, validating the migration from InfiniBand to Ethernet. This is a rich portfolio of 800-gig products, not just a point product but in fact, a complete portfolio that is both NIC and GPU agnostic. The AI portfolio consists of the 7060 x6 AI [ph] switch that supports 64800 gig or 128 400-gig Ethernet ports with a capacity of 51 terabits per second. The 7800 R4 AI Spine is our fourth generation of Arista's flagship 7800, offering 100% nonblocking throughput with a proven virtual output queuing architecture. The 7800 R4 supports up to 460 terabits in a single chassis, corresponding to 576800 gigabit Ethernet ports or 1,152 400 gigabit port density. The 7700 R4 AI distributed Etherlink Switch is a unique product offering with a massively parallel distributed scheduling and congestion-free traffic spraying fabric. The 7700 represents the first in a new series of ultra-scalable intelligent distributed systems that can deliver the highest consistent throughput for very large AI clusters. Let's just say once again, Arista is making Ethernet great. First, we began this journey with low latency in 2009 time frame. And then there was cloud and routing in the 2015 era, followed by WAN and Campus in the 2020 era and now AI in our fifth generation in 2025 era. Our Etherlink portfolio is in the midst of trials and can support up to 100,000 XTUs in a 2-tier design built on our proven and differentiated extensible OS. We are quite pleased with our progress across Cloud, AI, Campus and Enterprise customers. I would like to invite Ashwin Kohli, our newly appointed Chief Customer Officer, to describe our diverse set of customer wins in 2024. Ashwin, over to you.
Jayshree Ullal: Well, thank you, Ashwin and congratulations. Hot off the press is our new and highest Net Promoter Score of 87 which translates to 95%. Hats off to your team for achieving that. It's so exciting to see the momentum of our enterprise sector. As a matter of fact, as we speak, we are powering the broadcasters of the Olympics, symbolic of our commitment to the media and entertainment vertical. And so it's fair to say that so far in 2024, it's proving to be better than we expected because of our position in the marketplace and because of our best-of-breed platform for mission-critical networking. I am reminded of the 1980s when San [ph] was famous for declaring the network is the computer. Well, 40 years later, we're seeing the same cycle come true again with the collective nature of AI training models mandating a lossless highly available network to seamlessly connect every AI accelerator in the cluster to 1 another for peak job completion times. Our AI networks also connect trained models to end users and other multi-tenant systems in the front-end data center, such as storage, enabling the AI system to become more than the sum of its parts. We believe data centers are evolving to holistic AI centers, where the network is the epicenter of AI management for acceleration of applications, compute, storage and the wide area network. AI centers need a foundational data architecture to deal with the multimodal AI data sets that run on our differentiated EOS network data systems. Arista showcased the technology demonstration of our EOS-based AI agent that can directly connect on the NIC itself or alternatively, inside the host. By connecting into adjacent Arista switches to continuously keep up with the current state, send telemetry or receive configuration updates, we have demonstrated the network working holistically with network interface cards such as NVIDIA BlueField [ph] and we expect to add more NICs in the future. Well, I think the Arista purpose and vision is clearly driving our customer traction. Our networking platforms are becoming the epicenter of all digital transactions, be they campus center, data center, plan centers or AI centers. And with that, I'd like to turn it over to Chantelle, our Chief Financial Officer, to review the financial specifics and tell us more. Over to you, Chantelle.
Jayshree Ullal: Sure. Michael, I think as the GPUs get faster and faster, obviously, the dependency on the network for higher throughput is clearly related. And therefore, our timely introduction of these 800-gig products will be required, especially more for Blackwell. In terms of its connection and modularity with NVLink and 72 port [ph], there's always been a market for what I call scale up, where you're connecting the GPUs internally in a server and the density of those GPUs connecting in the past has been more PCIe and cell and now NVLink and there's a new consortium now for called UAL that's going to specify that, I believe, eventually, by the way, even there, Ethernet will win. And so that density depends more on the AI accelerator and how they choose to connect. As I've often said, it's more a bus technology. So eventually, where Arista plays strongly both on the front end and back end is on the scale out, not on the scale up. So independent of the modularity, whether it's a wrap-based design, a chassis or multiple RU, the ports have to come out Ethernet and those Ethernet ports will connect into scale-out switches from Arista.
Jayshree Ullal: Yes, I get asked that question a lot. First of all, the TAM is far greater than the $750 million we've signed up for. And remember, that's early years. But that can consist of our data center TAM. Our AI TAM which we count in a more narrow fashion as how much of InfiniBand will move to Ethernet on the back end. We don't count the AI TAM that's already in the front end which is part and parcel of our data center. And then obviously, there's a campus TAM which is very, very big. It's north of $10 billion. And then there's the wide area in routing. So these 4 are the building blocks that I call the campus center, data center, AI center and ran center [ph]. And then laid upon that is some very nice software. If you saw, we had a nice bump in Software and Service renewals this quarter which would be largely centered around CloudVision observability and security. So I would say these are the 4 building blocks and then the 3 software components on top of it. Not -- of course, not to forget the services and support that are part of these TAMs as well.
Jayshree Ullal: We saw that last year. We saw that there was a lot of pivot going on from the classic cloud, as I like to call it, to the AI in terms of spend. And we continue to see favorable preferences to AI spend in many of our large cloud customers. Having said that, at the same time, simultaneously, we are going through a refresh cycle where many of these customers are moving from 100 to 200 or 200 to 400 gig. So while we think AI will grow faster than cloud, we're betting on classic cloud continuing to be an important aspect of our contributions.
Jayshree Ullal: Look at us. We're known to be traditionally conservative. We went from 10 to 12, 12 to 14. And now my CFO says at least 14. So let's see how the second half goes. But I think at this point, you should think we are confident about second half and we're getting increasingly confident about 2025.
Jayshree Ullal: So Tal, if you go back to November Analyst Day, to call out gross margin lower, I would disagree because I think we're just blowing it off our guide. Our guide was 63 to 64. And we have now shown 2 quarters of amazing gross margin. Hats off to my Mike Capas [ph] and John McCool, Alex and the entire team for really working on disciplined cost reductions. But yet, if you look at mix, in general, the costs and et cetera, I would say you should plan on our gross margins being as we projected. They're not lower. I think we just did exceptionally well the last 2 quarters, so it's relatively lower. That's the first thing. Second, in terms of growth, I would say we always aim for double-digit growth. We came in with 10% to 12%. And again, Q2 is just an outstanding quarter. I don't want to use it as a benchmark for how Q3, Q4 will be. But of course, we're operating off large numbers. We'll aim to do better but we'll have more visibility as we go into Q3 and we'll be able to give you a good sense of the year.
Jayshree Ullal: Yes. I was just going to add that -- there are 4 things Ashwin and the team are seeing in the enterprise and provider sector. I think the migration to 100 gig data center is pretty solidly going on. If anybody is still on a 10 and 40, they're definitely not a early adopter of technology. And some of them are even moving to 400 gig, I would say.
Jayshree Ullal: So that's on the data center. Campus, I know, in general, is a slow market. But for Arista, we are still seeing a lot of desire. And you heard Ashwin talk about a campus win but they're really frustrated and they're struggling with existing campus deployments. So we feel really good about our $750 million target for next year. The routed ran, again, we're both in Tier 2 and service providers and even in enterprise, there's a lot of activity going on there. And finally, the AI trials you talked about, they tend to be smaller but it's a representation of the confidence the customer has. They may be using other GPUs, servers, et cetera. But when it comes to the mission critical networks, they've recognized the importance of best-of-breed reliability, availability, performance, no loss and the familiarity with the data center is naturally leading to pilots and trials on the AI side with us.
Jayshree Ullal: Okay. So perhaps it may come as a surprise to you but servers aren't always related to campus. Devices and users are much more related to campus, right? Service tend to be dealing with more data center upgrades. So in the campus, we're tending to see 2 things right now. Greenfield buildings that are -- they're planning for '25, '26 and they're smack in the middle of those RFPs. Or they're trying to create a little oasis in the desert and prove that our post-pandemic campus is much better with a Leaf/Spine topology, wired/wireless connecting to it at leaf and then enabling things like zero-touch automation, method segmentation, capabilities, analytics, et cetera. So the campus is really turning out to in a somewhat sluggish overall market. We are finding that our customers are very interested in modernizing their campus. And again, it has a lot to do with the familiarity with us in the data center and that's translating to more success in the campus.
Jayshree Ullal: Well, I think there are a lot of market studies that point to today is still largely InfiniBand. You remember me, Ben, saying we were outside looking in just a year ago. So step 1 is we're feeling very gratified that the whole world, even InfiniBand players have acknowledged that we are making Ethernet great again. And so I expect more and more of that back end to be Ethernet. One thing I do expect, even though we're very signed up to the $750 million number -- at least $750 million, I should say, next year, is it's going to become difficult to distinguish the back end from the front end when they all move to Ethernet. For this AI center, as we call it, is going to be a conglomeration of both the front and the back. So if I were to fast forward 3, 4 years from now, I think the AI center is a supercenter of both the front end and the back end. So we'll be able to track it as long as there's GPUs and strictly training use cases. But if I were to fast forward, I think there may be many more edge use cases, many more inference use cases and many more small-scale training use cases which will make that distinction difficult to make.
Jayshree Ullal: Alex, it's a very thought provoking question. I would say there's such a heavy bias towards -- in the Cloud Titans towards training and super training and the bigger and better the GPUs, the billion parameters, the OpenAI, ChatGPT and LAMAs [ph] that you're absolutely right that at some level, the classic cloud, what you call traditional, I'm still calling classic, is a little bit neglected last year and this year. Having said that, I think once the training models are established, I believe this will come back and it will sort of be a vicious cycle that feeds on each other. But at the moment, we're seeing more activity on the AI and more moderate activity on the cloud.
Jayshree Ullal: I think it does. I don't know -- I don't know how to measure it but I think the more AI back end we put in, we expect that to have a pressure on the front end of x percent. We're still trying to assess whether that's 10%, 20%, 30%. And so we do not count that in our $750 million number, to be accurate, to the only GPU native connections. But absolutely, as the 2 holistically come together to form this AI center, I believe the front end will have pressure.
Jayshree Ullal: Yes. Well, first, I just want to say, when you say competitive environment, it's complicated with NVIDIA because we really consider them a friend on the GPUs as well as the mix, so not quite a competitor. But absolutely, we will compete with them on the Spectrum switch. We have not seen the Spectrum except in 1 customer where it was bundled. But otherwise, we feel pretty good about our win rate and our success. For a number of reasons, great software, portfolio products and architecture has proven performance, visibility features, management capabilities, high availability. And so I think it's fair to say that if a customer were bundling with their GPUs, then we wouldn't see it. If a customer were looking for best of breed, we absolutely see it and win it.
Jayshree Ullal: And if you look at -- this isn't just this quarter. John and the team have done a fantastic job for the last year. So yes, I'm going to go back to them and ask for more but I think they will say they've done so much and might be squeezing blood out of a stone. So we'll see if they respond to more cost reductions. But I absolutely agree with Chantelle. It's largely driven by mix. And I think we've taken a lot of the cost reductions out in the last year.
Jayshree Ullal: I mean, I'll try but you're right to say that we don't know. It's only half the year. I expect both Microsoft and Meta to be greater than 10% customers for us. I don't expect any other 10% concentration. Now in Microsoft and Meta, how they will pivot to AI and how they will reduce the spend and all that things, that movie will play out in the next 6 months. So we'll know better. But at this point, I think you can assume they won't be exactly the same. Some may go up, some may go down. But these are 2 extremely vital customers, strategic customers, we co-develop with them. We partner with them very, very well. And we expect to do well with them, both in cloud and AI, depending on their priorities, of course.
Jayshree Ullal: Yes. Thank you. Thank you for the change in question. I appreciate that. I think we will be in the teens for some time because there are 3 building blocks and a lot of this, Ashwin, you know very well. There's the services building block that as product goes up, there's a lot of pressure on us on a percentage of service to be lower, right? So that, while it may -- historically has been in the teens, can be lower. Then the second one is our perpetual software which, again, is a strong function of use cases, particularly things like routing, et cetera, where we've done very, very well. The stronger we do there, the better we do there. An extension of that is CloudVision which can be -- is more a subscription service with CloudVision as a service, either Network-as-a-Service or on the premise. So that's the second building block for us that's going strong. The one I want to point to a little more which could help us is the security [indiscernible]. You may know in May, we introduced the micro and macro segmentation. We also announced UNO, our Unified Network Observability. And while this is new, Ashwin and I have great plans for that and I think this could be a swing factor. As the services components may reduce over time, these new product components may increase. So 17 points, whatever percentage was is a great number. And if we can consistently stay there, I'd be very proud, particularly as our numbers get larger.
Jayshree Ullal: Yes, so if you look at an AI network design, you can look at it through 2 lenses, just through the compute, in which case you look at scale up and you look at it strictly through how many processes there are. But when we look at an AI network design, it's a number of GPUs or XTUs per workload. Distribution and location of these GPUs are important. And whether the cluster has multiple tenants and how it's divvied up between the host, the memory, the storage and the wide area plays a role and the optimization to make on the applications for the collective communication libraries for specific workloads, levels of resilience, how much redundancy you want to put in, active, link base, load balancing, types of visibility. So the metrics are just getting more and more. There are many more commutations in combination. But it all starts with number of GPUs, performance and billions of parameters. Because the training models are definitely centered around job completion time. But then there's multiple concentric circles of additional things we have to add to that network design. All this to say, a network design-centric approach has to be taken for these GPU clusters. Otherwise, you end up being very siloed and that's really what we're working on. So it goes beyond scale and performance to some of these other metrics I mentioned.
Jayshree Ullal: Yes. Let me just remind you of how we are approaching 2024, including Q4, right? Last year, trials. So small, it was not material. This year, we're definitely going into pilots. Some of the GPUs and you've seen this in public blogs published by some of our customers have already gone from tens of thousands to 24,000 and are heading towards 50,000 GPUs. Next year, I think there will be many of them heading into tens of thousands aiming for 100,000 GPUs. So I see next year as more promising. Some of them might happen this year. But I think we're very much in -- going from trials to pilots, trials being hundreds. And this year, we're in the thousands. But I wouldn't focus on Q4. I'd focus on the entire year and say, yes, we've gone into the thousands. And I'll let Chantelle turn for this glide path, right? So we expect to be single-digit small percentages of our total revenue in AI this year. But we are really, really expecting next year to be the $750 million a year or more.
Jayshree Ullal: Yes. I think and simple things like power and cooling are affecting the ability to deploy in massive scale. So there's nothing magic about Q4. There's plenty of magic about the year in its entirety and next year.
Ashwin Kohli: Many thanks, Jayshree. Thank you for inviting me to my first earnings call. Let me walk everybody through the 4 global customer wins. The first example is an AI enterprise win with a large Tier 2 cloud provider which has been heavily investing in GPUs to increase their revenue and penetrate new markets. Their senior leadership wanted to be less reliant on traditional core services and work with Arista on new, reliable and scalable Ethernet fabrics. Their environment consisted of new NVIDIA H100s [ph]. However, it was being connected to their legacy networking vendor which resulted in them having significant performance and scale issues with their AI applications. The goal of our customer engagement was to refresh the front-end network to alleviate these issues. Our technical partnership resulted in deploying a 2-step migration path to alleviate the current issues using 400-gig 7080s, eventually migrating them to an 800-gig AI Ethernet link in the future. The second next win highlights our adjacencies in both campus and routing. This customer is a large data center customer which has deployed us for almost a decade. The team was able to leverage that success to help them demonstrate our value for the global campus network which spans across hundreds and thousands of square feet globally. The customer had considerable dissatisfaction with a current vendor which led them to a last-minute request to create a design for their new corporate headquarters. Given only 3 months' window, Arista leveraged the existing data center design and adapted this to the campus topology with a digital twin of the design in minimal time. CloudVision was used for visibility and life cycle management. The same customer once again was struggling with extreme complexity in their routing environment, as well with multiple parallel backbones and numerous technical complexities. Arista simplified their routing network by removing legacy routers, increasing bandwidth and moving to a simple fixed form factor platform router. The core spine leverages the same U.S. software, streamlining their certification procedures and instilling confidence in the stability of the products. Once again, CloudVision came to the rescue. The third example is the next win in the international arena of a large automotive manufacturer that due to its size and scale, previously had more than 3 different vendors in the data center which created a very high level of complexity both from a technical and also from an operational perspective. The customer's key priority was to achieve a higher level of consistency across their infrastructure which is now being delivered via a single U.S. binary image and CloudVision solution from Arista. Their next top priority was to use automation, consistent end-to-end provisioning and visibility which can be delivered by our CloudVision platform. This simplification has led the customer to adopt Arista beyond the data center and extend the Arista solution into the routing component of the infrastructure which included our 7500 R3 spine platforms. This once again shows a very clear example of the same Arista 1 EOS and One CloudVision solution delivering multiple use cases. And Jayshree, this last win demonstrates our strength in service provider routing space. We have been at the forefront of providing innovative solutions for service provider customers for many years. As we all know, we are in the midst of an optical and packet integration. As a result, our routers support industry-leading dense 400-gig Zero Plus [ph] coherent pluggable optics. In this service provider customer example, we provided a full turnkey solution, including our popular 7280 R3 routers and our newly announced AWE 7250 WAN router as a BGP route reflector along with CloudVision and professional services. We showcased our strength in supporting a wide variety of these pluggable coherent optics, along with our SR and EVP and solutions which allowed this middle mine service provider customer to build out a 400-gig state-wide backbone at cloud scale economics. Thanks, Jayshree and back over to you.
Ashwin Kohli: Yes. Absolutely, Jayshree. Meta, so I just wanted to clarify the example that I shared with you was more around a Tier 2 cloud provider. And if I take a step back, the types of conversations my team is having with customers is either around general-purpose enterprise customers or it's around Tier 2 cloud providers, right which are different to the ones Jayshree's referring to.
Ashwin Kohli: Yes. Sure, Jayshree, right? And so to answer your question, from what I'm seeing from customers, they're kind of fed up with being deployed in the data center, specifically, something which is proprietary lock-in, right? Something which is -- does not give them the flexibility to do -- join multiple use cases such as data center, campus, routing and they want something that just works. They want something that is just simple. They want to make sure that when they wake up in the morning, the network is not down. And so Arista today actually has a brand. It has a value for there. And we've actually been delivering this for the last 10-plus years. So James, I would say the message is echoing successfully in our existing customers who are taking Arista not only in the single use case of data center but expanding that across data center, campus, routing, WAN. And then the team is eagerly working with a new set of Global 2000 and Fortune 500 customers to go evangelize a message to them as well.
Chantelle Breithaupt: Yes, thank you, Jayshree. The only thing I would add to that is that the deferred balance is always a mix of customers and use cases. So I wouldn't rotate on any 1 particular intersection of those. It really is a mix of those combined.
Chantelle Breithaupt: Yes. I think that Jayshree and I came to this guide of at least 14% because we do see multiple scenarios as we go through the second half of the year. We do expect to continue to see some acceleration in growth but I would say that from the perspective of the forward scenarios, we are comfortable with at least 14% and we'll come back in Q3 and see where we were able to guide for the rest of the year.
Chantelle Breithaupt: Yes. Yes, nothing's changed in our philosophy that we don't guide product deferred revenue. So that's -- there's nothing new there to report. I would say in the sense of coming to this quarter, we -- we don't guide the product deferred revenue. We have an idea in the sense of where we're going to land as we go through the quarter. But I would say that it met expectations from what we were having in our planning forecast process.
Chantelle Breithaupt: Yes. They all have different timings because they're unique to the customer, the use case, AI, classic cloud, et cetera. So they're all unique and bespoke that way. So there's no set trending on that. And so as we roll through the quarters, they'll come off as they get deployed and -- and then that's where we'll land from a forecasting perspective.
Chantelle Breithaupt: Yes. No, it's a great question. And so our goal is always to try to do better than our guide. But within that guide, the question is there's -- mostly what's influencing the second half consideration is the expected mix of customers. As you can appreciate, we do have different mixes depending on the demographics of who we're selling to. So I think that's a big part of our second half, why we kept the guide as it is. We will keep looking for variable cost productivity in cost management as we go through and hope to deliver more. But at this point in time, what's mostly built into it is the mix assumption in the second half.
Chantelle Breithaupt: Yes. Yes. I think so. I completely agree, Jayshree. The only thing I would add to it is you have to think of the kind of the matrix we're working within. So we have cloud and enterprise customers and we have very, very different scopes of readiness at those customers. So Q3, Q4, Q1, Q2 next year, they're all eligible for timing for types, et cetera. So we just want to make sure that variability that I spoke to in my prepared remarks is understood in that context.
