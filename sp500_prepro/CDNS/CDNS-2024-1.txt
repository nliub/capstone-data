John Wall: Thanks, Anirudh, and good afternoon, everyone. I am pleased to report that Cadence delivered strong results for the first quarter of 2024. First quarter bookings were a record for Q1 and we achieved record Q1 backlog of approximately $6 billion. A good start to the year coupled with some impressive new product launches, sets us up for strong growth momentum in the second half of 2024. Here are some of the financial highlights from the first quarter starting with the P&L. Total revenue was $1.009 billion. GAAP operating margin was 24.8% and non-GAAP operating margin was 37.8%. GAAP EPS was $0.91 and non-GAAP EPS was $1.17. Next, turning to the balance sheet and cash flow, cash balance at quarter end was [$1.012 billion] (ph). While the principal value of debt outstanding was $650 million. Operating cash flow was $253 million. DSOs were 36 days and we used $125 million to repurchase Cadence shares in Q1. Before I provide our updated outlook, I'd like to share some assumptions that are embedded in our outlook. Given the recent launch of our new hardware systems, we expect the shape of hardware revenue in 2024 to weigh more toward the second half, as our team works to build inventory of the new system. Our updated outlook does not include the impact of our [pending] (ph) BETA CAE acquisition and it contains the usual assumption that export control regulations that exist today remain substantially similar for the remainder of the year. Our updated outlook for fiscal 2024 is revenue in the range of $4.56 billion to $4.62 billion. GAAP operating margin in the range of 31% to 32%. Non-GAAP operating margin in the range of 42% to 43%. GAAP EPS in the range of $4.04 to $4.14. Non-GAAP EPS in the range of $5.88 to $5.98. Operating cash flow in the range of $1.35 billion to $1.45 billion. And we expect to use at least 50% of our annual free cash flow to repurchase Cadence shares. With that in mind, for Q2 we expect revenue in the range of $1,030 million to $1,050 million. GAAP operating margin in the range of 26.5% to 27.5%. Non-GAAP operating margin in the range of 38.5% to 39.5%. GAAP EPS in the range of $0.73 to $0.77. Non-GAAP EPS in the range of $1.20 to $1.24. And as usual, we've published a CFO commentary document on our investor relations website, which includes our outlook for additional items, as well as further analysis and GAAP to non-GAAP reconciliations. In summary, Cadence continues to lead with innovation and is on track for a strong 2024 as we execute to our intelligent system design strategy. I'd like to close by thanking our customers, partners, and our employees for their continued support. And with that operator, we will now take questions.
John Wall: Hi Charles, that's a great observation. If you recall this time last year we were talking about a very strong Q1 for China for functional verification and for upfront revenue. I think those three things are often linked. You contracted with this year, China is down at 12%. Upfront revenues is lower at 10% compared to 20%. And functional verification, of course, is lapping those really tough comps when we dedicated 100% production to deliveries. I think when you look at China, we're blessed that we have the geographical diversification that we have across our business. But -- what we're seeing in China is strong design activity. And while the percentage of revenue dropped to 12%, it pretty much goes in-line with a lower hardware, lower functional verification, lower upfront revenue quarter would generally lead to a lower China percentage quarter. But we have good diversification. While China is coming down, we can see other Asia increasing and our customer base is really mobile. That geographical mix of revenue is based on consumption and where the products are used. But as we do more upfront revenue in the second half, we'd expect the China percentage to increase.
John Wall: Yeah, thanks Ruben. I mean Q1 IP performance and bookings were ahead of our expectations. And everything remains on track there for a very strong growth year for 2024 for the IP business. Of course, the timing of revenue recognition depends on the timing of deliveries, but we had a tremendous bookings quarter in Q1 and we're preparing to scale for a number of deliveries of IP in the second half, but we expect the IP to have a very strong year this year. We're pleased with the overall business momentum, but we need to scale up some headcounts to prepare to deliver on some of the larger backlog orders.
John Wall: Hi, Gary, thanks for the question. And just to clarify, I think last quarter, I said I expected China revenue to be flat to down this year. I think, we still expect that. And that's because last year was such a strong year and there was a lot of -- there was kind of an oversized-portion of that hardware catch-up that we had that was delivered to China. So I think, it skewed the China number higher last year. So we are lapping pretty tough comps. But the design activity in China remains very strong, though. And -- we have a lot of diversification. There is strength in other parts of the world -- but we're very comfortable with the 2024 outlook and we factored all the impact of geopolitical risk in there to the best we can and try to derisk China, as much as we can in our guide.
John Wall: Yes. I mean, it's hard to predict in terms of Z3 and X3 that we definitely need another quarter to see that. I expect -- we expect strong demand and we expect strong revenue growth into the -- that we are preparing for scale into the second half on the hardware side, but we need to at least see another quarter of demand. And normally with hardware, I don't like taking up the year for hardware until I see the pipeline in the summer. So we are trying to be conservative there. But generally on the hardware side, yes we are basically preparing for scale we’re trying to build -- we'll build those systems as quickly as possible. We expect strong demand there.
John Wall: Yeah, we try to derisk the guide -- with the assumption that there is going to be strong demand for the newer systems. But it will give us the opportunity to put some of the older systems into the cloud because we have a large underserved community that want to use our emulation capacity. But we haven't had a lot of capacity to share with them through our cloud offering. To the extent we do that, that will lead to ratable revenue though, because I think when it is used in the cloud, you get revenue over time, whereas when we deliver and they use it on-prem, we take revenue upfront.
John Wall: Well, the guide -- we've de-risk the guide on the assumption that many customers might wait. But we intend to sell them side by side. But to the extent the customers wait, it will shift some hardware revenue into the second half of the year. And we have anticipated that. So that's within the guide. To the extent the customers continue to buy Z2. And we're not putting those into the cloud, but selling those outright as well. Well, then that will change the profile of the shape of revenue. But we expect that this new system, the strength of this new system will trigger a lot of demand for it.
John Wall: Yes. And Vivek if I could -- I'd like to kind of take -- carry in some of Gary's question earlier that I don't think I addressed because he was asking about the bookings profile for the year. Q2 for software renewals, I think is our [latest] (ph) software renewals quarter for the year. But I think, we explained last quarter that we expect the weighting of bookings first half to second half to be about 40-60 this year. But the recurring revenue right now in the guide is about double digits -- above 10%. And in the past, it's been about 13%. Now we are not really anticipating a huge number of add-ons, but to grow that above 10%. To the extent that, that comes through, it will be upside to the guide. But what we try to do when we do the guide is de-risk for the risks that we can see.
John Wall: Yes, Vivek, I think what you are referring to really is that, I mean, for what, seven years in a row now, we think we've been achieving over 50% incremental margins. It's a matter of pride here, we try to achieve that every year -- we'll certainly be trying to achieve that this year. I think we are in the high 40s. It's probably about 47% when you look at this guide right now. I think, one of the biggest challenges with something like that is you know, we do small tuck-in M&A, but I don't want to go over Lee Simpson – answer Anirudh gave to Lee Simpson, but organic is delicious here. At Cadence, we focus on innovation and growing with organically driven products and then with small tuck-in M&A. But to the extent that we do some larger M&A and of course, we have BETA CAE, which apparently is the gold standard in structural simulation. So that's a big acquisition for us. But -- now I think the size of that probably still qualifies as a small tuck-in. But when you do something like that -- that those M&A transactions typically are headwinds to that incremental margin calculation in the short-term, they will be beneficial in the long-term. But in the short-term, M&A can be -- dilutive pretty much in the first year and then becomes accretive later. When we look at our incremental margin that's a headwind. But we try to overcome that headwind because normally, all we do with these small tuck-in M&As So I haven't given up on 50% incremental margin for this year. It's a challenge, but we'll do our best to achieve this.
John Wall: Yes, Harlan. That's a great question. And thanks for highlighting that -- because I had that on my list of things to say. I think there's something funny going on with the rounding on when you kind of apply the growth rates for SDA for Q1 over Q1, the actual growth rate is probably high single digits Q1-over-Q1. I know, that's lapping tough comps against Q1 '23. I think, if you look on a two year CAGR basis, I think it's up about 17% per annum on a two year CAGR basis for SDA. But we're expecting strong SD&A growth again this year, and it will be higher than the Cadence average. That's our expectation.
Anirudh Devgan: Yeah, absolutely. So we are very proud of the new systems we launched. As you know, we are a leader in hardware-based emulations with Z2 X2. And last time we launched them was in 2021. So that was like a six-year cycle. You know Z1 X1 was 2015 and then Z2 X2 was 2021. So what I'm particularly pleased about is, we have a major, major refresh, you know, it's a game-changing product, but it was also developed in only three years. So in 2024, we have a new refresh and it's a significant leap in terms of capacity. And even last week at our CadenceLIVE conference, Nvidia and Jensen talked about how they use Z2 to design their latest chip like Blackwell. And it's also used by all the major silicon companies and system companies to design their chips. But what is truly exciting about Z3 and X3 is this a big leap, it’s like Z3 is 4 times or 5 times more capacity than Z2. It's a much higher performance. So it sets us up nicely for next several years to be able to design the several generations of the world's largest chips. So that's the right thing to do. And the reason we can do it in three years versus six years is, we use our own design internally in Cadence for TSMC advanced node. So we're using all our latest tools, all the latest AI tools, we are using all our IP. There's a very good validation of our own capabilities that we can accelerate our design process, but really sets up hardware verification and overall verification flow for using the new systems. Now, as a result, normally there is a transition period when you have a new system and we went through that twice already in the last 10 years. And the customers naturally will go to the new systems and then we build them over next one or two quarters. But that is the right thing to do for the business long-term. The time -- it's good to accelerate the -- because these AI chips are getting bigger and bigger, right? So the demand for emulation is getting bigger and bigger, and I can give you more stats later. So we felt that it was important to accelerate the development of the next generation system, to get ready for this coming AI wave for next several years, and we are very well positioned. As a result, it does have some impact on quarter-to-quarter, but that's well worth it in the long run.
Anirudh Devgan: Yeah, that's the correct observation. You know, like as you know, what we have said before, AI has a lot of profound impact to Cadence, a lot of benefit to our customers. So there are three main areas. One is, you know, the build out of the AI infrastructure, whether it's Nvidia or AMD or all the hyperscalers. And we are fortunate to be working with all the leading AI companies. So that's the first part. And in that part, as they design bigger and bigger chips, because the big thing in AI systems is they are parallel. So they need to be bigger and bigger chips. So the tools have to be more efficient, the hardware platform have to support that. And that's why the new systems. Now, the second part of AI is applying AI to our own products, which is the Cadence.AI portfolio. And like you mentioned last week, we had several customers talking about success, you know, with that portfolio, including Intel, you know, like I mentioned Intel, Broadcom, Qualcomm, Juniper, Arm, and the results are significant. So we are no longer in kind of a trial phase of whether these things will work. Now we're getting pretty significant improvements. Like we mentioned, MediaTek got like 6% power improvement. And one of the hyperscale companies got 8% to 10% power improvement. These are significant numbers. So it is leading to deployment of our AI portfolio. And I think we mentioned like the AI run rate on a trailing 12 months basis is up 3x. And I think design process already was well automated. EDA has a history of automating design over the last 30 years. So AI is in a unique position because you need the base process to be somewhat automated to apply AI. So we were already well automated and now AI can take it to the next level of automation. So that's the second part of AI which I'm pretty pleased about, is applying to our own product. And then the third part of AI proliferation is new markets that open up, which things like data center design with reality that we announced or Millennium, which is designing systems with acceleration or digital biology. Those are like a little, they take a little longer to ramp up, but we have these three kinds of impact of AI. The first being direct design of AI chips and systems. Second, applying AI to our own products. And third being new applications of AI.
Anirudh Devgan: Charles, all good observations. So let me try to answer that one by one. So, I mean, in terms of your last point, normally if the system has more capacity like this one has, it can do more. So it produces, it gives more value to our customers. So we are able to get more value back. So typically newer systems are better that way for us and better for the customer. And to give you an example, I mean, these things are pretty complicated. So, we'll just take Z3 for example. So Z3 itself, we designed this advanced TSMC chip by ourselves and this is one of the biggest chips that TSMC makes. And one rack will have like, more than a hundred of these chips. And then we can connect like up to 16 racks together. So if you do that, you have thousands of full radical chips emulating -- that's, and these are all liquid cooled connected by optical and InfiniBand interconnect. So this is like a truly a multi rack supercomputer. And what it can do is just emulate very, very large systems very, very efficiently. So even Z2, like Nvidia talked about it last week, even Blackwell, which is the biggest chip in the world right now with 200 billion transistors, was emulated on few racks of Z2. Okay, so now with 16 rack of Z3, we can emulate chips which are like 5 times bigger than Blackwell, which is already the biggest chips in the world, right? So that gives a lot of runway for our customers because with AI, the key thing is that is the capacity of the chip needs to keep going up, not just a single chip. Look at Blackwell, they have two full radical chips on a package. So as you know, you will see more and more, not just big chips on a single node, but multiple chips in a package for this AI workload and also 3D stacking of those chips. So what this allows is not just emulating a single large chip, but multiple chips, which is super critical for AI. So I think this is what I feel that this puts us in a very good position for all this AI boom that is happening, not just with our partners like Nvidia and AMD, but also all the hyperscalers companies. And so that will be the primary demand is more capacity chips require more hardware. And then X3 will go for that with the software prototyping which is used on FPGA. And then we have some unique workload capabilities apart from size of these big systems being, the capacity being much better and performance, there are new features for low power, for analog emulation that helps in the mobile market. So we talked about Samsung, working with us, especially on this four state emulation, which is a new capability in emulation over the last 10 years. So I think it's just -- it's a combination of new customers, a combination of competitive win, but also continuing to lead in terms of the biggest chips in the world which are required for AI processing now and you know years from now. I think the size of these chips as you know is only going to get bigger in the next few years and we feel that Z3 X3 is already set up for that.
Anirudh Devgan: Yeah, thank you for the question. I mean, we are proud to have a very strong partnership with Arm and with our joint customers, Arm and Cadence customers. I think we have had a very strong partnership over the last 10 years, I would like to say, and it's getting better and better. You know, and yes, we talked about our new partnership on Total Compute. Also, I think this quarter we talked about our partnership with HARMAN Automotive. Because what is interesting to see, which of course you know this already, but Arm continues to do well in mobile, but also now in kind of HPC server and automotive end markets. So we are pleased with that partnership, you know, and they are also doing more subsystems and higher order development and that requires more partnership with Cadence in terms of the backend, Innovus and Digital Flow and also verification with hardware platforms and other verification tools.
Anirudh Devgan: Well, thank you for the question. And a lot of times there are a lot of reports and we don't normally don't comment on these reports and people get very creative on these reporting. But What I would like to say is that our strategy hasn't changed. It's the same strategy from 2018. First of all, I want to make sure that we are focused in our core business, which is EDA and IP. And, yes, I launched this whole initiative on systems and it's super critical, you know, chips silicon to systems. But what is one thing that I even mentioned last time, what is different from 2018 to now, is that EDA and IP is much more valuable to the industry. You know, Our core business itself has become much more valuable because of AI. So our first focus is in our core business. We are leading in our core business. Our first focus is on organic development. That's what we like. We always say that's the best way forward. Now, along with that, we will do some, we have done, like you mentioned, some opportunistic M&A, which is usually, I would like to say, the tuck-in M&A in the past. And that adds to our portfolio, it helped us in system analysis. We also did it in IP because I'm very optimistic about IP growth this year. And we talked about our new partnership with Intel Foundry in Q1. Also, we acquired Rambus IP assets, which are HBM. And HBM is of course a critical technology in AI. And we are seeing a lot of growth in HBM this year. Now, if we have booked that business, the deliveries will happen towards second half of the year, as John was saying earlier. But so that's the thing. Now in terms of BETA, it made sense because it is a very good technology. It's the right size for us. And we are focused on finishing that acquisition, and also integrating that -- that will take some time. So that's our primary focus in terms of M&A. And it's a very good technology. They have very good footprint in automotive and aerospace vertical. So just to clarify, we have the same strategy from ‘18, and that's doing working as well. It's primarily organic with very synergistic computational software, mostly tuck-in acquisitions.
Anirudh Devgan: Yes, absolutely. I think I started this in, I forget now, 2016, I think, in a Dynamic Duo are ‘15 and ‘16, which is we have a custom processor for palladium and we use FPGA for Protium. So this is what we call dynamic duo, because then palladium is best in class for chip verification and RTL design, and Protium is best-in-class for software bring up and with the common front end. So as a result, over the years, this has become the right approach. And our customers are fully embracing both these systems as they invariably do both chip development and software development. I mean, perfect example is of course, our long-term development partner, Nvidia. I mean, Nvidia is no longer doing just chip development. They have a massive software stack. And that's true for all the hyperscalers. So we see that trend continuing. And now we do use, you know, Nvidia's products like InfiniBand in our systems on Z3 to your question, which is, because Z3 is a very unique architecture. So it requires very, very high speed interconnect. So it's almost like a super computer. So then it requires optical and InfiniBand in Z3. Now in X3, we are using AMD FPGAs, which are fabulous, but it does not require that tight interconnect speed. So InfiniBand is more used in Z3 versus X3. But X3 is a great system too, we're using the latest AMD FPGAs, it has 8x higher capacity than X2, and all kinds of innovation on the software side as well. So we are very pleased -- I'm very confident that we have true leadership in these hardware platforms, both Palladium and Protium. And we're also pleased, like I said earlier, that we are able to refresh it much sooner than the market expected, given our track record. And then we are seeing a lot of demand for both of these systems together going forward.
Anirudh Devgan: Yeah, 1 thing, I want to highlight, I think you may have seen this, I just want to highlight our partnership with Intel and IFS. That was concluded in Q1. And so it's really good to see, you know, [Pat] (ph) and Intel investing more in the foundry business and also working more closely with us. So that's also a key contributor to IP, but like John said, we have to hire the people, do the -- we need to port our portfolio to the Intel process, okay? And that takes some time. So that's more will come towards the end of the year and next year. But we are pleased with that new partnership on IP.
Anirudh Devgan: Yes. Thanks, Jay for the question. So as you know, we have five major AI platforms with Cerebrus and Digital implementation being the one that has been out the longest and Cerebrus is doing quite well, like you noted. And we also commented on more than 350 tapeouts, lot of PPA improvement. But all the other ones are doing well, too. Sometimes we have like too many products, we don't talk enough about the others, but like verification, like Verisium is doing quite well. And I mentioned Qualcomm last week talked about pretty impressive results because verification, as you know is an exponential problem, because as the chips get bigger, the verification task gets exponentially bigger. So the benefit of AI can be significant in verification. So I think, you will see that in the next few quarters and years that verification will be as important as implementation in terms of benefits of AI. And then the other area I’d like to highlight is PCB and Allegro and Packaging because that area hasn't seen that much automation. And PCB – and Allegro is a leading-platform for packaging and PCB, but really proud of Allegro X AI. And we talked about several customers, including Intel last week talked about 4x to 10x improvement using X AI in PCB. So apart from Digital, I think the next two ones, I feel are verification and Allegro and PCB and then the areas that haven't done as well, I mean is more not in design optimization is like design generation. And I think, there -- this LLM based models do provide a lot of promise. So historically, we haven't done as much design-generation, which is -- this is like almost pre-RTL, right going from Spec to RTL. That's the -- truly the creative part of the design process. And then once you have RTL, it is more optimization part in digital and verification. So I think that's where we have to see, but some initial results, which we haven't talked but I think mentioned last week. But we work with a -- but we have to see it still in early stages, but we work with one or two customers in which we took like a 40, 50 page Spec document, this English document, and able to automatically generate RTL from it, okay? And the RTL quality is pretty good. So again we have to see how that goes, but that requires these really advanced LLM capabilities. So that's something to be seen. But if that works well, that could be another kind of very interesting kind of application of Gen AI.
Anirudh Devgan: Yes. First of all, thanks. And yes, they become available now, okay? But it will ramp Q3 and then Q4. But we already have them running at several early customers. So I mean, normally when we announce something, as you know, like and one of our lead partners, they have been running for three months already and very stable. But in general, it will be more Q3 and then Q4 in terms of -- because normally, in any system, there is like a three months to six months kind of overlap. So we will still sell Z2 X2, and then move to Z3 X3, so that's a natural part. And that's also contributing to this quarter-by-quarter variation a little bit, but it will ramp. And Q3 will be bigger and then Q4 should be bigger than that.
Anirudh Devgan: Very good question. First of all in non-recurring, it's not just hardware, but it's also IP in terms of the second half because like we mentioned, we have new IP business driven by HPM and AI and also by Intel IFS. So that is also back-end loaded along with hardware. And then hardware, hardware normally when we launch a new system, it takes one years or two years for it to fully. So even though we are not commenting about next year, I would be surprised if this time, it's only a six month impact. So I expect like these things is built for to be used in design for next five years, seven years. So the impact will be also not just this year but following years. And in terms of recurring revenue, I think the best way like we have said is to look at a three year CAGR basis because there could be some fluctuation in all. And overall, we are pleased with the recurring revenue growth and we go from there.
Anirudh Devgan: Yeah Harlan, that's a very good observation. And the pace of AI innovation like is increasing and not just in the big semi companies, but of course, in these system companies. And I think several announcements did come out, right, including, I think now Meta is public that Meta is designing a lot of silicon for AI, and of course, Google, Microsoft, Amazon. So all the big, really hyperscaler companies, along with Nvidia, AMD, Qualcomm, all the other kind of Samsung had AI phone this year. So I mean, there is a lot of acceleration both on the semi side and on the system side. And we are involved with all the major players there, and we are glad to provide our solutions. And I do think -- and this is the other thesis we have talked about for years now, right, five years, seven years that the system companies will do silicon because of a lot of reasons for customization, for schedule and supply chain control for cost benefits, if there is enough scale. And I think, the workload of AI, like if you look at I think some of the big hyperscaler and social media companies, they are talking about using like 20,000, 24,000 GPUs to train these new models. I mean this is immense amount. And then the size of the model and the number of models increased, so that could go to a much, much higher number than right now that is required to train these models and of course, to do inference on these models. So I think, we are still in the early innings in terms of system companies developing their own chips and at the same time, working with the semi companies. So I expect that to grow and those that -- our business with those system companies doing silicon, I would like to say is growing faster than Cadence average. But the good thing is the semi guys are also doing a lot of business. So I don't know, if that 45% will -- because that's a combination of a lot of companies. But overall, the AI and hyperscalers, they are doing a lot more than so are the big semi company.
Anirudh Devgan: Thank you all for joining us this afternoon. It is an exciting time for Cadence as our broad portfolio and product leadership highly positions us to maximize the growing opportunities in the semiconductor and systems industry. And on behalf of our employees and our Board of Directors, we thank our customers, partners and investors for their continued trust and confidence in Cadence.
