Mike Spencer: Good afternoon and thanks for joining us today on our fiscal 2024 first quarter results conference call. Our press release, SEC filings, and a replay of today's call can be found on our website. With me on the call today is Marc Benioff, Chair and CEO; Amy Weaver, President and Chief Finance Officer; and Brian Millham, President and Chief Operating Officer. As a reminder, our commentary today will include non-GAAP measures. Reconciliations between our GAAP and non-GAAP results and guidance can be found in our earnings and press release. Some of our comments today may contain forward-looking statements and are subject to risks, uncertainties, and assumptions, which could change. Should any of these risks materialize or should our assumptions prove to be incorrect, actual company results could differ materially from these forward-looking statements. A description of these risks, uncertainties, and assumptions and other factors that could affect our financial results is included in our SEC filings, including our most recent report on Forms 10-K, 10-Q, and any other SEC filings. Except as required by law, we do not undertake any responsibility to update these forward-looking statements. And with that, let me hand the call to Marc.
Mike Spencer: Thanks, Amy. Operator, we'll move to questions now. I ask that everyone only ask one question in respect for others on the call. In addition, I'd like to introduce Srini Tallapragada, our Head of Engineering, who will be joining us for Q&A today. With that Emma, let's move to the questions.
Mike Spencer: So with that, we want to thank everyone for joining us today, and we look forward to seeing everyone over the coming weeks. Have a great one.
Marc Benioff: Well, I think this is the absolute question of the day, which is we are about to enter an unbelievable super cycle for tech and everyone can see that. This is an incredible opportunity for not only Salesforce, but our entire industry. I mean, perhaps only a year ago or less than a year ago, no one on this call even knew what GPT was. Today, ChatGPT is the fastest growing consumer product of all time, and has transformed many, many lives. It's definitely not just the technology of this lifetime, but maybe any lifetime. It's an incredible technology. And every company is going to have to transform because every company is going to have to become more productive or automated more intelligent through this technology to be competitive with other companies. And just yesterday, I'm in a room here at the top of Salesforce Tower on the 60th floor, and we have the CEO of a very large bank here. And like every other sales call I've made in the last quarter, there's only one thing that customers want to talk about, and that's artificial intelligence and specifically, generative AI. Of course, we have been a leader in this area with Einstein, more than 1 trillion transactions delivered this week, but these are primarily predictive transactions built on machine intelligence, machine learning, and deep learning. But in 2018, deep learning evolved and became much more sophisticated and became generative as these neural networks expanded their capabilities and also the hardware went to another level as well. So, now we have this incredible new capability. It's a new platform for growth, and I couldn't be more excited. But yesterday, there were many questions from my friend who I'm not going to give you his name because he's one of the – the CEO of one of the largest and most important banks in the world. And I'll just say that, of course, his primary focus is on productivity. He knows that he wants to make his bankers a lot more successful. He wants every banker to be able to rewrite a mortgage, but not every banker can, because writing the mortgage takes a lot of technical expertise. But as we showed him in the meeting through a combination of Tableau, which we demonstrated and Slack, which we demonstrated, and Salesforce's Financial Services Cloud, which he has tens of thousands of users on, that banker understood that this would be incredible. But I also emphasize to him that LLMs, or large language models, they have a voracious appetite for data. They want every piece of data that they can consume, but through his regulatory standards, he cannot deliver all that data into the LLM because it becomes amalgamated. Today, he runs on Salesforce, and his data is secure down to the row and cell level. He knows that readers don't block [riders] [ph] that there's all types of security provisions and regarding who can see what data about what account or what customer. And when you put it into an LLM, those permissions are not understood. So, that is a very powerful moment to realize that the way that LLMs operate is in a way state where they're kind of consuming all this data and then giving us that information back out, well, that Salesforce's opportunity. That's why we built this GPT trust layer. And through the GPT trust layer and rebuilding all of our apps, including Slack and Tableau, but as we demonstrated him yesterday, a new Sales Cloud, a new Service Cloud, a new marketing cloud, and what we'll show on June 12 in New York City, a complete reconceptualization of our product line. What that means for this customer and for every customer is that they have an opportunity to transform their business. And for Salesforce, that also means an opportunity to transform ourselves and for our industry, a new super cycle where every company will have to transform to be AI first.
Marc Benioff: Well, I'll tell you that. I think that as you know, in Q1, we went through tremendous disruption with human resources in our company, and it was very disruptive to all of our Ohana. And I'm so grateful to them for how they supported the whole company, all the customers and themselves during what was probably one of the most disruptive quarters that I've seen and yet we delivered these incredible numbers and this incredible technology vision going forward. In terms of enablement of the sales organization, its ability to kind of move forward, that is not, I would say, a material part of what happened in the quarter or what's going to happen for the year. Our sales organization remains with a very high level of productivity, but let me turn it over to Brian to speak directly to his strategy on delivering the year.
Marc Benioff: Well, I think that this is a great question. And I tried to address it on the last call. I just really think you have to look at 2020, 2021 was just this massive super cycle called the pandemic. I don't know if you remember, but we had a pandemic a couple of years ago. And during that, we saw tech buying like we never saw. It was incredible and everybody surged on tech buying. So, you're really looking at comparisons against that huge mega cycle. And that is what I think is extremely important to understand, the relative comparisons. And that is where my head is at, which is I am constantly comparing against what happened in 2021, but also looking at 2020 and 2019. That's a little bit different than 2008 and that's a little bit different than 2001. We didn't exactly have these huge mega cycles that kind of we were exiting. And I – that's also what gives me tremendous confidence going forward and what we're really seeing is that customers are absorbing the huge amounts of technology that they bought. And that is about to come, I believe, to a close. I can't give you the exact date, and it's going to be accelerated by this AI super cycle.
Marc Benioff: Srini, I want to ask you a question. In January, you published a paper in nature from your research team, which was called large language models, generating functional protein sequences across diverse families, and you really showed something amazing, which was that deep learning language models have shown this incredible promise that you just articulated in various biotechnological applications, including protein design, engineering, and you also described very well one of our models that we've created internally, ProGen, which was a language model that can generate protein sequences with predictable function across large protein families. I was very impressed with that. And the entire research team deserves a huge amount of congratulations. So, when you look at that, especially dramatically and semantically correct natural language sentences for diverse topics or how you're going to use that inside our platform against other models that you're seeing like Llama, OpenAI's model, Anthropic and others, when will Salesforce use our own models like [CoGen] [ph], ProGen, T-code, our lit model, when will we use an outside commercial model like an OpenAI or an Anthropic? And when will we go to an open source model like we've seen emerge so many of those, including like Llama.
Marc Benioff: Well, that's a very key point. Isn’t it? That you're seeing a 30% productivity increase in your own engineers using our own LLM.
Marc Benioff: Well, we're really excited to show all of this technology at our AI Day on June 12 in New York City. And then also when we get to [Dreamforce GPT] [ph], we're going to have an incredible demonstration of this technology.
Brian Millham: Thanks, Brad, for the question. I really appreciate it. As you know, we're operating in a constrained environment right now. And so, we are really focused on this productivity measure and metric for our organization right now, investing heavily, as I mentioned earlier, and the enablement part of our organization. Also looking at other ways to drive productivity. And one of the things that we're talking quite a bit about right now is pricing and packaging, bringing together logical products that we can be selling in a single motion versus our go-to-market, which is largely aligned by product., how do we focus on a larger average deal size for every transaction, and so big investments on that front, really a strong focus on productivity as it relates to moving people up market as well. We're thinking about self-serve in the bottom end of our market. How do we drive a self-serve motion, automated motion at the low end of our market to bring our account executives upmarket to drive higher productivity in the sales organization? So clearly, a big motion for us right now. Feel very good about our big deal motion. Actually in Q4, we saw some – sorry, in Q1, we saw some very good big deal execution from the team. That is not really an area that has held us back. We feel very good about our ability to transform companies and transact these large businesses. It really is the velocity business that has held us back a bit on our create and close some of the SMB transactions. So, we have a clear focus in this area to drive the productivity with our plans going into Q2 and beyond into Q4.
Brian Millham: Sure. Yes. I think when we think about our business from an industry perspective, we have a very nice footprint of our great technology companies and financial services company, both of which were a bit slower than we would have liked in the Americas in Q1. And so, as we think about the all-in size of our Americas business, those industries felt a little bit more of the economic headwinds in the quarter in Q1. And so, I think a bit of a slowdown from that perspective is a result you're seeing in the Americas business.
Brian Millham: Yes, thanks for the question. When we think about longer-term structures, we obviously took the action in Q1. But longer term, we're looking at things like how do we leverage comp plan redesign to drive better efficiencies in our organization going forward. How do we continue to look at self-serve at the low end of the market to drive better efficiencies in our organization. So, resellers as a potential investment that we'll make in emerging markets is long-term leverage on the efficiency gains. So lots of things that we're doing that will be in sort of the Phase 2 oriented around process improvement and systems improvement. And again, as I mentioned, top plan design that will drive better efficiencies in the organization.
Srini Tallapragada: Yes, Marc. So, I think I met about 70 customers in the last quarter. And like Marc was saying, the only conversation everybody is interested is on – and while everybody understands the used cases, they're really worried about trust. And what they are looking for us is guidance on how to solve that. For example, so we are doing a lot of things as the basic security level, like we are really doing tenant level isolation coupled with zero retention architecture, the LLM level. So the LLM doesn't remember any of the data. Along with that, they – for them to use these used cases, they want to have – they have a lot of these compliances like GDPR, ISO, SOC, [Quadrant] [ph], they want to ensure that those compliances are still valid, and we're going to solve it for that. In addition, the big worry everybody has is, people have heard about hallucinations, toxicity, bias, this is what we call [model trust] [ph]. We have a lot of innovation around how to ground the data on 360 data, which is a huge advantage we have. And we are able to do a lot of things at that level. And then the thing which I think Marc hinted at, which is LLMs are not like a database. These intra-enterprise trust, even once you have an LLM, you can't open the data to everybody in the company. So, you need ability to do this – who can access this data, how is it doing both before the query and after the query, we have to build that. And then we have to be not only open, but also optimized. We are running an open – the way we'll run is, we'll run like a model [indiscernible] because one of the things everybody has to watch out is it's great, but what about the cost to serve, not all models are equal. So, we are going to run this and pick very – we are going to pick a very cost-optimized curve, so the value is very high. And our Salesforce AI research has a lot of sales for state-of-the-art models and industry cases, which we are optimizing to run at very low cost and high value. Add to that, we've got the Trailblazers platform, which allows low code, high code, and many other things, and we're going to optimize sort of jobs to be done for each industry and jobs. That's really what they're looking for because they have been using our AI platform. Like Marc mentioned, we already do 1 trillion transactions per day. And by the way, the data cloud, just in a month, we are importing more than 7 trillion records into the data layer, so which is a very powerful asset we have. So, coupled with all of this is what they are looking for guidance and how we think we can deliver significant value to our customers.
Srini Tallapragada: Yes. I think you hinted something very important. I think, as you know, Marc, we have – our I research team is one of the best-in-class model – state-of-the-art models from different areas. The way we are thinking of it is like anything else, where the world is going to go, which we strongly believe is going to be multiple models. And depending on the used case, you will pick the right models, which will provide you the value at the lowest cost. Where we have to run with highly regulated industries, where the data cannot leave the trust boundary or where we have significant advantage, where we can train on industry-specific data or Salesforce-specific – 360-specific data, like, for example, our FX model are helping our customers implement or our flow, we will use our internal model. Where we need more generated image models or something where it needs public image databases, we may use a coherent or an OpenAI. It depends on the use case and which is why, at a given request, a secure trusted gateway will decide smartly which is the best used case, which is the model, and we always keep running the [indiscernible], which is what I mean. So today, one particular model may be good. Tomorrow, something else will come, and we'll behind the team flip it, but our customers don't need to know that. We will handle all of it. We'll handle the model trust. We'll handle all the compliances and all behind the scenes. And this is always what we promise to our customers, we'll always future-proof. That's the Salesforce promise to our customers so that they can focus on the business used cases.
Srini Tallapragada: I think the key is innovations we are doing, which people will see starting next month is around what we call [from generation] [ph] and grounding. These are techniques, which we'll have to do, but it will work only because we have – all of this as based on underlying data. We have the Data Cloud, where we have all the 360 data, which is there. So, we're able to ground these models and do it. So, there are a lot of other techniques, which are very technical, which we put it on our block. But that's the innovation that we're doing. And you have to remember that Salesforce also is a metadata model. So, we have a semantic understanding of what our customers are trying to do. We're going to leverage the Metadata platform and do this grounding automatically for our customers, of course, while keeping the trust. That's the base line.
Srini Tallapragada: Thanks, Marc. So, I think the way I see it is this AI technologies are a continuum that is predictive then they generate, and the real long-term goal is autonomous. The initial version of the generative AI will be more in terms of assistance. And like Marc was saying, we are seeing like the most common used case everybody understands implicitly is self-service bots or in the call center or agent-assistant assistance, which I think really helps productivity. But the other used cases, which we are going to see, and in fact, I have rolled out our own code LLMs in our engineering organ, we are already seeing minimum 20% productivity. And in those cases...
Srini Tallapragada: 20%, we are seeing minimum. In some cases, up to 30%. Now, a lot of our customers are asking the same. We are going to roll Einstein GPT for our developers in the ecosystem, which will not only help not only the local developers to bridge the gap, where there's a talent gap, but also reduce the cost of implementations for a lot of people. So there's a lot of value. This assistant model is where we'll see a lot of uptick. And then I think the fully autonomous cases, for example, in our own internal used cases with our models, we are able to detect 60% of instance and auto remediate. That requires a little bit more fine-tuning and we'll have to work with specific customers to get to that level of model performance. So, I see this is just the start of this [cut] [ph]. The resistant model is the initial thing to build trust and a human in the loop and validate it. And then as the models get better and better, we'll keep taking used cases where we can fully automate it.
Srini Tallapragada: Because even today, any example you see, even though we have hundreds of Slack channels, there are a lot of specific Slack channels, which only you want access to. You don't want that. LLM doesn't know. There is no concept of – it combines all this information. So, unless you put the layer both before who can access the data and then when it generates response, what he can do, you don't want one wealth manager to generally generate a report, an account report where you're mixing customers' balances. So there are a lot of trust issues we have to solve. So, LLMs are good for a lot of very creative generative used cases, initially, where it's public data that everybody can use it. Those are used cases. I think there is enough of low-hanging fruit in the initial phases with assistant model, which we'll solve. The really complex automated cases, the role level, record level sharing, we have a lot of techniques, which we are developing, which we will do. It's also a research area, too. That one, I think we should be tempered with expectations, but there's enough of, like I said, the develop, for example, I gave product example there's enough of productivity which we will get.
