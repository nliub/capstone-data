Lisa Su: Sure, Aaron. Let me start and then see if Jean has something to add. So, relative to the Data Center GPU business, we were very pleased with performance that we saw in the fourth quarter. It was always going to be a very sort of back-end quarter weighted as we were ramping the product and we saw MI300A, our HPC product actually ramped very well. And then we saw MI300X. The AI product actually exceed our expectations based on strong customer demand, the way the qualifications went and then the ramp -- manufacturing ramp. So, we were over $400 million for that business in the fourth quarter. And then, going into the first quarter, as we look at the business, server seasonality, call it, something around, let's call it, high-single-digit, low-double-digit. There are also some other pieces of the Data Center business. I think, the key piece of it is we had originally expected the ramp to be a little bit more shallow of our MI300X and what we're seeing now is the supply chain is operating really well, and the customer demand is strong. And so, we will see MI300X increase as we go into the first quarter, and things are going relatively well so.
Lisa Su: Sure, Aaron. So look, I think, I agree with your characterization of the 2023 demand, although we did see some strong progress in the second half of the year, especially as customers in Cloud and Enterprise adopted our Genoa and our Zen 4 family. So going into 2024, I would say the traditional server market is probably still mixed, especially into the first half of the year. There's still some cloud optimization going on, as well as sort of enterprise being a little bit cautious. That being the case though, we also see opportunities for us to continue to grow share in the traditional server business. I think our portfolio is extremely strong. The adoption of Genoa and Bergamo, as well as our new Siena product lines are getting a lot of traction. And then, we also see Turin, our Zen 5 product coming in the second half of the year. So, even in a mixed demand environment, I think we're bullish on what traditional server CPUs can do in 2024.
Lisa Su: Yeah. Thanks, Tim, for the question. I would say a couple of things. First of all, we're really pleased with the progress that we've made in our Data Center GPU business. I think the ramp that we've seen, the customer traction that we've seen even in the last few months, I think has been great. And that gives us a lot of confidence in the ramp of this business. I think the beauty of the AI market here is, it's growing so quickly that I think we have both the market dynamic as well as our ability to gain share in that framework. The point I will make is our customer engagements right now are all quite strategic, dozens of customers with multi-generational conversations. So, as excited as we are about the ramp of MI300 and, frankly, there's a lot to do in 2024. We are also very excited about the opportunities to extend that into the next couple of years out into that '25, '26, '27 timeframe. So, I think, we see a lot of growth. I think it's a little early to make market share projections, but I would say it's a significant growth driver given the market demand, as well as our own product capabilities.
Lisa Su: Yeah, sure, Matt. So, I appreciate the comments. I think the traction that we're getting with the MI300 family is really strong. I think what's benefited us here is our use of chiplet technologies, which has given us the ability to have sort of both the APU version, as well as the GPU version and we continue to use that to differentiate ourselves and that's how we get our memory bandwidth and memory capacity advantages. As we go forward, you can imagine, like we did in the EPYC timeframe, we planned multiple generations in sequence. That's the way we're planning the roadmap. One of the things I will note about the AI accelerator market is the demand for compute is so high that we are seeing sort of an acceleration of the roadmap generations here and we are similarly planning acceleration of our roadmap. I would say that we'll talk more about the overall roadmap beyond MI300 as we get into later this year. But you can be assured that we're working very closely with our customers to have a very competitive roadmap for both training and inference that will come out over the next couple of years.
Lisa Su: Sure. Well, Matt, I don't know-how precise it is, but I think we said approximately $400 billion. But I think what we need to look at is growth rate and how do we get to those growth rates. I think we expect units to grow sort of substantial double-digit percentage. But you should also expect that content is going to grow. So, if you think about how important memory and memory capacity is as we go forward, you can imagine that we'll see acceleration there and just the overall content as we go to more advanced technology nodes. So, there's some ASP uplift in there. And then, what we also do is, we are planning longer-term roadmaps with our customers in terms of how they're thinking about sort of the size of training clusters, the number of training clusters. And then, the fact that we believe inference is actually going to exceed training as we go into the next couple of years just given as more enterprises adopt. So, I think as we look at all those pieces, I think we feel good that the growth rate is going to be significant and sustained over the next few years. In terms of what's in that TAM, it really is accelerator TAM. So, within accelerators, there are certainly GPUs and there will also be some ASICs that are other accelerators that are in that TAM. As we think about sort of the different types of models from smaller models to fine-tuning of models, to the largest large language models, I think you're going to need different silicon for those different use cases. But from our standpoint, GPUs are still going to be the sort of the compute element of choice when you're talking about training and inferencing on the largest language models.
Lisa Su: Yeah, sure, Joe. So, yes, look, we are really happy with how the MI300 has come up and we've now deployed and working with a number of customers. What we have seen is certainly ROCm 6 has been a very important, as well as the direct optimization with our top cloud customers. We always said that the best way of optimizing the software is working directly on the most important workloads. And we've seen performance come up nicely, which is what we expect frankly with the GPU capabilities that we would have to do some level of optimization, but the optimization has gone well. I think to your broader question. The way I look at this is, there are lots of opportunities for us to work directly with large customers, both on the Cloud side as well as on the Enterprise side, who have specific training and inferencing workloads. Our job is to make it as easy as possible for them and so our entire tool chain all of our, sort of the overall ROCm suite has really gone through significant progress over the last six to nine months. And then, we're also getting some nice support from the open-source community. So, the work that Hugging Face is doing is tremendous. In terms of just real-time optimization on our hardware, our partnership with OpenAI on Triton and our work across a number of these open source models has helped us actually make very rapid progress.
Lisa Su: Yeah. So, I mean, Joe, I think we updated our revenue expectations this quarter from our original number of $2 billion to $3.5 billion to try to give some bounding on some of the discussion out there. The way to think about the $3.5 billion is these are customers that we're working with, who have given us firm commitments on what they need. As you know, the lead times on these products are quite long. So, it's important to have those forecasts in early and we have a strong order book. So, that gives us good confidence to exceed the $3.5 billion. From a supply chain standpoint, our goal is always to build more supply we -- and so, from that standpoint, we have also worked with our supply chain partners and secured significant capacity. Think about it as first half capacity is tight and more comes on in the second half of the year, but we've certainly made more progress there. So, we do have more supply, and we're going to continue to work with our customers on their deployments and we'll update that number as we go through the year.
Lisa Su: Yeah, Toshiya, thanks for the question. So, first on the MI300 trajectory. I think you would expect that revenue should increase every quarter from now through sort of the end of the year, but it will be a bit more second half weighted and part of that is just customers as they're finishing up their qualifications in their lines as well as sort of how our supply chain is ramping. So, yes, it should increase each quarter, but be a bit more second half weighted. And then, to your comment about customers, look, we are engaged with all of sort of the large customers. These are all folks that know what's really well, given our deep relationships in EPYC. I think people just have different adoption cycles as they consider what they're trying to do in their roadmap. But I view this as still very, very early innings for us in this space. And I think the question was asked earlier. I think the key is this is not just about MI300 conversation. But it really is about sort of our long-term multi-generational roadmap. And so, that's the context on which we're working with our largest customers, as well as, as you know, there's a lot of demand coming from folks that are more AI centric and not necessarily typical cloud customers, but more enterprise or let's call it AI-specific companies that we're also very well engaged with.
Lisa Su: Sure, Ross. So look, I think the environment for us is always competitive. So, I think that has not changed. If I look at the Instinct side, I think we have -- I think we've shown that MI300 and our roadmap are actually very competitive. There are some places where let's call it, it's more even like in the training environment. But as we look at the inferencing environment, we think we have significant advantages. And that's showing through in some of our customer work. So we think for both training and inference, we will continue to be very competitive. And then, as you go into the CPU side again, from our view, with each generation of EPYC, we've continued to gain share. I think, we exited the fourth quarter at record share for AMD. And we're still quite underrepresented in Enterprise. So I think there is an opportunity for us to continue to gain share as we go through 2024. From a competitive standpoint, what we see is Zen 4 is extremely competitive right now with Genoa, Bergamo, Siena. And as we go into Turin, we're deep in the design in-cycle for Zen 5 and Turin and we feel very good about how we're positioned.
Lisa Su: I think we expect the CPU business from a market standpoint to grow, Ross. As we go into 2024, I think the rate and pace of growth will depend a little bit on the macro and just overall CapEx trends. But from our standpoint, we are starting to see some of our larger customers plan their refresh cycles. There's a lot of let's call it older equipment that has yet to be refreshed and the value proposition for refresh is so strong because the energy efficiency and sort of the footprint of the newer generations are so much better than sort of the four or five year old infrastructure that we do see that refresh cycle happening as we get into 2024. I think the exact timing, we will have to understand more as it, as the market evolves as we go through the year.
Lisa Su: Yes. Sure, Vivek. So, I think what we said is as we went from $2 billion to $3.5 billion, it really is mostly customer demand signals. So as orders have come on books and as we've seen programs moved from, let's call it, pilot programs into full manufacturing programs, we have updated the revenue forecast. As I said earlier, from a supply standpoint, we are planning for success. And so, we worked closely with our supply chain partners to ensure that we can ship more than $3.5 billion, substantially more depending on what customer demand is as we go into the second half of the year. And then, in terms of again roadmaps, as I said, we are very focused on a competitive roadmap this -- that sort of what the next generations are beyond MI300. So, I do believe that we have a strong roadmap in-place and continue to work with our customers to sort of adopt our roadmap as quickly as possible.
Lisa Su: Yeah. Sure, Vivek. Look, we're always looking at what's next, right? So, on the chiplet technology, I mean, we're sort of on the fourth generation of the chiplet technologies. I think we've learned a lot about how to optimize performance there. We are very aggressive with our adoption of leading-edge technology as it's needed. But I think those are only a few of the pieces. We're also focused on continuing to innovate on architecture and design. So, I think the longer-term question that you ask is I think we're expecting that the competition is going to be on a similar process technology and even in that case, I think we feel like we have a very strong roadmap going forward and will continue to drive both the CPU and the GPU roadmap very aggressively.
Lisa Su: Okay, Harsh. So, let me start, to your question about the value proposition for MI300. Again, customers are using it for different reasons, but presume that there is a performance per dollar benefit to using AMD. So that's one piece of it. The other piece of it though is we intrinsically have more bandwidth and memory capacity on MI300X compared to the competition. And what that means is for large language models that are many tens of billions of parameters you make -- you could potentially do the workload in fewer GPUs. So, it's a substantial system savings and allows you to do much more work within the same system. In terms of what customers are using MI300 for today, I would say there are a number of customers using it for large language model inferencing and there are also customers that are using it for training. So I think the whole point is being a strong partner. When you put these AI systems in-place, they are sometimes mixed-use systems. So they would be used for both training and inference.
Lisa Su: Sure, Stacy. I don't think it's a pull-forward of demand. I think what it is we wanted to see how long it would take for customers to fully qualify and get their workloads performance. So, yeah, that depends a lot on the actual engineering work that's done and now that we're, let's call it a quarter later, we've seen that it's gone really well. So, it's actually gone a bit better, then our original forecast. And as a result, we've seen stronger demand signals and customers are gaining confidence in their ability to deploy a significant number of MI300 this year.
Lisa Su: Yeah, I thought I had answered it, but yes, I'll answer it again. It is accelerator chips. It is not systems. So, think of it as GPUs, ASICs that will be there. Those types of things, but it includes, obviously, it includes memory and other things that are packaged together with the GPUs.
Lisa Su: I don't think it will be one or two that are half the revenue, Chris. I think we are building this as a -- really, we're happy to see sort of the broad adoption as always with sort of the large cloud partners. We might see sort of one or two that are higher than others, but I don't think you see the type of concentration that you mentioned.
Lisa Su: Look, I feel very good about our partnership with TSMC, they continue to execute extremely well. We'll see what happens over the next few years. But I'd like to kind of reemphasize what I said earlier, even in the case of process parity, we feel very good about our architectural roadmap and all the other things that we add, as we look at our entire portfolio of CPUs, GPUs, DPUs, adaptive SoCs and kind of put them together to solve problems. I think we feel really good about what we can do with our customers. So, we're always going to be paying attention to sort of the process race, but I think we feel very good about sort of our strategy and how do we continue to sort of push the envelope on the computing roadmaps.
Jean Hu: Yeah, Aaron, I'll give you some color about Client seasonality and others. So, Client is very similar to server, typically Q1 is high-single-digit to low-double-digit. That's consistent with past. On the Embedded side, it's very consistent with what we said in the past and the consistent with what you see in the industry's Embedded business is going through a bottoming process, and we think Q1, it will have a low-double-digit sequential decline. That's Embedded. On the Gaming side, Lisa mentioned during his -- her prepared remarks is we have the latest stage of product cycle in the year five of gaming console. But at the same time, we also have inventory at the customers. So, the combination of those impact, we expect the Q1 Gaming sequential declines probably more than 30%, so hopefully that help you a little bit.
Jean Hu: Hi, Tim. Thank you for the question. Yeah, we're not guiding a year. It's very early of the year, literally, January. I think the way to think about it is, Lisa mentioned during her prepared remarks we feel pretty good about both our Data Center and the Client business to grow in 2024. Of course, the largest incremental revenue opportunities are going to come from Data Center between both the server side gaining more share, and Data Center GPU side with the significant ramp up of our MI300. I think that's how we think about it. We do have a headwind from Gaming segment. We do think year-over-year, we'll see very significant double-digit decline on the Gaming segment. And at the same time Embedded is going through the bottoming process. We do think the second-half we will see the recovery. So those are the puts and takes. I can talk about it.
Jean Hu: Yeah, Toshiya, thank you for the question. Yeah, you're absolutely right. We have some puts and takes that impact our gross margin. We guided the Q1, 120 basis points higher than Q4 sequentially, primarily because the higher Data Center contribution actually more than offset the decline of Embedded business in Q1. Going forward, the way to think about it is as you said is the major driver is going to be Data Center business is going to grow much faster than other segment. That mix change will help us to expand the gross margin nicely. I think you also are spot on, the Embedded coming back in second half, which will be a tailwind. With the Data Center GPU, we are at the very early stage of ramp. We are improving testing time yield and continue to expand gross margin and we expect to be accretive to corporate average. So, those are all the tailwinds coming in the second half. I would say the headwinds side continue to be in the first half where we see Embedded business not only Q1 we see sequential decline, Q2 probably are going to be sequentially flattish versus Q1. That is a headwind for us. Because it does have a very nice gross margin. But overall, we feel pretty good about the trajectory of the gross margin improvement, especially second half.
Jean Hu: Yeah, Harsh. Let me answer your question about the MI300. Exiting Q4 2024, is it possible to get to $1.5 billion? It is possible, right, because Lisa mentioned earlier, we'll see sequential increase in each quarter and more back-end loaded in second-half and we do have a supplies more than $3.5 billion. And of course we will continue to make progress with our customers. So the math, yeah, it's possible, but right now we are really looking at focus on the execution of the current $3.5 billion plus.
