Jean Hu: Thank you, Lisa, and good afternoon, everyone. I'll start with a review of our financial results and then provide our current outlook for the second quarter of fiscal 2024. We delivered strong year-over-year revenue growth in our Data Center and Client segments in the fourth quarter and grew 230 basis points of gross margin expansion. For the first quarter of 2024, revenue was $5.5 billion, up 2% year-over-year as revenue growth in the Data Center and the Client segment was partially offset by lower revenue in our Gaming and Embedded segment.  Revenue declined 11% sequentially as higher Data Center revenue resulting from the ramp of our AMD Instinct GPUs was offset by lower Gaming and Embedded segment revenues. Gross margin was 52%, up 230 basis points year-over-year, driven by higher revenue contribution from the Data Center and Client segment, partially offset by lower Embedded and Gaming segment revenue contribution.  Operating expenses were $1.7 billion, an increase of 10% year-over-year, as we continued investing aggressively in R&D and marketing activities to address the significant AI growth opportunities ahead of us. Operating income was $1.1 billion, representing a 21% operating margin. Taxes, interest expense and other was $120 million. For the fourth quarter of 2024, diluted earnings per share was $0.62, an increase of 3% year-over-year.  Now turning to our Reportable segment, starting with the Data Center. Data Center delivered record quarterly segment revenue of $2.3 billion, up 80%, a $1 billion increase year-over-year. Data Center accounted for more than 40% of total revenue, primarily led by the ramp of AMD Instinct GPUs from both cloud and enterprise customers and a strong double-digit percentage growth in our server process revenue as a result of growth across our sample products.  On a sequential basis, revenue increased 2%, driven by the ramp of our AMD Instinct GPUs, partially offset by seasonal decline in server CPU sales. Data Center segment operating income was $541 million or 23% of revenue compared to $148 million or 11% a year ago. Operating income was up 266% year-over-year due to operating leverage even as we significantly increased our investment in R&D.  Client segment revenue was $1.4 billion, up 85% year-over-year, driven primarily by Ryzen 8000 series processors. On a sequential basis, Client revenue declined 6%. Client segment operating income was $86 million or 6% of revenue compared to an operating loss of $172 million a year ago, driven by higher revenue.  Gaming segment revenue was $922 million, down 48% year-over-year and down 33% sequentially due to a decrease in semi customer and Radeon GPU sales. Gaming segment operating income was $151 million or 16% of revenue compared to $314 million or 18% a year ago.  Embedded segment revenue was $846 million, down 46% year-over-year and 20% sequentially as customers continue to manage their inventory levels. Embedded segment operating income was $342 million or 41% of revenue compared to $798 million or 51% a year ago.  Turning to the balance sheet and cash flow. During the quarter, we generated $521 million in cash from operations, and free cash flow was $379 million. Inventory increased sequentially by $301 million to $4.7 billion, primarily to support the continued ramp of data center and client products in advanced process node.  At the end of the quarter, cash, cash equivalent and short-term investment was $6 billion. As a reminder, we have $750 million of debt maturing this June. Given our ample liquidity, we plan to retire that utilizing existing cash.  Now turning to our second quarter 2024 outlook. We expect revenue to be approximately $5.7 billion, plus or minus $300 million. Sequentially, we expect Data Center segment revenue to increase by double-digit percentage, primarily driven by the Data Center GPU ramp; client segment revenue to increase; embedded segment revenue to be flat; and in the Gaming segment, based on current demand signals, revenue to decline by significant double-digit percentage.  Year-over-year, we expect our Data Center and Client segment revenue to be up significantly, driven by the strength of our product portfolio; the Embedded and the Gaming segment revenue to decline by a significant double-digit percentage. In addition, we expect second quarter non-GAAP gross margin to be approximately 53%. Non-GAAP operating expenses to be approximately $1.8 billion. Non-GAAP effective tax rate to be 13% and the diluted share count is expected to be approximately 1.64 billion shares.  In closing, we started the year strong. We made significant progress on our strategic priorities, delivering year-over-year revenue growth in our Data Center and the Client segment and expanded the gross margin. Looking ahead, we believe the investments we are making will position us very well to address the large AI opportunities ahead.  With that, I'll turn it back to Mitch for the Q&A session. 
Jean Hu: Vivek, thank you for the question. I think the Embedded business declined a little bit more than expected, really due to the weaker demand in some of the markets, very specifically, communication has been weak. And some pockets of industrial and automotive, as you mentioned, it's actually quite consistent with the peers.  Second half, we do think the first half is the bottom of Embedded business and will start to see gradual recovery in the second half. And going back to your gross margin question is, when you look at our gross margin expansion in both Q1 and the guide at Q2, the primary driver is the strong performance on the Data Center side. The Data Center will continue to ramp in second half. I think that will continue to be the major driver of gross margin expansion in second half. Of course, if Embedded is doing better, we'll have a more tailwind in the second half. 
Jean Hu: Yes. Thank you, Joe. Our team has done an incredible job to ramp MI300. As you probably know, it's a very complex product, and we are still at the first year of the ramp, both from yield, the testing time and the process improvement, those things are still ongoing. We do think over time, the gross margin should be accretive to corporate average. 
Jean Hu: Stacy, thanks for the question. You always ask a math question. So I think, in general, it is more. The Data Center GPU ramp, it will be more than the overall company's $200-some million ramp. 
Jean Hu: So maybe -- yes, maybe let me give you some color about the Gaming business, right? If you look at the Gaming, the demand has been quite weak, that's quite very well-known and also their inventory level. So based on the visibility we have, the first half, both Q1, Q2, we guided down sequentially more than 30%. We actually think the second half will be lower than first half.  That's basically how we're looking at this year for the Gaming business. And at the same time, Gaming's gross margin is lower than our company average. So overall, will help the mix on the gross margin side. That's just some color on the Gaming side. But you're right, Q2 Gaming is down a lot. 
Jean Hu: Yes. I think you're right. It's -- the GPU gross margin right now is below the Data Center gross margin level, I think there are 2 reasons. Actually, the major reason is we actually increased the investment quite significantly to, as Lisa mentioned, to expand and accelerating our road map in the AI side. That's one of the major drivers for the operating income coming down slightly.  On the gross margin side, going back to your question, we said in the past, and we continue to believe the case is, Data Center GPU gross margin over time will be accretive to corporate average. But it will take a while to get to the Server level for gross margin. 
Jean Hu: Yes. Harlan, this is Jean. I think the Server business has been performing really well. Year-over-year, it actually increased a very strong double digit. I think, sequentially, it is more seasonal, but we feel pretty good about continue gaining share there. 
Jean Hu: Harsh, we're not going to guide a specific segment below the segment revenue. I think the most important thing is that we did say Data Center is going to grow double digit sequentially. I will leave it over there. Subsegment, there are a lot of details. 
Lisa Su: Great. Thank you, Toshiya, for the question. Look, the MI300 ramp is going really well. If we look at just what's happened over the last 90 days, we've been working very closely with our customers to qualify MI300 in their production data centers, both from a hardware standpoint, software standpoint. So far, things are going quite well.  And what we see now is just greater visibility to both current customers as well as new customers committing to MI300. So that gives us the confidence to go from $3.5 billion to $4 billion. And I view this as very much -- it's a very dynamic market, and there are lots of customers. We said on the -- in the prepared remarks that we have over 100 customers that we're engaged with in both development as well as deployment. So overall, the ramp is going really well.  As it relates to the supply chain, actually, I would say, I'm very pleased with how supply has ramped. It is absolutely the fastest product ramp that we have done. It's a very complex product, chiplets, CoWoS, 3D integration, HBM. And so far, it's gone extremely well. We've gotten great support from our partners. And so I would say, even in the quarter that we just finished, we actually did a little bit better than expected when we first started the quarter.  I think Q2 will be another significant ramp. And we're going to ramp supply every quarter this year. So I think the supply chain is going well. We are tight on supply. So there's no question in the near term that if we had more supply, we have demand for that product, and we're going to continue to work on those elements as we go through the year. But I think both on the demand side and the supply side, I'm very pleased with how the ramp is going. 
Lisa Su: Yes, sure. So look, Toshiya, when we start with the road map, I mean, we always think about it as a multiyear, multigenerational road map. So we have the follow-ons to MI300 as well as the next, next generations well in development. I think what is true is we're getting much closer to our top AI customers. They're actually giving us significant feedback on the road map and what we need to meet their needs.  Our chiplet architecture is actually very flexible. And so that allows us to actually make changes to the road map as necessary. So we're very confident in our ability to continue to be very competitive. Frankly, I think we're going to get more competitive. Right now, I think MI300x is in a sweet spot for inference, very, very strong inference performance.  I see as we bring in additional products later this year into 2025, that, that will continue to be a strong spot for us. And then we're also enhancing our training performance and our software road map to go along with it. So more details to come in the coming months, but we have a strong road map that goes through the next couple of years, and it is informed by just a lot of learning in working with our top customers. 
Lisa Su: Yes, sure, Ross, thanks for the question. I think the -- our EPYC business has actually performed pretty well. The market is a bit mixed. I think some of the cloud guys are still working through sort of their optimizations. I think it's different by customer. We did see here in the first quarter, actually, some very nice early signs in the enterprise space, sort of large customers starting refresh programs. The value proposition of Genoa is very, very strong, and we're seeing that pull through across the enterprise.  In the second quarter, we expect overall Data Center to be up strong double digits. And then within that, we expect server to be up as well. And as we go into the second half of the year, I think there are a couple of drivers for us.  We do expect some improvement in the overall market conditions for the server business. But we also have our Turin launch in the second half of the year that will also, we believe, extend our leadership position within the server market. So overall, I think the business is performing well, and we believe that we're continuing to be very well positioned to gain share throughout the year. 
Lisa Su: Yes. So I think, again, I think we're pretty excited about the AI PC, both opportunity in, let's call it, the near term and even more so in the medium term. I think the client business is performing well, both on the channel and on the MNC side. We expect clients to be up sequentially in the second quarter.  And as we go into the second half of the year, to your question about units versus ASPs, I think we expect some increase in units as well as ASPs. The AI PC products, when we look at the Strix products, they're really well-suited for the premium segments of the market. And I think that's where you're going to see some of the AI PC content strongest in the beginning. And then as we go into 2025, you would see it more across the rest of the portfolio. 
Lisa Su: Yes. Sure, Matt. Thanks for the question. So look, I think one of the things that we see and we've said is that the TAM for AI compute is growing extremely quickly. And we see that continuing to be the case in all conversations. We had highlighted a TAM of let's call it, $400 billion in 2027. I think some people thought that was aggressive at the time. But the overall AI compute needs, as we talk to customers is very, very strong. And you've seen that in some of the announcements even recently with some of the largest cloud guys.  From my view, there are several aspects of it. First of all, we have great relationships with all of sort of the top AI companies. And the idea there is we want to innovate together. When you look at these large language models and everything that you need for training and inferencing there, although -- there will be many solutions. I don't think there's just one solution that will fit all. The GPU is still the preferred architecture, especially as the algorithms and the models are continuing to evolve over time. And that favors our architecture and also our ability to really optimize CPU with GPU.  So from my standpoint, I think we're very happy with the partnerships that we have. I think this is a huge opportunity for all of us to really innovate together. And we see that there's a very strong commitment to working together over multiple years going forward. And that's, I think, a testament to some of the work that we've done in the past, and that very much is what happened with the EPYC road map as well. 
Lisa Su: Sure, Matt. Look, I think I might have said it earlier, but maybe I'll repeat it again. I think the demand side is actually really strong. And what we see with our customers and what we are tracking very closely is customers moving from, let's call it, initial POCs to pilots to full-scale production to deployment across multiple workloads. And we're moving through that sequence very well. I feel very good about the deployments and ramps that we have ongoing right now.  And I also feel very good about new customers who are sort of earlier on in that process. So from a demand standpoint, we continue to build backlog as well as build engagements going forward. And similarly, on the supply standpoint, we're continuing to build supply momentum. But from a speed of ramp standpoint, I'm actually really pleased with the progress. 
Lisa Su: Yes, Aaron. So we've said before that our goal is to ensure that we have supply that exceeds the current guidance, and that is true. So as we've upped our guidance from $3.5 billion to $4 billion, we still -- we have supply visibility significantly beyond that. 
Lisa Su: Yes. I think, Aaron, what I would say is there are -- the need for refresh of, let's call it, older equipment is certainly there. So we see a refresh cycle coming. We also see AI head nodes as another place where we see growth in, let's call it, the more traditional SSD market. Our sweet spot is really in the highest performance, sort of high core count, energy efficiency space, and that is playing out well.  And we're also -- we've traditionally been very strong in, let's call it, cloud first-party workloads, and that is now extending to cloud third-party workloads, where we see enterprises who are, let's call it, in more of a hybrid environment, adopting AMD both in the cloud and on-prem. So I think, overall, we see it as a continued good progression for us with the server business going through 2024 and beyond. 
Lisa Su: Yes. Vivek, let me try to make sure that we answered this question clearly. From a full year standpoint, our $4 billion number is not supply capped -- I'm sorry, yes, it's not supply capped. It is -- we do have supply capability above that. It is more back half weighted. So if you're looking at sort of the near term, I would say, for example, in the second quarter, we do have more demand than we have supply right now, and we're continuing to work on pulling in some of that supply.  By the way, I think this is an overall industry issue. This is not at all related to AMD. I think overall, AI demand has exceeded anyone's expectations in 2024. So you've heard it from the memory guys. You've heard it from the foundry guys. We're all ramping capacity as we go through the year.  And as it relates to visibility, we do have good visibility into what's happening. As I said, we have great customer engagements that are going forward. My goal is to make sure that we pass all of the milestones as we're ramping products. And as we pass those milestones, we put that into the overall full year guidance for AI.  But in terms of how customer progression, things are going, they're actually going quite well. And we continue to bring new customers on, and we continue to expand workloads with our current customers. And so hopefully, that clarifies the question, Vivek. 
Lisa Su: Yes, sure, Timothy. Thanks for the question. For sure, look, our customers are engaged in the multigenerational conversation. So we're definitely going out over the next couple of years. And as it relates to the overall system integration, it is quite important. It is something that we're working very closely with our customers and partners on. That's a significant investment in networking, working with a number of networking partners as well to make sure that the scale-out capability is there.  And to your question of do we have the pieces? We do absolutely have the pieces, I think the work that we've always done with our Infinity Fabric as well as with our Pensando acquisition that's brought in a lot of networking expertise. And then we're working across the networking ecosystem with key partners like Broadcom and Cisco and Arista, who are with us at our AI data center event in December.  So our work right now in future generations is not just specifying a GPU, it is specifying, let's call it, full system reference designs. And that's something that will be quite important going forward. 
Lisa Su: Yes. I mean, I think, Tim, I think the best way to say it is our Data Center segment is on a very, very strong ramp as we go through the back half of the year. Server CPUs, certainly, Data Center GPUs, for sure. So I don't know that we're going to get into specifics, but I could say, in general, you should expect overall at the segment level to be very strong double digits. 
Lisa Su: Yes. Joe, I think from what we see, look, think Turin is the same platform so that does make it an easier ramp. I do think that Genoa and Turin will coexist for some amount of time because customers are deciding when they're going to bring out their new platforms. We expect Turin to give us access to a broader set of workloads. So our SAM actually expands with Turin, both in enterprise and cloud. And from our experience, I think you'll see a faster transition than, for example, when we went from Milan to Genoa. 
Lisa Su: Yes. Harlan, thank you for the question. What it really is, is both us and our customers feeling confident in broadening the ramp? Because if you think about it, first of all, the ROCm stock has done really well. And the work that we're doing is hand in hand with our customers to optimize their key models. And it was important to get sort of verification and validation that everything would run well, and we've now passed some important milestones in that area. . And then I think the other thing is, as you said, there is a huge demand for more AI compute. And so our ability to participate in that and help customers get that up and running is great. So I think, overall, as we look at it, this ramp has been very, very aggressive as you think about where we were just a quarter ago. Each of these are pretty complex bring ups. And I'm very happy with how they've gone. And by the way, we're only sitting here in April. So there's still a lot of 2024 to go, and there's a great customer momentum in the process. 
Lisa Su: Yes. And if I'd just add, Harlan, to your question, we did see strength in enterprise in the first quarter. And I think that has -- that offset perhaps some of the normal seasonality. 
Lisa Su: Yes. When we look at our server CPU sort of ASPs, they're actually very stable. I think we -- again, we tend to be indexed towards the higher core counts. Overall, I would say, the pricing environment is stable. This is about sort of TCO for sort of the customer environment and sort of our performance and our performance per watt, our leadership. And that usually translates into TCO advantage for our customers. 
Lisa Su: Yes. I think it's very important to say we are very supportive of the open ecosystem. We're very supportive of the Ultra Ethernet Consortium. But I don't believe that, that is a limiter to our ability to build large-scale systems. I think Ethernet is something that many in the industry feel will be the long-term answer for networking in these systems, and we have a lot of work that we're doing with internally as well as with our customers and partners to enable that. 
Lisa Su: Yes. So I'll talk maybe a little bit more at the strategic level. I think as we look at sort of how AI shapes up over the next few years, there are customers who would be looking at very large training environments and perhaps that's what you're talking about. I think our view of that is, number one, we view that as a very attractive area for AMD. It's an area where we believe we have the technology to be very competitive there.  And I think the desire would be to have optionality in terms of how you build those out. So obviously, a lot has to happen between here and there. But I think your overarching question of. Is it winner takes all? I don't think so. That being the case, we believe that AMD is very well positioned to play in those, let's call it, of very large scale systems. 
