Hock Tan: Okay. Well, on your first part of your question, yes, we -- I mean, last earnings call, we have indicated there was a strong sense of demand, and we have seen that continue unabated in terms of that strong demand such that's coming in. Now, of course, we all realize lead times -- manufacturing lead times on most of these cutting-edge products is fairly extended. I mean, you don't make this -- manufacture these products under our process anything less than 6 months or thereabouts. And while there is strong demand and a strong urgency of demand, the ability to ramp up will be more measured and addressing demands that are most urgent. On the second part, no, we've always seen competition. And really, even in traditional workloads in enterprise data centers and hyperscale data centers, our business, our markets in networking, switching, routing continues to face competition. So really nothing new here. Competition continues to exist, and we -- each of us do the best we can in the areas we are best at doing.
Hock Tan: On your first part of your question -- you guys love your question in two parts, let's do the first part first. We guided or we indicated that for fiscal '23 that the revenue we are looking in this space is $3.8 billion. There's no reason nor are we trying to do it now in the middle of the year to change that forecast at this point. So, we still keep to that forecast we've given you in fiscal '23. We're obviously giving you a sense of trajectory in my remarks on what we see '24 to look like. And that, again, is a broad trajectory of the guidance, nothing more than that, just to give you a sense for the accelerated move from '22, '23 and headed into ‘24. Nothing more than that. But in terms of specific numbers that you indicated we gave, it's -- we stay by our forecast of fiscal '23, 3.8%, frankly, because my view, it's a bit early to give you any revised forecast. Then beyond that, on your most broad specific question, ASICs versus merchant, I always favor merchant, whether it's in compute, whether it's in networking. In my mind, long-term, merchant will eventually, in my view, have a better shot at prevailing. But what we're not talking -- what we're talking about today is, obviously, a shorter term issue versus a very long-term issue. And the shorter term issue is, yes, compute offload exists. But again, the number of players in compute offload ASICs is very, very limited, and that's what we continue to see.
Hock Tan: Well, you're right in that -- this kind of AI product, this -- in this generative AI products, next-generation, current generation are all using very leading-edge technologies in wafers, silicon wafers and substrates and packaging, including memory stacking. And -- but it's -- from consumption, it's still -- there's still products out there. There's still capacity out there as I said. And this is not something you want to be able to ship or deploy right away. It takes time. And we see it as a measured ramp over -- that has started in fiscal '23 and will continue its pace through to '24.
Hock Tan: Well, it's -- we are -- our basic opportunity still lies in the networking of AI networks. And we have the products out there. And we are working with many, many customers, obviously, to put in place this disaggregated -- distributed, disaggregated architecture, which -- of Ethernet fabric on AI. And yes, that's a lot of obvious interest and lots of design that exists out there.
Hock Tan: Your guess is as good as mine, actually. I can tell you this. I mean, you're right, there's this AI networks and this budget that are now allocated more and more by the hyperscale towards this AI networks. But not necessary, particularly in enterprise, at the expense of traditional workloads and traditional data centers. I think there's going to be -- there's definitely coexistence. And a lot of the large amount of spending on AI today that we see for us, that is very much on the hyperscale. And so, enterprises are still focusing a lot of their budgets as they have on the traditional data centers and traditional workloads supporting x86. But it's just maybe too early to -- really for us to figure out whether that is that cannibalization.
Hock Tan: By the way, it's 50. Yes, my standard lead time for our products is 50 weeks, and we are still staying with it because it's not about as much lead time to manufacture the products as our interest and, frankly, mutual interest between our customers and ourselves to take a hard look at providing visibility for us in ensuring we can supply and supply in the right amount at the right time the requirements. So yes, we're still sticking to 50 weeks.
Hock Tan: Answer -- from an earlier question by a peer of yours, we do not see -- obviously, we do not know, we do not see cannibalization, but these are early innings, relatively speaking, and budgets don't change that rapidly. If there's cannibalization, obviously, it comes from where the spending goes in terms of priority. It's not obvious to us there is that clarity to be able to tell you there's cannibalization, not in the lease. And by the way, if you look at the numbers that all the growth is coming from it, perhaps you're right. But as we talk -- as we sit here in '23 and we still show some level of growth, I would say, we still show growth in the rest of our business, in the rest of products, augmented -- perhaps that growth is augmented with the growth in our AI revenue, in delivering AI products, but it's not entirely all our growth. I would say at least half the growth is still on our traditional business, the other half may be out of generative AI.
Hock Tan: Look, what you say is very, very insightful. It's -- a big part of our growth now in AI comes from the networking components that we're supplying into creating this Ethernet fabric for AI clusters. In fact, a big part of it, you hit on. And the rate of growth there is probably faster than our offload computing can grow. And that's where we are focused on, as I say, our networking products are merchant standard products, supporting the very rapid growth of generative AI clusters out there in the compute side. And for us, this growth in the networking side is really the faster part of the growth.
Hock Tan: Okay. Well, I don't want to be what’s me if you are nitpicky? It's an extension, I would call it, of our existing long-term agreement. And it's an extension in the form of a collaboration and strategic arrangement is the best way to describe. It's not really a renewal. But the characteristics are similar, which is with supply technology, we supply products in a bunch of very specific products related to 5G components and wireless connectivity, which is our strength, which is the technology we keep leading in the marketplace. And it's multiyear. And beyond that, I truly would -- I'll refer you to our 8-K and not provide any more specifics simply because of sensitivities all around.
Hock Tan: Very, very good question, Toshiya. Well, we are still a very broadly diversified semiconductor company, as I pointed out, with multiple -- with still multiple end markets beyond just AI, most of which AI revenue happen to sit in my networking segment of the business, as you all noted, and you see. So we still have plenty of others. And even as I mentioned, for fiscal '24, our view is that it could hit over 25% of our semiconductor revenue. We still have many large number of underpinnings for the rest of our semiconductor business. I mean, our wireless business, for instance, has a very strong lease of life for multi-years, and that's a big chunk of business. Just that the AI business appears to be trying to catch up to it in terms of the size. But our broadband server storage enterprise business continues to be very, very sustainable. And when you mix it all up, I don't know, we haven't updated our forecast long-term, so to show. I really have nothing more to add than what we already told you in the past. Would it have -- make a difference in our long-term growth rate? Don't know. We haven't thought about it. I’ll leave it to you to probably speculate before I put anything on paper.
Hock Tan: Thank you. We tend to be very loyal to our suppliers. The same reason we look at customers, the same -- in that same manner, it cuts both ways for us. So, there's a deep abiding loyalty in all our key suppliers. Having said that, we also have to be very realistic of the geopolitical environment we have today. And so, we are also very open to looking at in certain specific technologies to broaden our supply base. And we have taken steps to constantly look at it, much as we still continue to want to be very loyal and fair to our existing base. So -- and so we continue that way. And because of that partnership and loyalty, for us, price increase is something that is a very long-term thing, it’s part of the overall relationship. And put it simply, we don't move just because of prices. We stay put because of support, service and abiding sense of -- a very abiding sense of commitment mutually.
Hock Tan: You're not wrong. All this, as I indicated upfront in my remarks, current remarks, yes, we see our next generation coming up Tomahawk 5, which will have silicon photonics, which is co-packaging as a key part of that offering and not to mention that it's going up to 51 terabit per second cut-through bandwidth. It's exactly what you want to put in place for very high demanding AI networks, especially if those AI networks start running to -- over 32,000 GPU clusters running at 800 gigabit per second. Then you really need a big amount of switching because those kind of networks, as I mentioned, have to be very low latency, virtually lossless. Ethernet lossless calls for some interesting science and technology in order to make Ethernet lossless. Because by definition, Ethernet tends to have it traditionally. But the technology is there to make it lossless. So all this fits in with our new generation of products. And not to mention our Jericho3-AI, which, as you know, the router has a unique differentiated technology that allows for very, very low tail latency and in terms of how it transmits and reorder packets so that there's no loss and very little latency. And that exists in network routing in telcos, which we now apply to AI networks in a very effective manner, and that's our whole new generation products. So yes, we're leaning into this opportunity with our networking technology and next-generation products very much. So, you hit it right on, and which is, one, makes it very exciting for us in AI. It's in the networking area, networking space that we see most interesting opportunities.
Hock Tan: Thank you. Good question. And I’ll reiterate the answers in some other ways I've given to certain other audience who have asked this question. We really have only one rail customer -- one customer. And in my forecast, in my remarks so far in offload computing, it's pretty much very, very largely around one customer. It's not very diversified. It's very focused. That's our compute offload business.
Hock Tan: Okay. Well, our core -- our software segment comprises, you hit it correctly, two parts. That's our core software products revenues and sold directly to enterprises. And these are your typical infrastructure software products. And they are multiyear contracts. And we have ton -- and we have a lot of backlog, something like $17 billion of backlog, averaging over almost 2.5, 3 years. And every quarter, a part of that renews, and we give you the data on it. It's very stable. And given our historical pattern of renewing on expanding consumption of our core group of customers, we tend to drive that in a very stable manner. And the growth rate is very, very predictable, and we're happy with that. Then we overlay on it a business that is software, but also very appliance different, the fiber channel SAN business of Brocade. And that's very enterprise-driven, very, very much so. Only used by enterprises, obviously, and large enterprises at that. And it is a fairly cyclical business. And last year was a very strong up cycle. And this year, not surprisingly, the cycles are not as strong, especially compared year-on-year to the very strong numbers last year. So, that's -- well, this is the phenomenon -- the outcome of the combining the two is what we're seeing today. But given another -- my view next year, the cycle could turn around and Brocade would go on. And then instead of a 3% year-on-year growth in this whole segment, we could end up with high single digits year-on-year growth rate because the core software revenue, as I've always indicated to you guys, you want to plan long term on mid-single-digit year-on-year growth rate. And that's very predictable part of our numbers.
Hock Tan: I'm sorry to disappoint you on your two parts, but it's too early for me to be able to give you a good answer or a very definitive answer on that. Because by far the majority of servers today are your traditional servers driving x86 CPUs. And the networking today are very, very still running Ethernet traditional data center networking. Because most enterprises if not virtually, all enterprises today are very much still running their own traditional servers on x86. Generative AI is something so new and in a way, so -- the limits of it is so extended that what we largely see today are at the hyperscale guys in terms of deploying at scale those generative AI infrastructures. Enterprises continue to deploy and operate standard x86 servers and Ethernet networking in the traditional data centers. And so, that's still -- so what we're seeing today may be early part of the whole cycle, that's your question, which is why I cannot give you any definitive view, opinion of how -- what the attach rate, what the ratio will be or if there's any stability that could be achieved anywhere in the near term. We both -- we see both running and coexisting very much together.
