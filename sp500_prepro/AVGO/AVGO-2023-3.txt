Hock Tan: Love to answer your question, Vivek, but I will not, not directly anyway because we do not discuss our dealings and especially specific dealings of the nature you're asking with respect to any particular customer. So that's not appropriate. But I tell you this in broad generality, many ways you look over in our long-term arrangements -- long-term agreements with our large North American OEM customer in wireless, very similar. We have a multiyear, very strategic engagement in usually more than one leading-edge technologies, which is what you need to create those kind of products, whether it's in wireless or in this case, in generative AI, multiple technologies that goes in creating the products they want. And it's multiple -- it's very strategic and it's multiyear and engagement is very broad and deep.
Hock Tan: You're asking me to guide beyond a quarter. I mean, hey, that's beyond my pay grade, Harlan. But I know. But I just want to point out to you, we promised you a soft landing, late fiscal '22, that likely '23 will be a soft landing. And as you pointed out and what, to my remarks, that's exactly what we are seeing.
Hock Tan: They go hand -- Ross, these things go very hand-in-hand. You don't deploy those AI engines in these days for generative AI, particularly in onsies or twosies anymore. They come in large clusters or parts as hyperscalers will call -- some hyperscalers will call it. And with that, it's -- you need a fabric, networking connectivity among thousands -- tens of thousands today of those AI engines, whether it's GPUs or some other customized AI silicon compute engine, the whole fabric with its AI engine represents literally the computer, the AI infrastructure. So it's hand-in-hand that our numbers are driven very, very correlated to not just AI engines, whether we do the AI engines or somebody else, merchant silicon does those GPU engines. We supply a lot of the Ethernet networking solutions.
Hock Tan: Well, there are a couple of assumptions one has to make, none of which I'm going to help you with, as you know, because I don't guide next year. But except to tell you our AI revenue, as we indicated, has been accelerating -- on an accelerating trajectory and no surprise. You guys hear that because deployment -- it's been extremely on an urgent basis and the demand we are seeing has been fairly strong, very strong. And so we see it accelerating through end of '22, now accelerating and continued to accelerate through end of '23 that we just indicated to you. And for fiscal '24, we expect somewhat a similar accelerating trend. And so to answer your question, we have always indicated previously that for fiscal '24, which is just, which is a forecast, we believe it will be over 25% of our revenue -- of our semiconductor revenue over 25% of our semiconductor revenue.
Hock Tan: Okay. Well, on the first question, you're talking about supply chain. Well, these products for generative AI, whether they are networking or -- and the customer engines, take long lead times. These are very, very leading-edge silicon products, both in terms of across the entire stack from the chip itself, to the packaging to even memory, the kind of HBM memory that is used in those chips. It's all very long lead time and very cutting-edge products. So we're trying to supply, like everybody else wants to have, within lead times. So by definition, you have constraints. And so do we, we have constraints. And we're trying to work through the constraints, but it's a lot of constraints. And you'll never change as long as demand, orders flow in shorter than the lead time needed for production because the production of these parts are very long extended, and that's the constraint we see as they come in faster than lead times along as orders come in. The answer on your second part, well, as far as we do see we are kind of, as I indicated, I call it soft lending. Another way of looking at it is that $6 billion approximately of non-AI related revenue per quarter is kind of bumping up and down on a plateau. Think of it that way. We -- growth is kind of down to very little, but it's still pretty stable up there. And so we have a range -- as I indicated, too, we don't have any one product in any one end market. We have multiple products. As you know, our portfolio is fairly broad, diversified and categorized into each end market with multiple different products. And each product runs on its own cadence sometimes on the timing on when customer wants it. And so you see bumping up and down different levels. But again it averages out over each quarter, as we pointed out, around $6 billion. And for now, we're seeing that happen.
Hock Tan: You know, Karl, it is Hock. Let me take a stab at this question because it's really a more holistic answer, and here's what I mean. The impact to us on gross margin more than anything else, it's not related to transactional supply chain issues. I'm sure they have in any particular point in time, but not as material and not as sustained in terms of impacting trends. What drives gross margin largely for us as a company is, frankly, a product mix, it's a product mix. And as I mentioned earlier, we have a broad range of products even as we try to make order out of it from a viewpoint of communication and segment them classify them into multiple end markets. Within the end market, your products, and they all have different gross margins depending on the -- on where they used and the criticality and various other aspects. So they're different. So we have a real mixed bank. And what drives the trend in gross margin more than anything else is the pace of adoption of next-generation products in each product category, so think in that way. And you measure it across multiple products. And each time a new generation of product -- of a particular product gets adopted, we get the opportunity to lift -- uplift gross margin. And therefore, the rate of adoption matters, for instance, because for some products that changes gross margin every few years versus one that's more extended one. You have different gross margin growth profile. And this is what is all tied to the most important variable. Now the more interesting thing to come down to us on a specifically your question is during '21, '22, in particular, with an up cycle in the semiconductor industry. We had a lot of lockdowns, change in behaviour, and a high level of demand for semiconductors. Or put it this way, a shortage of supply to demand. There was accelerated adoption of a lot of products, accelerated adoption. So we benefited, among other things, not just revenue, as I indicated, we benefited from gross margin expansion across the board as a higher percentage of our products out there gets adopted into the next-generation faster. We pass this. There is probably some slowdown in the adoption rate. And so gross margin expansion might actually not expand as fast. But it will work itself out over time. And I've always told you guys, the model this company has seen and is it's empirical, but based on this underlying basic economics, it's simply that when we have the broad range of products we have and each of them a different product life cycle of upgrading and next generation. We have seen over the years on a long-term basis, an expansion of gross margin on a consolidated basis for semiconductors that ranges from 50 to maybe 150 basis points on an annual basis. And that's a long-term basis. In between, of course, you've seen numbers that go over to 200 basis points. That happened in 2022. And so now later, you have to offset that with years where gross margin expansion might be much less like 50. And I think with that the process, you will see us go through on an ongoing basis.
Hock Tan: Let's start with lead times and asking me to predict when the up cycle would happen. It's still too early for me to want to predict that, to be honest with you, because even though we have 50 weeks lead time, I have overlaid on it today. Nice, a lot of bookings related to generative AI. A decent amount of bookings related to wireless, too. So that kind of like buyers, what I'm looking at. So the answer to you -- a very unsatisfactory, I know answer to your question is too early for me to tell, but we do have a decent amount of orders. All right.
Hock Tan: Let me say this. I made those specific notes or remarks on regulatory approval. I ask that you think it through, read it through and let's stop right there.
Hock Tan: That's a very interesting question. And frankly, my personal view is InfiniBand has been the choice in the old -- for years and years, generations of high -- what we call -- what we have called before high-performance computing, right? And high-performance computing was the old term for AI, by the way. So that was it because it was very dedicated application workloads and not a scale out as large language models drive today. We launched language models driving and most of -- all this large language models are now being driven a lot by the hyperscale. Frankly, you see Ethernet getting huge amount of traction. And Ethernet is shipping. It's not just getting traction to the future. It is shipping in many hyperscales. And -- it coexist best way to describe it with InfiniBand. And it all depends on the workloads. It depends on the particular application that's driving it. And at the end of the day, it also depends on, frankly, how large you want to scale your AI clusters. The larger you scale it, the more tendency you have to basically open it up to Ethernet.
Hock Tan: Well, thank you. That's a very insightful question. We only have one large customer in AI engines. We're not a GPU company, and we're not -- we don't do much compute, as you know, other than offload computing having said that, but it's very customized. And I mean, what I'm trying to say is that I don't want to mislead you guys. The fact that I may have engagement, and I'm not saying I do on a custom program should not at all be translated into your minds as oh, yes, this is a pipeline that will translate to revenue. Creating hardware infrastructure to run these large language models of hyperscalers is an extremely difficult and complex test and -- for anyone to do. And the fact that even if there is any engagement, it does not translate easily to revenues. And so suffice it to say, I leave it at that. I have one hyperscale who we are shipping custom AI engines to today and leave it at that, if you don't mind, okay? Now as far as customized switching, routing, sure. I mean, that happens. Most of the -- many of the -- those few OEMs, some OEMs who are supplying systems. Switch -- systems, which are switches or routers and have their own custom solutions together with their own proprietary network operating system. That's been the model for the last 20, 30 years. And today, 70% of the market is on merchant silicon. Not yet, I won't say for not the network operating system, but certainly for the silicon is merchant silicon. So the message here is, there's some advantages to doing a merchant solution here then to trying to do a custom solution as behavior or performance over the last 20 years have shown.
Hock Tan: Thanks. First and foremost, and you've heard me talked about in preceding quarter earning calls, and I continue to say it, and Kirsten reemphasized it today, we shipped very much to only end demand of our end customers. And we're looking beyond in enterprise, even beyond and telcos even beyond OEM. We look to the end users, the enterprises of those OEM customers. We try to. Doesn't mean we are right all the time, but we pretty much are getting very good at it. And we only ship to that. And what you're seeing is -- why I'm saying this -- what you're looking at, for instance, some numbers in broadband, some numbers in service storage that seems not quite as flat, which is why I made the point of purposely saying, but look at it collectively taking out generative AI. My whole portfolio of products out there. It's pretty broad, and it gets segmented into different end markets. And when we reach, I call it a plateau as we are in, you got a soft landing, as you call it, you never stay flat. There will be some products because of timing of shipments come in more and some ship out the wrong timing come in a bit lower. And in each quarter, we may show you differences, and we are showing some of it that differences in Q3 and some even in Q4. And that's largely related to logistics timing of customer shipments, particular customers and a whole range of products that go this way. This one I referred in my remarks as revenues, which are puts and takes around a median. And that median also paints to highlight to you guys has sit around $6 billion, and it has been sitting around $6 billion since the start of fiscal '23. And as we sit here in Q4, it's still at $6 billion. Now not quite there because there are some parts of it, they may go up, some parts of it go down. And that's the puts and takes we talk about. And I hope that pretty much addresses what you are trying to get at, which is, is this -- what's -- the fact that is it a trend? Or is it just a factor? And to use my expression, I call those flatters or puts and takes around a median that we're seeing here. And I wouldn't have said it if I'm not seeing it now for three quarters in a row, around $6 billion.
Hock Tan: You should. I did. I made my investment, at least you should look at it a bit. I'm just kidding. But we have invested in silicon photonics, which is, I mean, literally integrating in one single solution packaging. As an example, our switch, our next-generation Tomohawk5 switch, which will start shipping middle of next year, what we call the program we call the Bailly, a fully integrated switch silicon photonic switch. And you're right, very low power, and it's -- you make optics have always have optical and mechanical characteristics by sucking them into an integrated silicon photonics solution. You take away this failures on yield rates on mechanical, optical and translated to literally silicon yield rates. And so it's much -- we like to believe very reliable than the conventional approach. So your question is, so why won't more people jump in it? Well, because nobody else has done it. This -- we are pioneering this silicon photonic architecture. And we're going to -- we have appeal done, a POC, proof of concept in selling Tomahawk 4 in a couple of hyperscale, but not in production volume. We now feel comfortable we have reliability data from those instances. And that's why we feel comfortable to now go into production launch in Tomahawk5. But as people say, the proof is in the eating. And we will get it in one or two hyperscale who will demonstrate how efficient power-wise, effective it can be. And once we do that, we hope it will start to proliferate to other hyperscalers because they cannot do it. If one of them does it and reap the benefits of this silicon photonics solution, and it's there. You know it. I have indicated the power is simply enormous, 30%, 40% power reduction. And power is a big thing now in data centers, particularly, I would add, in generative AI data centers. That's a big use case that could come over the next couple of years. All right.
