Hock Tan: Well, yeah, thank you for that question and opportunity to clarify why we highlighted and why I highlighted it very purposefully. In 2022, generative is just barely starting to kick off. But they exist AI networks within the hyperscalers, particularly in fairly significant volume. And one we are trying to say is, very similar to CPUs, traditional CPUs in traditional workloads in those same data centers. We have constrained on performance of those silicon CPUs and Moore’s Law, we are starting to see scale out buying positioning rows and rows of server, CPUs and networking them together to work closely in parallel. As we step up to large language models in AI, generative AI in particular coming into play. GPUs are starting to be strung together in hundreds, soon to be thousands of racks and working in parallel and you know how that goes. And basically, those GPUs work in parallel in fairly synchronous manner to basically run and do what we call bulk parametric exchange, basically, run GPUs together, all AI engines together, whether they are GPUs, AI or TPUs or other AI engines. You run them together. It becomes network. The network becomes now potentially a critical part of this whole AI phenomenon in hardware. To make it work, you have got to put together many racks of AI engines in parallel, very similar to what we have been doing -- hyperscalers have been doing on CPUs to make them run faster, high performance as Moore’s Law comes to an end and doesn’t make any difference here in the form of AI engine. They come from silicon, they have -- they face similar constraints. So network becomes a problem -- becomes the constrained, network becomes a very key part of fulfilling generative AI dream here. And what we are saying here -- what I am saying in my comments is, last year, 2022, these are more AI -- what you call the AI workloads that are running in hyperscale and the advent of generative AI is still relatively fresh and new, we are doing $200 million as far as we could estimate of silicon, Ethernet switches and fabric that goes into those AI networks as far as we could identify in hyperscalers. With generative AI and the urgency and excitement of it coming in that we are seeing today. We are seeing that increase very, very dramatically and we are seeing urgency in our hyperscale customers coming to us to secure products, to secure ability to put in place those very, very low lossless, I would call, very low latency networks that can scale and Ethernet is what makes those networks scale.
Hock Tan: Yes. We are seeing all of the foregoing by the way and that happened over the last 90 days. We have seen a lot of that urgency, a lot of that, you might call it excitement, but you hit it right on.
Hock Tan: Yes. Which is accounting for the color in my commentary about both net -- generative AI-based network and pushing us to develop a new generation altogether of Ethernet switching that can support this kind of very compute and data intensive workloads. So that’s one side of it. And the other side of it, you are right, we have typically not want to talk much about compute offload, which is another way of saying, yeah, these are very related to some of the engines that certain -- that are fairly customized dedicated to certain hyperscalers.
Hock Tan: In a sort of broadly conceptual, not a guidance, as you say, but trend this way. We are kind of getting rather hopeful that it would be a soft lending. That will be moderation as we are indicating this future -- in this Q2 quarter moderating growth, but we see nonetheless, as probably leading to a soft landing of still a year-on-year improvement in the second half.
Hock Tan: Stacy, thank you for your question. Very perceptive. And as I say, we are not -- we are trying not to -- we are not guiding you guys what happens beyond the second quarter, not the second half of this year and...
Hock Tan: But having said that, no, we are still working through timing of when our customers need those urgent -- those products in a fairly urgent manner and our ability to obviously want to be very, very helpful to help customers launch aggressively into generative AI. So we are in the midst of that.
Hock Tan: Stacy, I wish you guys will not do too much analysis, but I know that won’t happen. I am only guiding Q2. I will let you figure out what happens in the second half. I think you are probably better of did than I am.
Hock Tan: C.J., I appreciate the fact that you have been definitely reading a lot of those Reuters and Bloomberg and Lexicon report. I appreciate that. And you equally know that I cannot -- I will not comment on any of this as we are working very, very positively and progressively with regulators on all the issues related to our clearance. So, sorry, I can’t comment. But just to let you know, we are making good progress.
Hock Tan: Okay. Well, it’s -- I am sure I don’t need to elaborate on what we all hear about on generative AI and it’s -- I think it’s still early innings on generative AI. But we obviously are also indicating, as we are seeing a very strong and a strong sense of urgency among our customers especially in the hyperscale environment to be -- to not miss out -- not to be late in this trend. And with generative AI, as I said, with many more -- much more billions of parameters that come into the models that they are doing. You are talking about scale out of A -- of data centers driving AI engines network together in a manner that we probably have not seen before. It’s not a problem that’s not solvable. It is very, very clearly solvable. As evidenced by the fact that we have and deploy technology to support AI networks even today to certain hyperscalers, where we are talking about at least hundreds and on thousands of AI engines, AI service network together and working in a synchronous manner. So this is about ability to scale out in a fairly substantial manner. And that was the color I was providing and it’s really about trying to make sure that happens and not be the bottleneck to our ability to get the best performance -- system performance and I emphasize the word system performance of an AI data center. And where it’s coming from right now is, frankly, how to network them and how to do those massive parametric exchange, so to say, when you run large numbers of engines or machines in parallel, as you grind through this huge database and that we need to do. So that’s -- we are in early innings and which is why we think we have time to come up -- to start to work on even a new generation of switches in Ethernet that are dedicated -- that are specifically designed dedicated to these kind of workloads, which are very different from the normal workloads that we see today traditionally in data centers and we have to address that. They have to be, as I say, literally lossless, virtually lossless, very low latency and be able to scale into thousands of engines and that’s the main three criteria we are aware of and we are driving solutions -- silicon solutions that enable that. We have it but we think we need to improve the performance of what we have to -- and in anticipation of a trend that we foresee over the next several years and so we are putting…
Hock Tan: Well, we intend to launch Tomahawk 5 early 2024 as we indicated previously and that’s the conventional silicon-based with pluggable optics switch top of the rack switch Tomahawk 5 51.2 terabit per second. Bailey, which is the fully integrated silicon photonic version. You don’t fully integrate the active component -- element -- active elements of those pluggable optics into the switch. We anticipate launching that shortly thereafter. Power wise, you can see silicon photonics, that’s a lot. The Tomahawk 5 compared to what we have today is 2x the performance of Tomahawk 4 and but we believe we can do Tomahawk 5 at the same power, close to the same power if not lower than Tomahawk 4.
Hock Tan: Well, I’d rather not answer that question, Ross. Highly sensitive to some of my very limited customer base. But as I said, it includes some of the engines -- the compute engines and some are related components that support this engine.
Hock Tan: Thanks. Good question, Ed. As you know, our wireless division -- group, as you call it, division, it’s really not one single product line or one single division, it’s not one homogenous group either and it is several -- a few key products that comprises this wireless division. All selling -- you are right -- you are correct, to the same application and very high end flagship status handsets and largely focused on one key customer in North America are much below the North American OEM customers. So in that sense, it’s one single focus area. And to answer your question, while we are multiple -- on these multiple products and they tend to keep progress as each new generation happens may not be every year, but it happens pretty -- fairly regular frequency on a cadence that is pretty predictable after while each on its own cadence. It’s a very, very good business for us. And to answer your question directly, no, it’s nothing significant -- meaningful has changed. Our relationship, our strategic engagement continues very much the same as it has for the last multiple years and we see that to continue in a fairly predictable stable manner.
Hock Tan: You post very, very interesting and good questions, Pierre. The problem is, I do not -- my customers -- hyperscale customers do not necessarily honor me by sharing all those insights that you -- and on those questions you are asking. I do not know. I do not know. All I know and what I do know, because I don’t sell CPUs. I don’t even sell them GPUs, by the way. But I know what you know out there, which is in certain areas of their business, we are seeing some of these hyperscalers bringing on a sense of urgency and focus, and of course, spending to be up to speed if not to not be left behind as we see the excitement hype perhaps in pushing applications and workloads in generative AI. That’s what we see driving a lot of this excitement, and we are always saying is, we have seen some of that effect on our networking business with those hyperscalers. That’s what it is. Beyond that, we, unfortunately, other than the backlog we get in normal networking switches, routers and key components, we see that. And as I indicated in the last quarter’s results, we continue to see sustained strength. Now last quarter and continuing as we indicate this particular quarter Q2. Beyond that, we don’t get to see -- we do not want to guide what we are going to see beyond that. But right now, last quarter, this quarter, yeah, traditional data centers scale out in networking and deployment in networking continues to be strong and sustained in hyperscalers as well I might indicate in the enterprise.
Hock Tan: Thank you for that question. Yes. Broadband is, for us, a very, very good business and very sustaining. Used to be boring. Boring is good at this point. And last quarter, Q1, as I reported, we actually grew 34% year-on-year. In my view, that’s rather exceptional even though in broadband, we have been seeing year-on-year growth now at least for the past four quarters, five quarters. But still 34% was rather exceptional and sure enough Q2, it normalizes to a most date level, but still growing. And the growth in that is simply because we are very well positioned with respect to next-generation PON, 10-gig PON, which has been deployed in big volumes now by telcos supported by the governments and countries all over Europe and even in North America, not to mention other country -- other nations beyond that. Basically, it is about reaching these key utility broadband service to every household and we see a lot of deployment. And then more vertical market, we also see simultaneous with PON or fiber, as you call it, a large strong continued deployment of cable, DOCSIS certainly coax to the home, because the cable operators, a few of them who on the scale of the telcos and who need to maintain competitiveness as the telcos launch 10 gigabit PON. That cable has to update DOCSIS to be able to compete and not lose subscribers in the market -- in the same market they compete against each other. So we see strength both in cable, DOCSIS 3.1, as I call it, and potentially next generation, not yet happening, but hopefully within the next couple of years, DOCSIS 4. -- 4.0. Meanwhile, PON is happening, which accounts for the strength we saw last quarter and continuing strength over the last several quarters and content increases come to not just unit deployment of those gateways and infrastructure, but also the fact that a lot of this deployments come with very high attach rates of WiFi 6 and 6E. And that provides additional boost, content increases more, what I would call it, to our revenue growth in broadband. So that’s quietly still chugging along very nicely for us. All right.
