Hock Tan: As I indicated, with the acquisition of VMware we're very focused on selling, upselling and helping customers, not just buy but deploy this private cloud what we call virtual private cloud solution or platform on their on-prem data centers. It has been very successful so far. And I agree it's early innings still at this point. We just have closed on the deal -- well, we closed on the deal late November, and we are now March, early March. So we had the benefit of at least three months, but we have been very prepared to launch and focus on this push initiative on private cloud, VCF. And the results has been very much what we expect it to be, which is very, very successful.
Hock Tan: There's a lot of questions, a lot of information you want me to disgorge. Let's take them one at a time, shall we. Yes, the increase, as we have said before as we shown before, it's roughly two-thirds, one-third or 70-30, which is AI celebrators which are custom ASIC AI accelerators this. We've a couple of hyperscalers compared to the other components, which are collectively considered as networking components. And it's about 70%, 30% mix. And that increase of almost $3 billion that you mentioned, it's a similar combination.
Hock Tan: I have indicated that I only have to really only have to say, I don't count anybody. I do not go into production as a rail customer at this point.
Hock Tan: Yes. No, our Tomahawk 5 is going great guns. Now it's not driven unlike in the past, Tomawak 3, Tomawak 4 by traditional scale-out in hyperscalers on their cloud environment. This is all largely coming from a scaling out of AI data centers. The building of larger and larger clusters to enable generative AI computing functionality. And you're going for bigger and bigger pipes. Hence, the Tomahawk 5 51 terabit is a perfect solution, and we're seeing a lot of demand. And in many cases, we are basically, they are surpassing the rate of adoption that we had previously thought. So it is a very good solution in connecting GPUs. And with respect to AI accelerators where I think you are focusing on, is that a constraint on supply chain? We do get enough lead time out of our hyperscale customers that we do not have a supply chain constraint.
Hock Tan: Yes, don't get too excited over that. I think it's certain products, contracts we obtained and -- but it's very strong contract renewals in the older -- from old Broadcom contracts, especially in mainframes were very strong, as was some of our other distributor software platform. So that has also accelerated, but that's not the star of this show, Stacy. Star this show is the accelerating bookings and backlog we are accumulating on VMware.
Hock Tan: I think now we haven't had it for that long, to be honest. It's like three months, about three months. But yes, it's what -- and it seems to be that things to work out, but things seem to be progressing very well as we had hoped it would. Because where we are focusing our go-to-market -- and more than go to market, where we are focusing our resources on don't just go to market but on engineering a very improved VCF stack, which we have and selling it out there and being able to then support it and in the process that help customers deploy and start to really make it stand up in your data centers. All that focus is on the largest, I would say, 2,000 strategic customers. These are guys who want to still have significant distributed data center on-prem. Many of our customers is looking at a hybrid situation, not trying to use the word too loosely. Basically, a lot of these customers had some very legacy but critical mainframe. That's an old platform not growing, except it's still vital. Then what they have in modernizing workloads cut today and in the future, is they really have a choice, which they are taking both angles of running a lot of applications in data centers on-prem distributed data centers on-prem, which can handle these modernized workloads while at the same time to -- because of elastic demand, to be able to also put some of these applications into public cloud. Today's environment, most of these customers do not have an on-prem data center that resembles what's in the cloud, which is very high availability, very low latency, highly resilient, which is one we are offering with VMware Cloud Foundation of VCF. It's exactly replicate what they get in a public cloud. And they love it. Now three months. But we are seeing it in the level of bookings we are generating over the last three months.
Hock Tan: Chris, thanks for this question. Love it. Because perhaps let me try to perhaps give you a sense how we think of the AI market, the new generative AI market, so to speak, using it very loosely and generically as well. It's really -- we see it as two broad segments. One segment is hyperscalers, especially very large hyperscalers with huge, huge consumer subscriber base. You probably can guess who this few people are, very large subscriber base and very -- an almost infinite amount of data.  And their model is getting subscribers to keep using this platform they have. And through that, be able to generate a better experience for not only the subscribers, but a better advertising opportunity for their advertising clients. It's a great ROI as we are seeing ROI that comes very quickly. And the investment continues vigorously with those -- with that segment, comprising very few players. But we will choose subscriber base, but with the scale to invest a lot. And here, ASICs custom silicon, custom AI accelerators makes plenty of sense and that's where we focus that attention on. They also buy as a scale out those AI accelerators, through clusters increasing large clusters because of the way the models are running, the foundation models run and large language models need to generate those parameters. They buy a lot of networking together with it. But in comparison, obviously, to the value of AI accelerators we sell. Now the network working side, while growing its small percentage compared to the size, the value of the accelerators. That's one big segment we have. The other segment we have, which is smaller is the enterprise, what I broadly call enterprise segment in AI. Here, you're talking about companies, large not so large, but large who wants to do -- who have AI initiatives going on. All these big news and hype about AI being the savior to productivity and all that gets all these companies on multiple on their own initiatives. And here, short of going to public cloud, they're trying to run it on-prem. If they try to run it on-prem, they take standard silicon for an AI accelerators as much as possible. And here, in terms of the AI accelerator, we don't have a market. That's the merchant silicon market. But in the networking side as they tie it together with their data centers, they do buy all those are -- our networking components beginning with switches, routers even through people like Arista 7800 but switches for sure, and the various other components I mentioned. And that's a different sense market that we have. So it's an interesting mix, and we see both.
Hock Tan: You're correct. We are -- as I say, we are almost like near the trough. This year, '24, first half, for sure, will be the trough. Second half 24, don't know yet. But I tell you what, we have 52-week lead time, as you know. We are very disciplined in sticking to it. And based on that, we are seeing bookings lately, significantly up from bookings a year ago.
Hock Tan: Okay. Before you get carried away, please, and those in the other categories outside AI accelerators, all those things that PAM-4, DSPs, optical components, retirements. They are small compared to Tomahawk switches and Jericho routers using AI networks. And also being in an environment where, as you all know, traditional enterprise networking is kind of also in a bit of a slowdown now.  So always think, it's demand driven very much by AI. And that tends to push us in a line of thinking that could be very biased because what it is showing is that the mix and the content on networking relative to compute, is very skewed very different from -- in an AI data center compared to a traditional CPU-based data center. So I don't want to get you guys all in the wrong way. But you're right, in the AI data center, there's a lot of -- there's quite a bit of content on DSPs, PAM4s, optical components and retirements and PCI Express switches. But they are still not that big in the overall scheme of things compared to what we sell in switches and routers. And compared to AI accelerators, they are even small, I think in that ratio. As I said, AI revenue of $10 billion plus this year, 70% would be AI accelerators. 30%, everything else. And within that everything else, 30% or so, I would say more than half of that 30%, more like 20% are the switches and routers. And the rest are other various retirement DSP components because we are not -- unlike what you said, we're not vertically integrated in the sense we do not do the entire transceiver the optical transceiver. We don't do that. Those are manufactured typically by OEM contract manufacturers like in online, Eoptolink guys in China. Where those guys are much more competitive, but we provide those key components we talk about. So when you look at it that way, you can understand the kind of the weighting of the various values.
Hock Tan: You know, I can't stop somebody from trash talking, okay? It's the best way to describe it. Let the numbers speak for themselves, please, and leave it that way. And I add to it like most things we do in terms of large critical technology products. We tend to always have, as we do here, a very deep strategic and multiyear relationship with our customer.
Hock Tan: Well, number one, we don't dominate this market. I only have two. I canâ€™t be dominating it, too and number one. Number two, the second point is very -- it takes years. It takes a lot of heavy lifting to create that custom silicon because you need to do more than just hardware of silicon to really have a solution for generative AI or even AI in trying to create those AI capabilities in our data centers. It's more than just silicon.  You have to invest a lot in creating software models that works on your custom silicon that matches. You've got to match your business model in the first place, which leads to and create foundation models which then needs to work and optimize on the custom silicon you are developing. So it's an iterative process. And it's a constant evolving process even for the same customer we deal with. I mentioned that in the last call.  So it takes years to really understand or to be able to basically reach a point where you can say that, hey, I'm finally delivering production worthy -- and it's not because silicon is bad. It's because it doesn't work well with the foundation models that the customer put in place and a software layer that works with it, the firmware, the software layer that translates into it. All that has to work you are almost like creating an entire ecosystem on -- in a limited basis, which we are recognized very well in x86 CPUs, but in GPUs, those kind of AI accelerators is something still very early stage. So it takes years. And for our two customers, we have engaged for years. With one of them, we have engaged for eight years to get to this point. So it's something you have to be very patient, persevere and hope that everything lines up because ultimate success, if you are just a silicon developer, it's not just dependent on you, but dependent as much even more on your partner or customer doing it. So just got to be patient guys. I got the two only so far.
Hock Tan: Oh, custom silicon market. I have no comment to be made on it. All I do say is I have no interest in going into a market where -- we have a philosophy in running our business, Broadcom. And maybe other people have a different philosophy. Let me tell you my simple philosophy, which I've articulated over time, every now and then, which is very clear to my management team and to the whole Broadcom.  You do what you're good at. And you do -- you keep doubling down on things you know you are better than anybody else. And you just keep doubling down because nobody else will catch up to you if you keep running in of the pack. But do not do something that you think you can do, but somebody else is doing much better job than you are. That's my philosophy.
Hock Tan: I'll take that, because you're asking business model, you're not asking really number crunching. So let me try to answer in this way. No, there's no particular reason shot of what constitutes an AI accelerator. An AI accelerator, the way it's configured now, whether it's a merchant or its customer has a -- for AI accelerator to run foundation models very well needs not just a whole bunch of loads of floating point multipliers to do matrix multiplication, matrix analysis on regression. That's the logic part, compute part. It comes -- you have to come with access to a lot of memory, literally almost cash memory tied to it. The chip is not just a simple multiplier. It has -- it comes attached to memory. It's almost a layer 3-dimensional chip, which it is. Memory is not something we -- any of us in AI accelerators are super good at designing or building. So we buy the memory from very specialized high-bandwidth memory, you all know about that, from key memory suppliers. Every one of us does that. So you parted combine the two together, that's what an AI accelerator is.  So even if I get very good net corporate silicon gross margin on mine compute, logic chip on multipliers. There's no way I can apply that kind of add-on margin to the high-bandwidth memory, which is a big part of the cost of the total chip. And so naturally, by simple math, it will -- that hold an entire consolidated AI accelerator brings a gross margin below on a traditional silicon product we have out there. No going away from that because you are adding on memory, even though we have to create the excess, the IOs that attach in, we do not and could not justify adding that kind of margin to memory, Nobody could, for us. So it brings a natural lower margin. That's really the simple basis to it. But on the logic part of it, sure, with the kind of content with the kind of IP that we develop cutting edge to make those high-density floating point multipliance [ph] on 800 square millimeters of advanced silicon we can command the margin similar to our corporate gross margin.
Hock Tan: You better go on to VMware customers. And because on the first -- I don't tell about my customer individually, sorry.
Hock Tan: We have a shift here. And it's interesting. You're right, now we've got. We are spending more on go-to-market and support because we have a lot of customers with VMware. They are 300,000, but we stratify. So we have the strategic guys. We sell, upsell VC private cloud very good. But the long tail of what we call smaller commercial customers, we continue to support and sell improved versions of just vSphere compute virtualization to improve productivity on their service.  We don't attempt to say, go build up your whole VCM. They don't have the skills, don't have the scale to do it. But only it adds up you're right, costs of my spend, OpEx spend, be it support services, go-to-market will increase. But the difference between that and, say, CA, under acquisition we did is we're growing this business very fast. And you don't have to increase your spend growing this business. So we have operating leverage through revenue growth over the next three years.
Hock Tan: Well, we find now that we could generate more value to you, the shareholders. I assume you are -- I'm just kidding, but we would generate more value to our shareholders by taking Carbon Black, which is not that big and integrating it, Symantec, that by doing it, we would generate much better value to our shareholders than taking a one-shot divestiture on this asset, not particularly large to begin with.
