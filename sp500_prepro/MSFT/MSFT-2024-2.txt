Satya Nadella: Yes, let me -- just on the inferencing and training, most of what you've seen for the most part is all inferencing. So, none of the large model training stuff is in any of our higher numbers at all. What small batch training, so somebody is doing fine-tuning or what have you, that will be there, but that's sort of a minor part. So, most of what you see in the Azure number is broadly inferencing. And Mark, I think it may be helpful to sort of think about, like what is the new workload in AI? The new workload in AI, obviously, in our case starts with one of the frontier -- I mean, starts with the Frontier model Azure OpenAI. But it's not just about just one model, right. So, you first -- you take that model, you do RLHF, you may do some fine-tuning, you do retrieval, which means you are sort of either heating some storage meter or you're heating some compute meters. And so to -- and by the way, you will also distil a large model to a small model and that would be a training perhaps. But that's a small batch training that uses essentially inference infrastructure. So, I think that's what's happening. So you could even say these AI workloads themselves will have a lifecycle which is they'll get rebuilt, then there'll be continuously optimized over time. So, that's sort of one-side. And I think if I understand your question, what's happening with the traditional optimization, and I think last quarter we said. One, we're going to continue to have these cycles where people will build new workloads, they will optimize the workloads and then they'll start new workloads. So I think that that's what we continue to see. But that period of massive, I'll call it, optimization only and no new workloads start, that I think has ended at this point. So what you're seeing is much more of that continuous cycles by customers, both whether it comes to AI or whether it comes to the traditional workloads.
Satya Nadella: Yes, I think it's going to have a very foundational impact. In fact, you could say the core compute architecture itself changes, everything from power, power density to the data center design, to what used to be the accelerator, now is the sort of the main CPU, so to speak, or the main compute unit. And so, I think in the network, the memory architecture, all of it. So as the core computer architecture changes, I think every workload changes. And so yes, so there is a full, like, take our data layer, the most exciting thing for me in the last year has been to see how our data layer has evolved to be built for AI, right? If you think about Fabric, one of the genius of Fabric is to be able to say, let's separate out storage from the compute layer. In compute we'll have traditional SQL, we’ll have Spark. And by the way, you can have an Azure AI job on top of the same data lake, so to speak, or the lakehouse pattern. And then the business model you can combine all of those different compute. So that's the type of compute architecture. So it's sort of a -- so that's just one example. The tool stuff is changing. Office, I mean if you think about what -- if I look at Copilot; Copilots extensibility with GPT, Copilot apps to the Copilot stack, that's another sort of part of what's happening to the tech stack. So yes, I mean, definitely builds. I mean. I do believe, being in the cloud has been very helpful to build AI. But now, AI is just redefining what it means to have, what the cloud looks like, both at the infrastructure level and the app model.
Satya Nadella: No, thank you for the question, Brad. So, a couple of things. In my comments I said increase in relation to our previous suites like, let's say, E3 or E5. Whatever two months in, it's definitely much faster than that. And so, from that perspective. It's exciting to see, I’d say, the demand signal, the deployment signal. I was looking at by tenant, even usage, it's faster than anything else because it's easier, right. I mean, it's sort of -- it shows up in your app, if you click on it, like any ribbon thing and it becomes a daily habit. So it in fact, it reminds me a little bit of sort of the back-in-the day of PC adoption, right. It's kind of -- I think it first starts off with few people having access. There are many companies that are doing standard issue, right. So just like PCs became standard issue at some point after PCs being adopted by early adopters. I think that's the cycle that at least we expect. In terms of what we're seeing, it's actually interesting, If you look at the data we have, summarization, that's what it's like number-one, like I'm doing summarization of Teams meetings inside of Teams, during the meeting, after the meeting, word documents summarization, I get something in email on summarizing. So summarization has become a big deal. Drafts, right, you're drafting emails, drafting documents. So, anytime you want to start something, the blank page thing goes away and you start by prompting and drafting. Chat, to me, the most powerful feature is now you have the most important database in your company, which happens to be the database of your documents and communications. It is now queryable by natural language in a powerful way, right. I can go and say, what are all the things Amy said, I should be watching out for next quarter and it will come out with great detail. And so Chat, summarization, draft, also by the way, actions. One of the most used thing is, here's the Word document, go complete, I mean, create a PowerPoint for me. So, those are the stuff that is also beginning. So, I feel like these all become -- but fundamentally, what happens is, if you remember the PC adoption cycle, what it did was work artifact and work flow changed, right. You can imagine what forecasting was before excel and email and what it was after. So similarly, you'll see work and workflow change as people summarize faster, draft regulatory submissions faster. Chat to get knowledge from your business. And so, those are the things that we are seeing as overall patterns.
Satya Nadella: Azure OpenAI and then OpenAIs on APIs on top of Azure would be the sort of the major drivers. But there is a lot of the small batch training that goes on, whether it's let Jeff for fine-tuning. And then lot of people who are you starting to use models as a service with all the other new models, but it's predominantly Azure open AI today.
Satya Nadella: Yeah. I mean -- it's -- I always go back to sort of my own conviction that this generation of AI is going to be different, started with the move from 2.5 to 3 of GPT. And then it's use inside of developer scenario with GitHub copilot and so yes. I think this is the place where it's most evolved. In terms of its economic benefits or productivity benefits and you see it. We see it inside of Microsoft, we see it in all of the key studies we put out of customers, everybody had talked to its pick-up, but it is the one place where it's becoming standard issue for any developer is like if you takeaways spell check from Word, I'll be unemployable. And similarly, it will be like -- I think GitHub Copilot becomes core to anybody who is doing software development. The thing that you brought up is a little bit of a continuation to how Amy talked about, right. So you are going to start seeing people think of these tools as productivity enhancers, right. I mean, if I look at it, our ARPUs have been great, but they're pretty low.. You know even though we've had a lot of success, it's not like we had a high-priced ARPU company. I think what you're going to start finding is, whether it's sales copilot or service copilot or GitHub Copilot of security copilot. They are going to fundamentally capture some of the value they drive in terms of the productivity of the OpEx, right. So it's like 2 points, 3 points of OpEx leverage would be goal is on software spend. I think that's a pretty straightforward value equation. And so that's the first time, I mean this is not something we've been able to make the case for before whereas now I think we have that case. Then even the horizontal copilot is what Amy was talking about, which is at the Office 365 or Microsoft 365 level, even there, you can make the same argument whatever ARPU we may even have with E5, now, you can see incrementally as a percentage of the OpEx, how much would you pay for a copilot to give you more time savings for example. And so yes, I think all up, I do see this as a new vector for us in what I'll call the next phase of knowledge work and frontline work even in their productivity and how we participate. And I think GitHub copilot, I never thought of the tools business as fundamentally participating in the operating expenses of a company's spend on, let's say, development activity and now you're seeing that transition, it is just not tools, it's about productivity of your dev team.
Amy Hood: No, maybe I'll just add just a few things to that. I think whether you use the word lapping, these optimization comparables or the comparables easing, is all sort of the same thing, that we're getting to that point, in H2 that's absolutely true. We'd like to talk about the contribution of AI specifically for the reason Satya talked about, these are -- this is starting to see the application of AI at scale. And we want to be able to show people, this is how that point will work, it's inferencing workloads where people are expecting productivity gains, other benefits that grow revenue and so, I do think about those as both related. And ultimately the TAM that we go after is best sort of across both of those, both AI workload and I guess, “non-AI workload” although to Satya's point, you need all of it.
Amy Hood: Thanks, Brent. First of all, thanks for the question. The teams are obviously been hard at work on this topic. We do point out that, Q2 because of the impact of the charge a year ago, you're seeing larger margin improvement than I would say, sort of a run-rate margin improvement. So, let me first say that. Secondly, the absolute margin improvement has also been very good and it speaks to, I think one of the things Satya talked about and I reiterated a bit, which is that, we want really to make sure we're making investments, we're making them in consistency across the tech stack. The tech stack we're building, no matter what team is on, is inclusive of AI enablement. And so think about as building that consistency without needing to add a lot of resources to do that. It's been a real pivot of our entire investment infrastructure to be working on this work. And I think that's important, because it means you're shifting to an AI-first position, not just in the language we use, but in what people are working on day-to-day. That does obviously create a leverage opportunity. There has also been really good work put in by many teams on improving the gross margin of the products; we talked about it with Office 365, we talked about in Azure core. We even talked about it across our devices portfolio, where we've seen material improvements over the course of the year. And so, when you kind of take improvements at the gross margin level, plus this consistency of re-pivoting our workforce toward the AI-first work we're doing, without adding material number of people to the workforce, you end up with that type of leverage. And we still need to be investing. And so, the important part, invest towards the thing that's going to shape the next decade and continue to stay focused on being able to deliver your day-to-day commitments. And so it's a great question. And hopefully, that helps piece apart a few of the components.
Amy Hood: Thanks, Karl. Maybe I'll start and Satya feel free to add on. I think we feel really good about where we have been in terms of adding capacity. You started to see the acceleration in our capital expense starting almost a year ago, and you've seen us scale through that process. And that is going toward as we talked about Servers and also new data center footprints to be able to meet what we see as this demand and really changing demand as we look forward. And so, I do feel like the team has done a very good job. I feel like, primarily obviously, this is being built by us, but we've also used third-party capacity to help when we could have that help us in terms of meeting customer demand. And I think looking forward, you'll tend to C&I guide toward it, accelerated capital expense to continue to be able to add capacity in the coming quarters, given what we see in terms of pipeline.
Amy Hood: And maybe just to add two points. One of the exciting things as I said for some companies, it's going to be standard issue like PC, for other companies, they may want to do a land with a smaller group, see the productivity gains and then expand. And so being able to lift some of the seat requirements that we did earlier this month, it's really going to allow customers to be able to use that approach too. And the other thing I would add, we always talk about in enterprise software, you sell software, then you wait and then it gets deployed, and then after deployment, you want to see usage. And in particular, what we've seen and you would expect this, in some ways with Copilot even in the early stages, obviously, deployment happens very quickly. But really what we're seeing is engagement grow. As to Satya's point on how you learn and your behavior changes you see engagement grow with time. And so I think those are just to put a pin in that, because it's an important dynamic when we think about the optimism you hear from us.
Amy Hood: Mark, without getting into tons of line items, it's more simple to think of it as really, it's people adopting it for inferencing at the API generally. I mean that's the easiest way to think about it. And we also saw growth in GitHub Copilot which you talked -- which Satya talked about and we saw a growing number of third parties using it in some small ways for training. But this is primarily an inferencing workload right now in terms of what's driving that number. We used to think of it that way.
Amy Hood: That's a great question, Brad. Let me split apart the components. And then we can come back to whether they should equalize or just go on sort of a bit, actually believe it or not, somewhat independent trajectory and I will explain why I say that. Your seat growth as we talk about is primarily from, at this point, small and medium-size businesses and really frontline workers scenarios. And to your point on occasion, those are lower ARPU seats, but there are also -- there are new seats and so you see that in the seat count number. And as we get through and we've seen that come down a little bit quarter-over-quarter and we've guided for that really to happen again next quarter, but a very separate thing is being able to add ARPU. And traditionally, and again this quarter, right, that's come over-time from E3. Then from E5. And we're continuing to see very healthy seat momentum and you heard very good renewals. So, all of that, right, completely independent in some way from seat growth. Then the next thing, that actually we just talked about, maybe in Brad's question I'm trying to recall is that, you're going to see Copilot revenue will run there as ARPU, right. That won't show a seat growth. So you'll have E3, E5 transition, Copilot, all show-up in ARPU over time, and then you’ll have the seat growth be primarily still small business and frontline worker and maybe new industry scenarios. So, I tend to not really, Brad, think about them as related lines, believe it or not. I think about them as sort of unique Independent motions we run and there is still room for seat growth and obviously with the levers we've talked about, there's room for ARPU growth as well.
