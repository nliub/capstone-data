Amy Hood: Thanks, Keith. In some ways, let me start by saying, it's a great partnership. We're proud to have worked together for a number of years, leading to some of the announcements that you've heard us make more recently. And we talked about the foundation of our partnership remains that when we both are successful, the other benefits. When we grow, it helps them, and when they grow, it helps us. But specifically to your question on how does it show up, it's easiest in this situation, to think about them as a customer of ours like any other customer who would use the Azure infrastructure and our Azure AI services in service of supporting their end customers. And so when they do that, like any other customer who has a commercial relationship with us, we recognize revenue on that behalf. That's probably the simplest frame, Keith, that I hope is helpful.
Amy Hood: Mark, maybe the one thing I would add to those comments is, we've been through almost a year where that pivot that Satya talked about from we're starting tons of new workloads, and we'll call that the pandemic time, to this transition post, and we're coming to really the anniversary of that starting. And so to talk to your point, we're continuing to set optimization. But at some point, workloads just can't be optimized much further. And when you start to anniversary that, you do see that it gets a little bit easier in terms of the comps year-over-year. And so you even see that in a little bit of our guidance, some of that impact from a year-over-year basis.
Amy Hood: Yeah, Brent. The best way of thinking about this is when we believe we’re adding a lot of value. And frankly, that’s what the Copilots are doing and some productivity improvement, you can expect that we will have this price for those and you’ll be able to look at that as we get to release, and to Satya’s point, GitHub Copilot is a great example.
Amy Hood: And Raimo, we talked a bit about this when we talked about the new Edge and the new Bing with analysts. And I think one of the important things to keep in mind, which Satya is pointing to is that it's not really just the cost of Azure and the ability to optimize across the Azure workloads. It's that really even our first-party workloads and apps that are built, right, are built on the same platform, and we're able because we are a hyperscaler and because we have a large commercial cloud first-party as well as consumer apps like Bing that are first party, we're able to take advantage of that and GPU utilization, AI services utilization across the stack. And so it's not just sort of where Satya wanted to see even a broader benefit of being a hyperscaler.
Amy Hood: Thanks, Keith. It's probably a good opportunity to explain a bit about how I think about where we are, which is -- if you look at all of the businesses we're in and we look about our competitiveness in those businesses. And this is before Satya started to comment a bit about our relative performance versus absolute. And I'll tell you that the energy and focus we put right now is on relative performance and share gains. Right now, we have the largest commercial cloud with increasing commitments by customers, with new workloads, new TAM opportunities that something you're talking about with customers. And our focus is going to be and will be on continuing to take a growing share of that while we continue to focus on our customers' success in getting a ton of value out of what we are selling, whether that's the E5 product, the Microsoft 365, whether that's Windows 365 to help, maybe it's on compute costs and PC cost, whether it's working across the Azure stack. And so with that opportunity plus in our consumer business, the largest number of active devices we've had in Windows, are still being used more being able to focus on edge share and being share and gaming -- bringing it to the PC as well across to mobile, these are the opportunities that we focus on as we think about next year. And so if we feel like and I do, that we are well positioned to continue to take share in so many key places, then I say, great, and we want to be able to focus on investing in AI, which I talked about, will increase COGS growth, but we're committed to making sure we have healthy profitability by keeping operating expenses low. And so what really this past year has been about, but really what Q3 starts to show is our willingness to pivot to the future to make sure we can keep all those commitments that Satya listed. So while I know that's not giving specificity, it is, in fact, how we think about long-term success, in being well positioned in big markets, taking share in those markets, committing to make sure we're going to lead this wave, staying focused on gross margin improvements where we can. Some of them will come in AI over time, given our commitment to the build-out. We will charge for those AI capabilities and then ultimately will deliver operating profit.
Amy Hood: Thanks, Karl. I think I would step back and say we have seen, I mean, the Office 365 suite but broadly, the Microsoft 365 suite adds a ton of value for users. And so if you think about the users and on the global base, we've been able to add users, which you continue to see. We still have in the frontline scenario at an SMB opportunity to continue to grow. And in the enterprise, where we are a basic productivity tool, the labor market is still tight in most places, and we continue to see customers committed to the value they're getting. And so I -- this is not something that -- I think our focus has really been on continuing to get healthy renewals, continue to add new products at renewal where it makes sense to save customers money and increase value. And so I think that's the story of the resilience you're seeing. And of course, we did have a good E5 quarter, which you're starting to see and it helps on ARPU.
Amy Hood: And maybe one thing I would add, Michael, is that I know the question is consolidation, but another aspect of that is that some of the new business process automation work that's going to get done, whether that's the Dynamics workload that we've talked about, it also will benefit from having the AI services available on Azure, from having the core Azure capabilities as well as actually some front-end stuff that people are buying in Microsoft 365 to close these loops in a new way. And so I think maybe it's not the traditional definition of consolidation. But when people look and say, what vendor adds a lot of value and has the tools that we need and, in many instances, already own to be able to do this business process work, I think we have a great value and, frankly, probably leading tools in almost every vertical.
Satya Nadella: Thanks, Mark for the question. Maybe I'll make three comments. And it's also important, I think to distinguish between what I'd say, macro or absolute performance and relative performance because I think that's perhaps a good way to think about how we manage our business. First is optimizations do continue. In fact, we are focused on it. We incent our people to help our customers with optimization because we believe in the long run that the best way to secure the loyalty and long-term contracts with customers when they know that they can count on a cloud provider like us to help them continuously optimize their workload. That's sort of the fundamental benefit of public cloud, and we are taking every opportunity to prove that out with customers in real time. The second thing I'd say is, we do have new workloads started because if you think about it, during the pandemic, it was all about new workloads and scaling workloads. But pre pandemic, there was a balance between optimizations and new workloads. So what we're seeing now is the new workloads start in addition to highly intense optimization driven that we have. The third is perhaps more of a relative statement because of some of the work we've done in AI even in the last couple of quarters, we are now seeing conversations we never had, whether it's coming through you and just OpenAI's API, right? If you think about the consumer tech companies that are all spinning essentially Azure meters, because they have gone to open AI and are using their API. These were not customers of Azure at all. Second, even Azure OpenAI API customers are all new, and the workload conversations, whether it's B2C conversations in financial services or drug discovery on another side, these are all new workloads that we really were not in the game in the past, whereas we now are. So those are the three comments that I'd make, both in terms of absolute macro, but more importantly, I think, what is our relative market position and how it's being changed.
Satya Nadella: I mean, overall, we do plan to monetize a separate set of meters across all of the tech stack, whether they're consumption meters or per user subscriptions. The Copilot that's priced and it is there is GitHub Copilot. That's a good example of incrementally how we monetize the prices that are there out there and others are to be priced because they're in preview mode. But you can expect us to do what we've done with GitHub Copilot pretty much across the board.
Satya Nadella: Yes, a couple of them. One is clearly the accelerated compute is what gets used to drive AI. And the thing that we are very, very focused on is to make sure that we get it very efficient in the usage of those resources. If you think about sort of what the hyperscaler does, it's not just rack and stack sort of hardware. They use software to optimize the performance of a given workload and, in fact, heterogeneous workloads on a given set of hardware. And so we have many knobs that we’ll continuously continue to drive optimization across it. And you see it even in the -- even for a given generation of a large model, where we started them to the cost footprint to where we end in the cost footprint even in a period of a quarter changes. So you can expect us to do what we have done over the decade plus with the public cloud to bring the benefits of, I would say, continuous optimization of our COGS to a diverse set of workloads. The other thing I'd mention is that there are a lot of workloads now. Like one of the reasons why we got together with OpenAI primarily was we came out and said, this type of workload, whether it's a training or an inference workload is going to be much more generally relevant for us, not just in the context of AI. And so you can see us apply it in other context as well.
Satya Nadella: Yeah. I mean, just to add to it, during these periods of transition, the way I think as shareholders, you may want to look at is what's the opportunity set ahead. We have a differentiated play to go after that opportunity set, which we believe we have. Both the opportunity set in terms of TAM is bigger, and our differentiation at the very start of a cycle, we feel we have a good lead and we have differentiated offerings up and down the stack. And so therefore, that's the sort of approach we're going to take, which is how do we maximize the return of that starting position for you all as shareholders long term. That's sort of where we look at it. And we'll manage the P&L carefully driving operating leverage in a disciplined way but not being shy of investing where we need to invest in order to grab the long-term opportunity. And so obviously, we will see share gains first, usage first, then GM, then OPInc, right, like a classic P&L flow. But we feel good about our position.
Satya Nadella: Yeah. Thanks, Rishi for the question. So overall, we've taken the approach that we are not waiting for regulation to show up. We are taking an approach where the unintended consequences of any new technology is something that from day 1, we think, about as first class and build into our engineering process all the safeguards. So for example, in 2016 is when we put out the AI principles, we translated the AR principles into a set of internal standards that then are further translated into an implementation process that then we hold ourselves to internal audit essentially. So that's the framework we have. We have a Chief AI Officer who is sort of responsible for both thinking of what the standards are and then the people who even help us internally audit our following of the process. And so we feel very, very good in terms of us being able to create trust in the systems we put out there. And so we will obviously engage with any regulation that comes up in any jurisdiction. But quite honestly, we think that the more there is any form of trust as a differentiated position in AI, I think we stand to gain from that.
Satya Nadella: Yeah. I mean, I'll start and Amy, you can add. I think Amy referenced to even in the context of Microsoft 365 and Office 365. But fundamentally, what we are focused on is making sure that the customers are able to derive the value out of our offerings, right, whether it's the Microsoft 365 suite value, which is significant, whether it's E3 or E5. And we want to make sure that our -- they're getting deployed, they're getting used and that's obviously going to lead to our share gains in many cases. Same thing in security. That's a place where this quarter, you saw some good results from us. And same on up and down the stack across Azure, right? So when you think about AI, the anatomy of an AI application is not just an AI model. In fact, ChatGPT itself is a great example. ChatGPT, for example, uses Cosmos DB as their core database. And so therefore, we want to make sure that our services, as they are competitive, get used together, whether it's the IaaS layer, the PaaS layer or the SaaS layer.
