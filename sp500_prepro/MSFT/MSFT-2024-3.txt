Satya Nadella: Thank you, Keith for the question, let me start and then Amy, you can add. At a high level, the way we, as a management team, talk about it is there are two sides to this, right? There is training and their inference. Given that we want to be a leader in this big generational shift and paradigm shift in technology, that's on the training side. We want to be able to allocate the capital required to essentially be training these large foundation models and stay on the leadership position there. And we've done that successfully all the way today, and you've seen it flow through our P&L, and you can continue to see that going forward. Then Amy referenced what we also do on the inference side, which is, one, we first innovate and build products. And of course, we have an infrastructure business that's also dependent on a lot of ISVs building products that run on our infrastructure. And it's all going to be demand driven. In other words, we track – we're closely what's happening with inference demand, and that's something that we will manage, as Amy said in her remarks very, very closely. So we feel – and obviously, we've been doing this, quite frankly, Keith, for now multiple years. So this is not the quarter. I realize in the news, it's a lot more in the quarter nowadays. But if you look at it, we have been doing what is essentially capital allocation to be a leader in AI for multiple years now, and we plan to sort of essentially keep taking that forward.
Satya Nadella: Great question, Brent. There are a couple of things I’d say. On the Azure side, which I think is what you specifically asked, we feel very good about the – we are fundamentally a share taker there because if you look at it from our perspective at this point, Azure has become a protocol for pretty much anybody who is doing an AI project. And so that's sort of been a significant help for us in terms of acquiring even new customers. Some of the logos I even referenced in my remarks, our new Azure customers. So, that's one. The second thing that we're also seeing is AI just doesn't sit on its own. So, AI projects obviously start with calls to AI models, but they also use a vector database. In fact, Azure Search, which is really used by even ChatGPT is one of the fastest-growing services for us. We have Fabric integration to Azure AI. And so, Cosmos DB integration. So, the data tier, the dev tools is another place where we are seeing great traction. So, we are seeing adjacent services in Azure that get attached to AI. And lastly, I would say, migrations to Azure as well. So, this is not just all an AI story. We are also looking at customers, I mean, this is something that we have talked about in the past, which is there's always an optimization cycle, but there's also – as people optimize, they spend money on new project starts, which will grow and then they'll optimize. So, it's a continuous side of it. So, these are the three trends that are playing out on Azure in terms of what at least we see on demand side.
Satya Nadella: Yes. Great set of questions, Mark. Let me just start by saying, a good place to start is to watch what’s happening in terms of standard issues for software teams, right? I mean if you think about it, they bought tools in the past. Now, you basically buy tools plus Copilot, right? So, you could even say that this is characterized as perhaps shift of what is OpEx dollars into effectively tool spend because it gives operating leverage to all of the OpEx dollars you’re spending today, right? That’s really a good example of, I think, what’s going to happen across the board. We see that in customer service. We see that in sales. We see that in marketing. Anywhere, there’s operations. That’s why I described it as knowledge turns. You can even think of it as lean for knowledge work, right, because it just reduces waste, increases speed, and customer value. And so, one of the interesting rate limiters here is culture change inside of organizations. When I say culture change that means process change. And Amy referenced this even in her answer to the first question because at the end of the day companies will have to take a process, simplify the process, automate the process, and apply these solutions. And so that requires not just technology, but in fact, companies to go do the hard work of culturally changing how they adopt technology to drive that operating leverage. And this is where we are going to see firm-level performance differences. So, one of the things we see is any customer who is working closely with us deploying it internally at Microsoft we see it, right. We’re also taking our own medicine to apply this across every process. And we know that this is not just about technology, it’s about being able to have the methodology that goes with it. And so, we see it in software development. We see it in customer service. We’re seeing it even in the horizontal use of Copilot today where every day people are discovering new workflows that they can optimize. And so, that’s like the PC when it became standard issue in early 1990s. That’s the closest analogy I can come up with. And so, yes, it will take time for it to percolate through the economy, but this is faster diffusion, faster rate of adoption than anything we have seen in the past, as evidenced even by Copilot, right. It’s faster than any suite we have sold in the past, but it is going to require workflow and process change.
Satya Nadella: Yes, it's a great question. So the way we see it play out is, if you think about it, the way Office was used broadly for knowledge work was in the context of business processes, right? So it's not like – when people do knowledge work, they're not doing knowledge work, they're doing knowledge work and support of making progress in the context of sales enablement, customer service, revenue ops, supply chain or what have you, right? So that's the first thing to note. And they do it inside of e-mail. They do it inside of Teams. They do it inside of Excel, PowerPoint, Word and what have you. Now we have the ability to essentially bridge the work and the work artifacts inside of these knowledge worker tools with the workflow and the business process and the business process data. So when we think about our Copilot, our Copilot has that ability to integrate, whether it's with ServiceNow, it has the ability to integrate with SAP with Salesforce, with obviously Dynamics. That's what we are seeing. In fact, you'll hear us talk a lot about it at our developer conference, which is the extensibility and Copilot Studio is really off to the races in terms of the product that most people are excited because one of the things in the enterprise if you want to ground your Copilot with enterprise data, which is in all of these SaaS applications and Copilot Studio is the tool to use it, to make that happen. And so that's what we are seeing, which is we are building a Copilot, which also happens to be an orchestrator of all in other Copilots, which to us appear as extensions. And net-net, what happens is some of these knowledge worker tools that people have used all the time, right? Because when you think about Teams, when you're having a meeting, you're not doing a random meeting, the meeting is in the context of some business process. It could be a supply chain meeting where you're trying to understand which suppliers to bet on or what terms to do. And so now you can access all that data right in the Team's context. So that's I think what's exciting for us, having built all these horizontal tools, which I would say we're under underappreciated for the amount of work. How people use those tools to make progress on business process, but we now get to bridge that between the business applications and knowledge worker tools, tomorrow horizontally.
Satya Nadella: Yes, it's a great question because there are two sets of things in order to make sense for successful deployment of these new AI capabilities. I mean if you sort of say this, what is this AI, it does two things, right? There's a new user experience, there is a natural language interface and second thing is it's the reasoning engine. And the reasoning engine requires good data, and it's good requires, good data for grounding, right? So people talk about something called retrieval augmented generation. And in that context, having good grounding data that then help with the reasoning, I think, is helpful. And then, of course, people are also looking to sort of fine-tune or RLHF or essentially take the large model and ground it further. So all of these tools are now available, the sophistication of how to people can deploy these models across various business processes where there is data and where there is tuning of these models is also getting more widespread, even at system integrators and other developers are there to help enterprises. So all that's maturing. So we feel good. And this is what I think on the commercial side, these are some of the harder problems to solve broad consumer, right? I mean I think this is a couple of orders of magnitude of improvements in, I'll call it, our models before we can sort of have more sophisticated open-ended consumer scenarios. Whereas in the enterprise, these are all things we can go tackle. Again, I point to get up, if you think about how it's got an entire system, right? It's just not an AI model. It's the, AI – the user experience, scaffolding, the editor, the chat, then interpreter and the debugger work along with the continuations of the model to help essentially create these reasoning traces, which help the entire thing work. And effectively, what we are doing with Copilot, Copilot Studio and connectors to all these business systems, think of it as we are creating GitHub Copilot like scenarios for every business system. That's what I think is going to have both what Amy referenced is business value and better grounding. But you're absolutely right in saying a lot of work we're doing with Fabric or Cosmos or PostgreSQL is about preparing that data so that it can be integrated with these AI projects.
Amy Hood: And Keith, I do think it's important to really think about our planning cycles and we do talk about spending sequentially higher. And we look forward to being able to continue to build out the infrastructure needed to meet the demand. Another thing that you’ve really asked in the beginning was the opportunity and the size of that. And I think in some ways, it's important to think about every business process that can be impacted and the opportunity that's represented by every business process. And so when you think of it that way, I think the opportunity is significant. The opportunity to power that next wave of “cloud infrastructure” is important. It's important because we've been the leader for this decade of the cloud transition, and it's important for us to confidently invest to do that in the second wave, building on our success in the first. And I think that's really the best way to think about how we'll spend is the same way we approached it for a decade. Watch the signal, invest to be a leader in the technical foundation and then execute consistently to add value to customers. The opportunity is represented by the amount of value we add and I look forward to being able to continue to deliver that.
Amy Hood: And Mark, maybe to answer your question on, are we seeing project starts transition from maybe the – something that was core consumption to an AI project? In our results, that’s not what we saw. We saw more what Satya was speaking to earlier, which is, you see maybe growth in migrations again. You’re seeing work in the data space, again, and you’re seeing AI project starts. And I think that’s why maybe you see our growth be different, of course, than you see IT budget spend. It’s because it’s a share, I think, improvement plus also really focusing on what Satya said, it’s about spending maybe in other areas that we don’t traditionally think of as being in the IT budget spend under a CIO. It’s spend being done by the Head of Customer Service, it’s spend being done by the Head of Marketing. And I do think that will be important as we think about the opportunity ahead.
Amy Hood: Thanks, Karl. There’s not a seasonality to the numbers. So, you’re absolutely right to start there, and it’s a good question. The way to think about it is a bit more by – it is how much capacity we have in play and how much capacity that we have to sell on the inferencing side, in particular. And so, that is partially why you see the capital investment in the shape that is, is because right this minute, we do have demand that exceeds our supply by a bit. So, it is fair to say that, that could have been an impact on the number for the quarter and it does impact a little bit the number in Q4.
Amy Hood: Thanks, Michael. I may take those a bit in reverse. It's a little easier to address them. When you think about – we've been talking about sort of stabilization and what you saw this quarter, if you break down the Azure number as you saw, which I think I talked a little bit about with Karl was 7 points of contribution from AI, and you could call them the difference '24 from our core really Azure business. And within that, the activity we saw and the consumption side was really this balance that we were quite used to and have seen throughout the cloud transition. We saw new workload starts and we saw optimizations. And then those optimizations create new budget, and you apply it. And that cycle which is actually quite normal. We saw it again this quarter in a balanced way. And I think when we talk about stabilization or even what we saw between Q2 and Q3, which is a bit of acceleration in that core, was a lot of the newer project starts relating back to not just AI starts, but lots of other workflows. The companies are still going from on-prem to cloud, Satya mentioned migrations. And some of that, which I know isn't as exciting as talking about all the AI projects. This is still really foundational work to allow companies to take advantage of the cost savings and the total TCO is still really good. And so I think that balance is really what you saw this quarter, and I do feel like there wasn't really a big difference, Michael, across industries or across geos. So I would say it was actually pretty consistent is the other maybe texture that I could give you to that question. And so then when you're saying do we keep sort of pointing to stabilization, I really do look sort of workload to workload. What are we seeing? Where it starts? And this one actually felt quite balanced and optimization looks like they normally would, which by the way, is super important. It's something we encourage customers to do. You want to run your workloads as efficiently as you possibly can. It's critical to customers being able to grow and get value out of that. So I sometimes think we – you all may ask the question more as a negative. And for us, it's just about a healthy cycle at the customer account level.
Amy Hood: Thanks for the question. It's a good opportunity to clarify. And I would not say that there is a capacity constraint on the Copilots. It's a real priority for us to make sure we optimize the allocation of our capacity to make sure that those per user businesses are able to continue to grow. And so think about that as our priority one. And so then what that does mean is capacity constraints when we have them, you'll tend to see them on the Azure infrastructure side, the consumption side of the business is a better way of thinking about it
